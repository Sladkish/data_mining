2019-10-21 20:32:45 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:32:45 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:32:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:32:45 [scrapy.extensions.telnet] INFO: Telnet Password: 26582551d748b1e3
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:32:45 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:32:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:32:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:32:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:32:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 11, in parse
    prof = response.css('span.g-user-content a').text
AttributeError: 'SelectorList' object has no attribute 'text'
2019-10-21 20:32:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:32:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 303,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49920,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.844354,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 32, 46, 161710),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 32, 45, 317356)}
2019-10-21 20:32:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:32:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:32:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:32:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:32:54 [scrapy.extensions.telnet] INFO: Telnet Password: 3e00087a49e41ea7
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:32:54 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:32:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:32:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:32:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:32:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 11, in parse
    prof = response.css('span.g-user-content a').text
AttributeError: 'SelectorList' object has no attribute 'text'
2019-10-21 20:32:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:32:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 303,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49901,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.866951,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 32, 55, 211552),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 32, 54, 344601)}
2019-10-21 20:32:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:33:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:33:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:33:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:33:23 [scrapy.extensions.telnet] INFO: Telnet Password: 63451b5519bff80d
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:33:23 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:33:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:33:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:33:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:33:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:33:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:33:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:33:56 [scrapy.extensions.telnet] INFO: Telnet Password: 7f05405f2761c291
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:33:56 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:33:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:33:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:33:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:35:20 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:35:20 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:35:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:35:20 [scrapy.extensions.telnet] INFO: Telnet Password: 50fa158b8c222b36
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:35:20 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:35:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:35:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:35:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:35:49 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:35:49 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:35:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:35:49 [scrapy.extensions.telnet] INFO: Telnet Password: dc2edd1bdcbbc55d
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:35:49 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:35:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:35:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:35:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:36:04 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:36:04 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:36:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:36:04 [scrapy.extensions.telnet] INFO: Telnet Password: cb0c6ceec5e55e03
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:36:04 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:36:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:36:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:36:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:41:02 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:41:02 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:41:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:41:02 [scrapy.extensions.telnet] INFO: Telnet Password: 420070c5508d18a4
2019-10-21 20:41:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:41:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:41:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:41:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:41:03 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:41:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:41:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:41:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:43:34 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:43:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:43:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:43:34 [scrapy.extensions.telnet] INFO: Telnet Password: 4b5905bb1825be20
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:43:34 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:43:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:43:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:43:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:44:11 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:44:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:44:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:44:11 [scrapy.extensions.telnet] INFO: Telnet Password: 4e450383d57cfc47
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:44:11 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:44:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:44:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:44:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:44:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:44:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:44:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:44:23 [scrapy.extensions.telnet] INFO: Telnet Password: f042aa94f0867e00
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:44:23 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:44:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:44:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:44:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:44:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:44:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:44:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:44:58 [scrapy.extensions.telnet] INFO: Telnet Password: 90eb21663e429b2b
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:44:58 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:44:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:44:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:44:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:45:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:45:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:45:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:45:32 [scrapy.extensions.telnet] INFO: Telnet Password: 691c9d7456cc668b
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:45:32 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:45:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:45:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:45:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:46:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:46:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:46:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:46:56 [scrapy.extensions.telnet] INFO: Telnet Password: 17641c7ef0c084fd
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:46:56 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:46:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:46:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:46:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:47:41 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:47:41 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:47:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:47:41 [scrapy.extensions.telnet] INFO: Telnet Password: 8d851a207dcb26b0
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:47:41 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:47:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:47:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:47:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:48:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:48:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:48:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:48:32 [scrapy.extensions.telnet] INFO: Telnet Password: 4dfb82d76362dc44
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:48:32 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:48:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:48:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:48:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:49:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:49:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:49:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:49:58 [scrapy.extensions.telnet] INFO: Telnet Password: 5729d65d902c5c20
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:49:58 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:49:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:49:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:49:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:50:34 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:50:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:50:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:50:34 [scrapy.extensions.telnet] INFO: Telnet Password: b7b88c9c8cb60924
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:50:34 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:50:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:50:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:50:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:50:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('span.pager-item-not-in-short-range a::attr')
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 81, in xpath_pseudo_element
    % pseudo_element)
cssselect.xpath.ExpressionError: The pseudo-element ::attr is unknown
2019-10-21 20:50:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:50:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42965,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.599956,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 50, 35, 224160),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 50, 34, 624204)}
2019-10-21 20:50:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:50:40 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:50:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:50:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:50:40 [scrapy.extensions.telnet] INFO: Telnet Password: 6209a54712c78494
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:50:40 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:50:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:50:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:50:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:50:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('span.pager-item-not-in-short-range a::attr')
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 81, in xpath_pseudo_element
    % pseudo_element)
cssselect.xpath.ExpressionError: The pseudo-element ::attr is unknown
2019-10-21 20:50:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:50:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43006,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.573457,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 50, 41, 547334),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 50, 40, 973877)}
2019-10-21 20:50:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:50:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:50:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:50:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:50:59 [scrapy.extensions.telnet] INFO: Telnet Password: acb47b24eea4212f
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:50:59 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:50:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:50:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:50:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:51:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:51:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:51:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:51:22 [scrapy.extensions.telnet] INFO: Telnet Password: c9261a97220d19fc
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:51:22 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:51:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:51:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:51:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:52:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:52:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:52:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:52:58 [scrapy.extensions.telnet] INFO: Telnet Password: 3d0846234adb1b9a
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:52:58 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:52:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:52:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:52:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:55:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:55:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:55:12 [scrapy.extensions.telnet] INFO: Telnet Password: b08e6479c137ba56
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:55:12 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:55:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:55:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:55:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 17, in parse
    print(pagination[-1])
IndexError: list index out of range
2019-10-21 20:55:12 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:55:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43149,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.637381,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 55, 12, 992806),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 55, 12, 355425)}
2019-10-21 20:55:12 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:55:48 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:55:48 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:55:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:55:48 [scrapy.extensions.telnet] INFO: Telnet Password: c8bdf400dc0e9ba2
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:55:48 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:55:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:55:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:55:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('a."bloko-button HH-Pager-Control" a::attr(href)').extract()
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 415, in parse
    return list(parse_selector_group(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 428, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 436, in parse_selector
    result, pseudo_element = parse_simple_selector(stream)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 492, in parse_simple_selector
    result = Class(result, stream.next_ident())
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 819, in next_ident
    raise SelectorSyntaxError('Expected ident, got %s' % (next,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected ident, got <STRING 'bloko-button HH-Pager-Control' at 2>
2019-10-21 20:55:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:55:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42982,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.757368,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 55, 49, 335132),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/SelectorSyntaxError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 55, 48, 577764)}
2019-10-21 20:55:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:55:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:55:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:55:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:55:56 [scrapy.extensions.telnet] INFO: Telnet Password: 2ca2fe503d852254
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:55:57 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:55:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:55:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:55:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('a."bloko-button HH-Pager-Control" a::attr(href)').extract()
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 415, in parse
    return list(parse_selector_group(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 428, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 436, in parse_selector
    result, pseudo_element = parse_simple_selector(stream)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 492, in parse_simple_selector
    result = Class(result, stream.next_ident())
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 819, in next_ident
    raise SelectorSyntaxError('Expected ident, got %s' % (next,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected ident, got <STRING 'bloko-button HH-Pager-Control' at 2>
2019-10-21 20:55:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:55:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42990,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.61138,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 55, 57, 673213),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/SelectorSyntaxError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 55, 57, 61833)}
2019-10-21 20:55:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:56:11 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:56:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:56:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:56:11 [scrapy.extensions.telnet] INFO: Telnet Password: 2e33f697e05bbc61
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:56:11 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:56:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:56:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:56:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:56:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('a.(bloko-button HH-Pager-Control) a::attr(href)').extract()
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 415, in parse
    return list(parse_selector_group(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 428, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 436, in parse_selector
    result, pseudo_element = parse_simple_selector(stream)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 492, in parse_simple_selector
    result = Class(result, stream.next_ident())
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 819, in next_ident
    raise SelectorSyntaxError('Expected ident, got %s' % (next,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected ident, got <DELIM '(' at 2>
2019-10-21 20:56:12 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:56:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43118,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.598932,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 56, 12, 384847),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/SelectorSyntaxError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 56, 11, 785915)}
2019-10-21 20:56:12 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:56:24 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:56:24 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:56:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:56:24 [scrapy.extensions.telnet] INFO: Telnet Password: c345b5679afbb285
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:56:24 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:56:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:56:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:56:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:56:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 17, in parse
    print(pagination[-1])
IndexError: list index out of range
2019-10-21 20:56:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:56:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42923,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.62601,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 56, 25, 71841),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 56, 24, 445831)}
2019-10-21 20:56:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:57:00 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:57:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:57:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:57:00 [scrapy.extensions.telnet] INFO: Telnet Password: 2305d4d195a36854
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:57:00 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:57:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:57:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:57:44 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:57:44 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:57:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:57:44 [scrapy.extensions.telnet] INFO: Telnet Password: 2b1ac1bd7966fa1c
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:57:44 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:57:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:57:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:57:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:58:31 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:58:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:58:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:58:31 [scrapy.extensions.telnet] INFO: Telnet Password: 03fda66eeb3b16f6
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:58:31 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:58:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:58:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:07:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:07:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:07:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:07:52 [scrapy.extensions.telnet] INFO: Telnet Password: d444d230bdbefcc7
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:07:52 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:07:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:07:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:07:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 18, in parse
    print(pagination[-1])
IndexError: list index out of range
2019-10-21 21:07:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:07:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43013,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.59302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 7, 53, 174670),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 18, 7, 52, 581650)}
2019-10-21 21:07:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:08:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:08:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:08:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:08:22 [scrapy.extensions.telnet] INFO: Telnet Password: 9438cb94c1de0486
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:08:22 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:08:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:08:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:08:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 19, in parse
    print(pagination[3])
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2019-10-21 21:08:23 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:08:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42968,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.574549,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 8, 23, 224253),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 18, 8, 22, 649704)}
2019-10-21 21:08:23 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:08:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:08:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:08:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:08:54 [scrapy.extensions.telnet] INFO: Telnet Password: a9027e9fefb517ee
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:08:54 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:08:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:08:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:08:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:08:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 19, in parse
    print(pagination[3])
IndexError: list index out of range
2019-10-21 21:08:54 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:08:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43048,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.572722,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 8, 54, 791839),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 18, 8, 54, 219117)}
2019-10-21 21:08:54 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:09:06 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:09:06 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:09:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:09:06 [scrapy.extensions.telnet] INFO: Telnet Password: 02ba7a4eae034b0d
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:09:06 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:09:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:09:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:09:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:09:55 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:09:55 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:09:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:09:55 [scrapy.extensions.telnet] INFO: Telnet Password: c85356c11ff57b6b
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:09:55 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:09:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:09:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:09:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:10:40 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:10:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:10:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:10:40 [scrapy.extensions.telnet] INFO: Telnet Password: 4a5f72623bc661d5
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:10:40 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:10:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:10:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:10:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:11:00 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:11:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:11:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:11:00 [scrapy.extensions.telnet] INFO: Telnet Password: c0c8224dabcdea43
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:11:00 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:11:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:11:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:11:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:12:06 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:12:06 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:12:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:12:06 [scrapy.extensions.telnet] INFO: Telnet Password: ed08d963d21be3c4
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:12:06 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:12:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:12:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:12:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:12:30 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:12:30 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:12:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:12:30 [scrapy.extensions.telnet] INFO: Telnet Password: 7a51aac067d05717
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:12:30 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:12:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:12:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:12:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:13:31 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:13:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?text=Data+science)
2019-10-21 21:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-21 21:13:37 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:13:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:13:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1277,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128179,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 72.127122,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 13, 42, 267199),
 'log_count/DEBUG': 4,
 'log_count/INFO': 11,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 12, 30, 140077)}
2019-10-21 21:13:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:15:44 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:15:44 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:15:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:15:44 [scrapy.extensions.telnet] INFO: Telnet Password: c310e7515ec97d93
2019-10-21 21:15:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:15:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:15:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:15:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:15:45 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:15:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:15:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:15:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:15:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:15:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-21 21:15:59 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:16:03 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:16:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1391,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128230,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.005512,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 16, 3, 57801),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 15, 45, 52289)}
2019-10-21 21:16:03 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:16:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:16:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:16:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:16:23 [scrapy.extensions.telnet] INFO: Telnet Password: 80e067fb2e4cd07d
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:16:23 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:16:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:16:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:16:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:16:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:17:31 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:17:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:17:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:17:31 [scrapy.extensions.telnet] INFO: Telnet Password: fb1aea8658affad8
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:17:31 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:17:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:17:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:17:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:17:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:17:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-21 21:17:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-21 21:17:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8)
2019-10-21 21:17:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10)
2019-10-21 21:17:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12)
2019-10-21 21:17:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14)
2019-10-21 21:18:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16)
2019-10-21 21:18:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18)
2019-10-21 21:18:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20)
2019-10-21 21:18:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22)
2019-10-21 21:18:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24)
2019-10-21 21:18:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26)
2019-10-21 21:18:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28)
2019-10-21 21:18:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30)
2019-10-21 21:18:31 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:18:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32)
2019-10-21 21:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34)
2019-10-21 21:18:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36)
2019-10-21 21:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38)
2019-10-21 21:18:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:18:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:18:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:18:59 [scrapy.extensions.telnet] INFO: Telnet Password: a6e194558d06cbf2
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:18:59 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:18:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:19:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:19:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:19:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-21 21:19:12 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:19:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:19:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1391,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128888,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.158348,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 19, 17, 895238),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 18, 59, 736890)}
2019-10-21 21:19:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:30:21 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:30:21 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:30:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:30:21 [scrapy.extensions.telnet] INFO: Telnet Password: e1c54ebe81309300
2019-10-21 21:30:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:30:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:30:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:30:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:30:22 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:30:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:30:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:30:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:30:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-21 21:30:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-21 21:31:09 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:31:09 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:31:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:31:09 [scrapy.extensions.telnet] INFO: Telnet Password: 9a02b8552549bd34
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:31:09 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:31:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:31:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:31:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:31:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-21 21:31:25 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:31:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:31:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1391,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128049,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 22.776688,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 31, 32, 48122),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 31, 9, 271434)}
2019-10-21 21:31:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:32:40 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:32:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:32:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:32:40 [scrapy.extensions.telnet] INFO: Telnet Password: 3b46aeddd56b7482
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:32:40 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:32:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:32:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:32:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-21 21:32:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:32:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:32:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:32:56 [scrapy.extensions.telnet] INFO: Telnet Password: 9d3787c4620d12ab
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:32:56 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:32:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:32:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:32:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-21 21:33:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-21 21:33:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-21 21:33:05 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:33:08 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:33:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1405,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128849,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 11.721174,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 33, 8, 323523),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 32, 56, 602349)}
2019-10-21 21:33:08 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 19:15:11 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:15:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:15:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:15:11 [scrapy.extensions.telnet] INFO: Telnet Password: e3568a9844dbf9d0
2019-10-23 19:15:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:15:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:15:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:15:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:15:12 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:15:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:15:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:15:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:15:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:15:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-23 19:15:20 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 19:15:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-23 19:15:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1405,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128232,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 7.953121,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 23, 16, 15, 20, 13120),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 23, 16, 15, 12, 59999)}
2019-10-23 19:15:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 19:31:47 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:31:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:31:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:31:47 [scrapy.extensions.telnet] INFO: Telnet Password: 3806715424bc0a88
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:31:47 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:31:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:31:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:31:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:31:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:31:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-23 19:31:56 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 19:31:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-23 19:31:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1405,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128456,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 8.479215,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 23, 16, 31, 56, 235945),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 23, 16, 31, 47, 756730)}
2019-10-23 19:31:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 19:32:31 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:32:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:32:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:32:31 [scrapy.extensions.telnet] INFO: Telnet Password: 4917a3e4dcd8a654
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:32:31 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:32:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:32:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:32:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:32:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 19:33:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-23 19:33:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8)
2019-10-23 19:50:11 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:50:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:50:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:50:11 [scrapy.extensions.telnet] INFO: Telnet Password: eb0e736bacff18a1
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:50:12 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:50:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:50:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:50:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:50:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:50:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 19:51:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:51:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:51:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:51:19 [scrapy.extensions.telnet] INFO: Telnet Password: 5717250efa1cdd15
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:51:19 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:51:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:51:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:51:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:52:25 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:52:25 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:52:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:52:25 [scrapy.extensions.telnet] INFO: Telnet Password: afed227caabec4a0
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:52:25 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:52:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:52:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:52:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:52:26 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 20:28:28 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 20:28:28 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 20:28:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 20:28:28 [scrapy.extensions.telnet] INFO: Telnet Password: 68a6d204f0219d47
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 20:28:28 [scrapy.core.engine] INFO: Spider opened
2019-10-23 20:28:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:28:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 20:28:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 20:28:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 20:28:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-23 20:28:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 20:28:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-23 20:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-23 20:29:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 20:29:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 20:29:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 20:29:19 [scrapy.extensions.telnet] INFO: Telnet Password: 5b376d9d1ac93168
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 20:29:19 [scrapy.core.engine] INFO: Spider opened
2019-10-23 20:29:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:29:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 20:29:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 20:29:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 20:29:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-23 20:29:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 20:29:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-23 20:29:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-23 20:29:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-23 20:29:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=9> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8)
2019-10-23 20:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=9)
2019-10-23 20:29:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=11> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10)
2019-10-23 20:29:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=11)
2019-10-23 20:30:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=13> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12)
2019-10-23 20:30:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=13)
2019-10-23 20:30:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=15> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14)
2019-10-23 20:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=15)
2019-10-23 20:30:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=17> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16)
2019-10-23 20:30:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=17)
2019-10-23 20:30:19 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 17 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:30:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=19> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18)
2019-10-23 20:30:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=19)
2019-10-23 20:30:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=21> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20)
2019-10-23 20:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=21)
2019-10-23 20:30:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=23> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22)
2019-10-23 20:30:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=23)
2019-10-23 20:30:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=25> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24)
2019-10-23 20:30:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=25)
2019-10-23 20:30:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=27> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26)
2019-10-23 20:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=27)
2019-10-23 20:31:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=29> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28)
2019-10-23 20:31:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=29)
2019-10-23 20:31:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=31> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30)
2019-10-23 20:31:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=31)
2019-10-23 20:31:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=33> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32)
2019-10-23 20:31:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=33)
2019-10-23 20:31:19 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:31:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=35> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34)
2019-10-23 20:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=35)
2019-10-23 20:31:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=37> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36)
2019-10-23 20:31:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=37)
2019-10-23 20:31:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=39> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38)
2019-10-23 20:31:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=39)
2019-10-23 20:31:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=41> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40)
2019-10-23 20:31:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=42> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=41)
2019-10-23 20:31:46 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 20:31:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-23 20:31:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20850,
 'downloader/request_count': 41,
 'downloader/request_method_count/GET': 41,
 'downloader/response_bytes': 1738900,
 'downloader/response_count': 41,
 'downloader/response_status_count/200': 41,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 146.689369,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 23, 17, 31, 46, 97076),
 'log_count/DEBUG': 42,
 'log_count/INFO': 12,
 'request_depth_max': 41,
 'response_received_count': 41,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 10, 23, 17, 29, 19, 407707)}
2019-10-23 20:31:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 20:33:04 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 20:33:04 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 20:33:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 20:33:04 [scrapy.extensions.telnet] INFO: Telnet Password: f0de803168ed8888
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 20:33:04 [scrapy.core.engine] INFO: Spider opened
2019-10-23 20:33:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:33:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 20:33:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 20:31:37 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 20:31:37 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 20:31:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 20:31:37 [scrapy.extensions.telnet] INFO: Telnet Password: 6127b1c7ae6bee43
2019-10-24 20:31:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 20:31:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 20:31:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 20:31:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 20:31:38 [scrapy.core.engine] INFO: Spider opened
2019-10-24 20:31:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 20:31:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 20:31:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 20:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 20:32:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 20:32:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 20:32:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 20:32:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 20:32:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 20:32:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 20:32:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 20:32:13 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 20:32:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 20:32:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361113,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 38.055391,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 17, 32, 16, 64971),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 17, 31, 38, 9580)}
2019-10-24 20:32:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 20:34:08 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 20:34:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 20:34:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 20:34:08 [scrapy.extensions.telnet] INFO: Telnet Password: 639eabe89ac050b0
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 20:34:08 [scrapy.core.engine] INFO: Spider opened
2019-10-24 20:34:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 20:34:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 20:34:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 20:34:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 20:34:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 20:34:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 20:34:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 20:34:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 20:34:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 20:34:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 20:34:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 20:34:18 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 20:34:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 20:34:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361232,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 10.45162,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 17, 34, 18, 874664),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 17, 34, 8, 423044)}
2019-10-24 20:34:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 21:11:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:11:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:11:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:11:19 [scrapy.extensions.telnet] INFO: Telnet Password: 9882409fa41a21af
2019-10-24 21:11:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:11:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:11:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:11:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:11:20 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:11:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:11:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:11:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:11:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 21:11:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 21:11:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 21:11:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 21:11:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 21:11:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 21:11:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 21:11:30 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 21:11:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 21:11:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361203,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 10.332518,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 18, 11, 30, 354262),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 18, 11, 20, 21744)}
2019-10-24 21:11:30 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 21:11:58 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:11:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:11:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:11:58 [scrapy.extensions.telnet] INFO: Telnet Password: ecc296afb34097d6
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:11:58 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:11:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:11:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:11:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:11:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:12:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 21:12:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 21:12:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 21:12:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 21:12:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 21:12:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 21:12:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 21:12:49 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 21:12:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 21:12:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361408,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 51.802708,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 18, 12, 49, 929097),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 18, 11, 58, 126389)}
2019-10-24 21:12:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 21:12:57 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:12:57 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:12:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:12:57 [scrapy.extensions.telnet] INFO: Telnet Password: 8fcf2087a6fd9c20
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:12:57 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:12:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:12:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:12:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:16:34 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:16:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:16:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:16:34 [scrapy.extensions.telnet] INFO: Telnet Password: c2363520ae973865
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:16:34 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:16:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:16:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:16:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:18:52 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:18:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:18:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:18:52 [scrapy.extensions.telnet] INFO: Telnet Password: a093ec02e72f3827
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:18:53 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:18:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:18:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:19:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:25 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:20:25 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:20:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:20:25 [scrapy.extensions.telnet] INFO: Telnet Password: a8c1692e65b3db7f
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:20:25 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:20:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:20:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:20:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:21:56 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:21:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:21:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:21:56 [scrapy.extensions.telnet] INFO: Telnet Password: 8cb8748d03ec26eb
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:21:56 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:21:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:21:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:21:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:21:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:25:08 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:25:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:25:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:25:08 [scrapy.extensions.telnet] INFO: Telnet Password: 1bbef3d4ebbf8b6e
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:25:08 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:25:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:25:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:25:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:25:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:25:59 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:25:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:25:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:25:59 [scrapy.extensions.telnet] INFO: Telnet Password: 83c0debd8e1c3ddb
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:25:59 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:25:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:25:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:25:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:26:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:34:46 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:34:46 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:34:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:34:46 [scrapy.extensions.telnet] INFO: Telnet Password: 10baddf48c2095ac
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:34:46 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:34:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:34:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:34:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:34:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:35:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:35:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:35:19 [scrapy.extensions.telnet] INFO: Telnet Password: 513f1139bc7ca114
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:35:19 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:35:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:35:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:35:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:35:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:17 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:36:17 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:36:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:36:17 [scrapy.extensions.telnet] INFO: Telnet Password: ee28b19a62bdaf74
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:36:17 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:36:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:36:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:36:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:54 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:36:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:36:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:36:54 [scrapy.extensions.telnet] INFO: Telnet Password: 7d975548396161ed
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:36:54 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:36:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:36:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:36:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:36:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:37:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:37:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:37:59 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:37:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:37:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:37:59 [scrapy.extensions.telnet] INFO: Telnet Password: a8f5d7586c9219b6
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:37:59 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:37:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:37:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:38:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:38:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:32 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:38:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:38:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:38:32 [scrapy.extensions.telnet] INFO: Telnet Password: 266452011b91eec9
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:38:32 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:38:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:38:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:38:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:39:44 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:39:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:39:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:40:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:40:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:40:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:41:59 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:41:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:41:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:41:59 [scrapy.extensions.telnet] INFO: Telnet Password: a1dacafe6a7d38d5
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:41:59 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:41:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:41:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:41:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:42:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:16 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 22:05:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-24 22:05:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-24 22:05:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/33753181?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-24 22:05:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/32322427?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-27 14:03:29 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:03:29 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:03:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:03:29 [scrapy.extensions.telnet] INFO: Telnet Password: 28987511e2f5fcd0
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-27 14:03:30 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:03:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:03:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:03:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:03:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:35 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:04:35 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:04:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:04:35 [scrapy.extensions.telnet] INFO: Telnet Password: 6847d3d8865f1bed
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-27 14:04:35 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:04:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:04:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:04:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:04:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:05:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:05:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:05:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 14:05:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof>: HTTP status code is not handled or not allowed
2019-10-27 14:05:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:05:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>: HTTP status code is not handled or not allowed
2019-10-27 14:05:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:05:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330>: HTTP status code is not handled or not allowed
2019-10-27 14:05:22 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:05:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873>: HTTP status code is not handled or not allowed
2019-10-27 14:05:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:05:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862>: HTTP status code is not handled or not allowed
2019-10-27 14:05:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:05:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585>: HTTP status code is not handled or not allowed
2019-10-27 14:05:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:05:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662>: HTTP status code is not handled or not allowed
2019-10-27 14:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:05:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel>: HTTP status code is not handled or not allowed
2019-10-27 14:05:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:05:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458>: HTTP status code is not handled or not allowed
2019-10-27 14:05:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:05:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter>: HTTP status code is not handled or not allowed
2019-10-27 14:05:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:05:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768>: HTTP status code is not handled or not allowed
2019-10-27 14:05:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:05:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770>: HTTP status code is not handled or not allowed
2019-10-27 14:05:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:05:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860>: HTTP status code is not handled or not allowed
2019-10-27 14:05:35 [scrapy.extensions.logstats] INFO: Crawled 50 pages (at 50 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:05:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:05:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746>: HTTP status code is not handled or not allowed
2019-10-27 14:05:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:05:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863>: HTTP status code is not handled or not allowed
2019-10-27 14:05:38 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:05:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/889>: HTTP status code is not handled or not allowed
2019-10-27 14:05:39 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:05:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668>: HTTP status code is not handled or not allowed
2019-10-27 14:05:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:05:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602>: HTTP status code is not handled or not allowed
2019-10-27 14:05:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:05:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976>: HTTP status code is not handled or not allowed
2019-10-27 14:05:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:05:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:05:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657>: HTTP status code is not handled or not allowed
2019-10-27 14:05:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:05:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/616>: HTTP status code is not handled or not allowed
2019-10-27 14:05:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:05:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072>: HTTP status code is not handled or not allowed
2019-10-27 14:05:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:05:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225>: HTTP status code is not handled or not allowed
2019-10-27 14:05:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:05:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307>: HTTP status code is not handled or not allowed
2019-10-27 14:05:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:05:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131>: HTTP status code is not handled or not allowed
2019-10-27 14:05:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:05:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:05:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804>: HTTP status code is not handled or not allowed
2019-10-27 14:05:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:05:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452>: HTTP status code is not handled or not allowed
2019-10-27 14:05:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:05:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730>: HTTP status code is not handled or not allowed
2019-10-27 14:05:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:06:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556>: HTTP status code is not handled or not allowed
2019-10-27 14:06:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:06:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421>: HTTP status code is not handled or not allowed
2019-10-27 14:06:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:06:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188>: HTTP status code is not handled or not allowed
2019-10-27 14:06:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:06:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959>: HTTP status code is not handled or not allowed
2019-10-27 14:06:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:06:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793>: HTTP status code is not handled or not allowed
2019-10-27 14:06:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:06:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281>: HTTP status code is not handled or not allowed
2019-10-27 14:06:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:06:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743>: HTTP status code is not handled or not allowed
2019-10-27 14:06:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:06:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358>: HTTP status code is not handled or not allowed
2019-10-27 14:06:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:06:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575>: HTTP status code is not handled or not allowed
2019-10-27 14:06:35 [scrapy.extensions.logstats] INFO: Crawled 101 pages (at 51 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:06:36 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32463248?query=data%20science)
2019-10-27 14:06:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721>: HTTP status code is not handled or not allowed
2019-10-27 14:06:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:06:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232>: HTTP status code is not handled or not allowed
2019-10-27 14:06:39 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:06:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769>: HTTP status code is not handled or not allowed
2019-10-27 14:06:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/25>: HTTP status code is not handled or not allowed
2019-10-27 14:06:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:06:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184>: HTTP status code is not handled or not allowed
2019-10-27 14:06:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:06:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182>: HTTP status code is not handled or not allowed
2019-10-27 14:06:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:07:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269>: HTTP status code is not handled or not allowed
2019-10-27 14:07:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:07:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307>: HTTP status code is not handled or not allowed
2019-10-27 14:07:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:07:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902>: HTTP status code is not handled or not allowed
2019-10-27 14:07:33 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:07:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749>: HTTP status code is not handled or not allowed
2019-10-27 14:07:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:35 [scrapy.extensions.logstats] INFO: Crawled 148 pages (at 47 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:07:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:07:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798>: HTTP status code is not handled or not allowed
2019-10-27 14:07:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:07:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986>: HTTP status code is not handled or not allowed
2019-10-27 14:07:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 14:07:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814>: HTTP status code is not handled or not allowed
2019-10-27 14:07:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:07:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od>: HTTP status code is not handled or not allowed
2019-10-27 14:07:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:07:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858>: HTTP status code is not handled or not allowed
2019-10-27 14:07:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 14:07:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004>: HTTP status code is not handled or not allowed
2019-10-27 14:07:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 14:07:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984>: HTTP status code is not handled or not allowed
2019-10-27 14:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 14:07:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298>: HTTP status code is not handled or not allowed
2019-10-27 14:07:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 14:07:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078>: HTTP status code is not handled or not allowed
2019-10-27 14:08:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 14:08:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130>: HTTP status code is not handled or not allowed
2019-10-27 14:08:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 14:08:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063>: HTTP status code is not handled or not allowed
2019-10-27 14:08:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 14:08:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211>: HTTP status code is not handled or not allowed
2019-10-27 14:08:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 14:08:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389>: HTTP status code is not handled or not allowed
2019-10-27 14:08:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:22 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 14:08:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301>: HTTP status code is not handled or not allowed
2019-10-27 14:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 14:08:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500>: HTTP status code is not handled or not allowed
2019-10-27 14:08:35 [scrapy.extensions.logstats] INFO: Crawled 197 pages (at 49 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:08:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 14:08:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/139>: HTTP status code is not handled or not allowed
2019-10-27 14:08:36 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 14:08:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/162>: HTTP status code is not handled or not allowed
2019-10-27 14:08:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 14:08:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186>: HTTP status code is not handled or not allowed
2019-10-27 14:08:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 14:08:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595>: HTTP status code is not handled or not allowed
2019-10-27 14:08:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 14:08:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069>: HTTP status code is not handled or not allowed
2019-10-27 14:08:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 14:08:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110>: HTTP status code is not handled or not allowed
2019-10-27 14:08:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 14:08:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435>: HTTP status code is not handled or not allowed
2019-10-27 14:09:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 14:09:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272>: HTTP status code is not handled or not allowed
2019-10-27 14:09:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:07 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 14:09:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245>: HTTP status code is not handled or not allowed
2019-10-27 14:09:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 14:09:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114>: HTTP status code is not handled or not allowed
2019-10-27 14:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:11 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 14:09:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687>: HTTP status code is not handled or not allowed
2019-10-27 14:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 14:09:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/854>: HTTP status code is not handled or not allowed
2019-10-27 14:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 14:09:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/217737>: HTTP status code is not handled or not allowed
2019-10-27 14:09:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 14:09:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2869446>: HTTP status code is not handled or not allowed
2019-10-27 14:09:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 14:09:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3289413>: HTTP status code is not handled or not allowed
2019-10-27 14:09:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 14:09:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/62126>: HTTP status code is not handled or not allowed
2019-10-27 14:09:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:35 [scrapy.extensions.logstats] INFO: Crawled 247 pages (at 50 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:09:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33847716?query=data%20science)
2019-10-27 14:09:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1304>: HTTP status code is not handled or not allowed
2019-10-27 14:09:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
2019-10-27 14:09:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/80>: HTTP status code is not handled or not allowed
2019-10-27 14:09:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3832?dpt=3832-3832-const> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
2019-10-27 14:09:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3832?dpt=3832-3832-const>: HTTP status code is not handled or not allowed
2019-10-27 14:09:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3358623> (referer: https://spb.hh.ru/vacancy/31015468?query=data%20science)
2019-10-27 14:09:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3358623>: HTTP status code is not handled or not allowed
2019-10-27 14:09:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/97598> (referer: https://spb.hh.ru/vacancy/33921789?query=data%20science)
2019-10-27 14:09:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/97598>: HTTP status code is not handled or not allowed
2019-10-27 14:09:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3491569> (referer: https://spb.hh.ru/vacancy/33912707?query=data%20science)
2019-10-27 14:09:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3491569>: HTTP status code is not handled or not allowed
2019-10-27 14:09:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/14153> (referer: https://spb.hh.ru/vacancy/33497437?query=data%20science)
2019-10-27 14:09:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/14153>: HTTP status code is not handled or not allowed
2019-10-27 14:09:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1587> (referer: https://spb.hh.ru/vacancy/34031778?query=data%20science)
2019-10-27 14:09:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1587>: HTTP status code is not handled or not allowed
2019-10-27 14:09:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2558659> (referer: https://spb.hh.ru/vacancy/33813402?query=data%20science)
2019-10-27 14:09:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2558659>: HTTP status code is not handled or not allowed
2019-10-27 14:09:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
2019-10-27 14:09:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/52729>: HTTP status code is not handled or not allowed
2019-10-27 14:09:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2097195> (referer: https://spb.hh.ru/vacancy/31252237?query=data%20science)
2019-10-27 14:09:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2097195>: HTTP status code is not handled or not allowed
2019-10-27 14:09:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 14:09:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 115681,
 'downloader/request_count': 265,
 'downloader/request_method_count/GET': 265,
 'downloader/response_bytes': 7544079,
 'downloader/response_count': 265,
 'downloader/response_status_count/200': 176,
 'downloader/response_status_count/404': 89,
 'dupefilter/filtered': 79,
 'elapsed_time_seconds': 321.248594,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 11, 9, 56, 752102),
 'httperror/response_ignored_count': 89,
 'httperror/response_ignored_status_count/404': 89,
 'log_count/DEBUG': 266,
 'log_count/INFO': 104,
 'request_depth_max': 10,
 'response_received_count': 265,
 'scheduler/dequeued': 265,
 'scheduler/dequeued/memory': 265,
 'scheduler/enqueued': 265,
 'scheduler/enqueued/memory': 265,
 'start_time': datetime.datetime(2019, 10, 27, 11, 4, 35, 503508)}
2019-10-27 14:09:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 14:21:00 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:21:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:21:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:21:00 [scrapy.extensions.telnet] INFO: Telnet Password: 625f1a4e3692ef70
2019-10-27 14:21:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:21:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:21:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:21:01 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:21:01 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:21:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:21:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': [' з/п не указана']}
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': [' з/п не указана']}
2019-10-27 14:21:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ['от 150\xa0000 руб. до вычета налогов']}
2019-10-27 14:21:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': [' з/п не указана']}
2019-10-27 14:29:03 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:29:03 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:29:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:29:03 [scrapy.extensions.telnet] INFO: Telnet Password: e66a13b7837a2c9d
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:29:03 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:29:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:29:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:29:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57f80a2de5eb72a9c98a0')}
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 1 items (at 1 items/min)
2019-10-27 14:30:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ['от 150\xa0000 руб. до вычета налогов'], '_id': ObjectId('5db57fc9a2de5eb72a9c98a1')}
2019-10-27 14:30:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcca2de5eb72a9c98a2')}
2019-10-27 14:30:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcda2de5eb72a9c98a3')}
2019-10-27 14:30:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcda2de5eb72a9c98a4')}
2019-10-27 14:30:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcea2de5eb72a9c98a5')}
2019-10-27 14:30:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcea2de5eb72a9c98a6')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcea2de5eb72a9c98a7')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcfa2de5eb72a9c98a8')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcfa2de5eb72a9c98a9')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcfa2de5eb72a9c98aa')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fcfa2de5eb72a9c98ab')}
2019-10-27 14:30:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fd0a2de5eb72a9c98ac')}
2019-10-27 14:30:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fd0a2de5eb72a9c98ad')}
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fd0a2de5eb72a9c98ae')}
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': [' з/п не указана'], '_id': ObjectId('5db57fd0a2de5eb72a9c98af')}
2019-10-27 14:30:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ['до 130\xa0000 руб. до вычета налогов'], '_id': ObjectId('5db57fd1a2de5eb72a9c98b0')}
2019-10-27 14:30:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ['от 100\xa0000 до 150\xa0000 руб. на руки'], '_id': ObjectId('5db57fd1a2de5eb72a9c98b1')}
2019-10-27 14:31:36 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:31:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:31:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:31:36 [scrapy.extensions.telnet] INFO: Telnet Password: dc506c537b6d77e9
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:31:36 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:31:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:31:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:31:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580199d1fdde02f064891')}
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db580259d1fdde02f064892')}
2019-10-27 14:31:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580259d1fdde02f064893')}
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580269d1fdde02f064894')}
2019-10-27 14:31:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db580269d1fdde02f064895')}
2019-10-27 14:31:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db580269d1fdde02f064896')}
2019-10-27 14:31:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580269d1fdde02f064897')}
2019-10-27 14:31:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db580289d1fdde02f064898')}
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db580289d1fdde02f064899')}
2019-10-27 14:31:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db580289d1fdde02f06489a')}
2019-10-27 14:32:47 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:32:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:32:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:32:47 [scrapy.extensions.telnet] INFO: Telnet Password: 75bbc2711534d2a7
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:32:47 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:32:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:32:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289da')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289db')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289dc')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580604d640ee4f41289dd')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289de')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289df')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e0')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e1')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e2')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e3')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e4')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e5')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e6')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e7')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289e8')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db580604d640ee4f41289e9')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db580604d640ee4f41289ea')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580604d640ee4f41289eb')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289ec')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db580604d640ee4f41289ed')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289ee')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289ef')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580614d640ee4f41289f0')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f1')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f2')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f3')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f4')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f5')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f6')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f7')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f8')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289f9')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289fa')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289fb')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289fc')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289fd')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f41289fe')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580614d640ee4f41289ff')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмент Коммерческий транспорт и мелкий опт)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', 'Математический анализ', 'Математическое моделирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a00')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a01')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a02')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a03')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a04')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a05')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a06')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a07')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a08')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db580614d640ee4f4128a09')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a0a')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a0b')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a0c')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a0d')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a0e')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': ['Поисковые системы'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a0f')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern / Стажер по направлению «Анализ данных»', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a10')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a11')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db580614d640ee4f4128a12')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db580624d640ee4f4128a13')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмента Авто)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a14')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a15')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a16')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a17')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': 'Руководитель отдела аналитики', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', 'Анализ данных', 'SQL', 'Статистический анализ', 'Математический анализ', 'Базы данных'], 'salary': 'от 350\xa0000 руб. на руки', '_id': ObjectId('5db580624d640ee4f4128a18')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a19')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a1a')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': 'Разработчик BI (QlikView, Qlik Sense)', 'company_name': 'ООО Клевер Солюшнс', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', 'ООП', 'Оптимизация запросов', 'Анализ данных', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', 'Управление проектами', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', 'Работа с базами данных', 'Cognos BI'], 'salary': 'от 30\xa0000 до 110\xa0000 руб. на руки', '_id': ObjectId('5db580624d640ee4f4128a1b')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': 'Специалист по анализу изображений', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a1c')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a1d')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a1e')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', 'Бизнес-анализ', 'Системный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a1f')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a20')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a21')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a22')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a23')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': 'ООО HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db580624d640ee4f4128a24')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a25')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a26')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a27')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a28')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', 'Математическая статистика', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', 'Английский язык', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a29')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a2a')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a2b')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a2c')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a2d')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a2e')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': 'от 260\xa0000 до 370\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580624d640ee4f4128a2f')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a30')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': 'Руководитель группы сервисов / Tech Lead', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', 'Управление проектами', 'Руководство коллективом', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a31')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a32')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': 'Системный аналитик (аналитическая система)', 'company_name': 'ООО ЦРПТ (Центр развития перспективных технологий)', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a33')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': 'Системный аналитик [id20580]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', 'Системная интеграция'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a34')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': 'Системный инженер / Стажер Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a35')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a36')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a37')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': 'Разработчик научного ПО', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 100\xa0000 до 120\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580624d640ee4f4128a38')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a39')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': 'Менеджер по аналитике', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Управление проектами', 'Ведение переговоров', 'Работа в команде', 'SQL', 'ORACLE'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a3a')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', 'Английский язык', 'MS Visio'], 'salary': ' з/п не указана', '_id': ObjectId('5db580624d640ee4f4128a3b')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' Рексофт', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a3c')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': 'Инженер-аналитик', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a3d')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer / Старший разработчик', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db580634d640ee4f4128a3e')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a3f')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': 'Специалист по веб-аналитике', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', 'Английский язык', 'Google Tag Manager', 'Анализ данных'], 'salary': 'от 60\xa0000 до 120\xa0000 руб. на руки', '_id': ObjectId('5db580634d640ee4f4128a40')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-специалист', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', 'Яндекс.Метрика', 'Google Analytics', 'SEO оптимизация', 'Лидогенерация', 'Составление семантического ядра', 'Внутренняя оптимизация сайта'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a41')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a42')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', 'микросервисы'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a43')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a44')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a45')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' ЭР-Телеком', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a46')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', 'Английский язык', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a47')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': 'Ведущий аналитик (проект по DWH и BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['Бизнес-анализ', 'Оптимизация бизнес-процессов', 'Бизнес-планирование', 'Руководство коллективом', 'Аналитические исследования', 'MS Visio', 'Управление проектами', 'Ведение переговоров', 'Big Data', 'Data Science', 'Моделирование бизнес процессов'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a48')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a49')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master –  Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a4a')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a4b')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a4c')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a4d')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a4e')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a4f')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db580634d640ee4f4128a50')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a51')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Навыки межличностного общения', 'MS Excel', 'Английский язык', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a52')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': 'от 2\xa0500 до 3\xa0500 USD на руки', '_id': ObjectId('5db580634d640ee4f4128a53')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a54')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a55')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a56')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': ['Английский язык', 'Математическая статистика', 'Математический анализ', 'языки программирования'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a57')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a58')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) / Ассистент по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a59')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a5a')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Компьютерное моделирование и прогнозирование в финансовой сфере)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580634d640ee4f4128a5b')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a5c')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', 'Английский язык', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a5d')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', 'Ведение переговоров', 'DWH', 'Управление проектами', 'Business Development', 'ETL', 'AWS', 'Английский язык', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a5e')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a5f')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a60')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', 'Английский язык', 'Умение принимать решения', 'Analytical skills', 'Управление талантами', 'Time management', 'Управление персоналом', 'Подбор персонала', 'Ведение переговоров'], 'salary': ' з/п не указана', '_id': ObjectId('5db580634d640ee4f4128a61')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR, Группа Компаний', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', 'Машинное обучение', 'компьютерное зрение', 'искусственный интеллект', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a62')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': 'Бухгалтер / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['МСФО', 'Бухгалтерская отчетность', 'Английский язык', 'Основные средства', '1C: Бухгалтерия', 'Бухгалтерский учет', 'Первичная бухгалтерская документация', 'Расчет затрат', 'Российские стандарты бухгалтерского учета', 'Консолидированная бухгалтерская отчетность', 'Работа с поставщиками', 'Учет затрат'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a63')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Машинное обучение и анализ данных в социальных и экономических системах)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db580644d640ee4f4128a64')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': 'Координатор онлайн-курсов', 'company_name': 'ООО Технологии в образовании', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': ['Информационные технологии', 'Клиентоориентированность', 'Консультирование клиентов', 'Консультирование клиентов по телефону', 'Ведение переписки'], 'salary': 'до 40\xa0000 руб. на руки', '_id': ObjectId('5db580644d640ee4f4128a65')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33964564?query=data%20science>
{'vacancy_title': 'Market Researcher (аналитик рынков)', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Маркетинговые исследования', 'Английский язык', 'Маркетинговый анализ', 'Работа в команде', 'Аналитические исследования', 'конкурентный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a66')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/21963813?query=data%20science>
{'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a67')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33998747?query=data%20science>
{'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a68')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' «Nissan», Конструкторский центр', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a69')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/Старший) специалист по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a6a')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a6b')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a6c')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-разработчик', 'company_name': ' Альфа-Банк', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a6d')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a6e')}
2019-10-27 14:32:52 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a6f')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a70')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer / Программист Python', 'company_name': ' Санкт-Петербургский филиал компании ГАЗПРОМ Германия ГмбХ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a71')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a72')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web аналитик', 'company_name': 'ООО Гаудеамус', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', 'Разработка технических заданий', 'СУБД'], 'salary': 'от 90\xa0000 руб. на руки', '_id': ObjectId('5db580644d640ee4f4128a73')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a74')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31189716?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db580644d640ee4f4128a75')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' Криптонит', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': ['Разработка ПО', 'SCALA', 'Python', 'Spark', 'Веб-программирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a76')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db580644d640ee4f4128a77')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33497437?query=data%20science>
{'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', 'Управление проектами', 'MS SQL Server', 'Business English'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a78')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34031778?query=data%20science>
{'vacancy_title': 'Администратор-ассистент (комплексные решения IoT, Ml)', 'company_name': ' КОМПЛИТ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a79')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32422629?query=data%20science>
{'vacancy_title': 'Senior .NET Developer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a7a')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933697?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a7b')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813402?query=data%20science>
{'vacancy_title': 'Разработчик .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': 'от 1\xa0800 USD на руки', '_id': ObjectId('5db580644d640ee4f4128a7c')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31252237?query=data%20science>
{'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': 'от 100\xa0000 руб. на руки', '_id': ObjectId('5db580644d640ee4f4128a7d')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813435?query=data%20science>
{'vacancy_title': 'Тестировщик ПО', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': 'от 650 USD на руки', '_id': ObjectId('5db580644d640ee4f4128a7e')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933692?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a7f')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135809?query=data%20science>
{'vacancy_title': 'Senior iOS Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db580644d640ee4f4128a80')}
2019-10-27 14:32:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 14:32:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 81070,
 'downloader/request_count': 176,
 'downloader/request_method_count/GET': 176,
 'downloader/response_bytes': 6407997,
 'downloader/response_count': 176,
 'downloader/response_status_count/200': 176,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 4.918857,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 11, 32, 52, 726385),
 'item_scraped_count': 167,
 'log_count/DEBUG': 344,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 176,
 'scheduler/dequeued': 176,
 'scheduler/dequeued/memory': 176,
 'scheduler/enqueued': 176,
 'scheduler/enqueued/memory': 176,
 'start_time': datetime.datetime(2019, 10, 27, 11, 32, 47, 807528)}
2019-10-27 14:32:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 14:42:10 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:42:10 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:42:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:42:10 [scrapy.extensions.telnet] INFO: Telnet Password: f25a07f900929909
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:42:10 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:42:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:42:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58293b7091f2069098927')}
2019-10-27 14:42:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db58293b7091f2069098928')}
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db582a9b7091f2069098929')}
2019-10-27 14:42:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db582aab7091f206909892a')}
2019-10-27 14:42:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582bcb7091f206909892b')}
2019-10-27 14:42:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582bdb7091f206909892c')}
2019-10-27 14:42:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db582beb7091f206909892d')}
2019-10-27 14:42:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db582bfb7091f206909892e')}
2019-10-27 14:43:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582cbb7091f206909892f')}
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db582d6b7091f2069098930')}
2019-10-27 14:43:38 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:43:38 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:43:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:43:38 [scrapy.extensions.telnet] INFO: Telnet Password: 1f41df7e3fbc62de
2019-10-27 14:43:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:43:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:43:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:43:39 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:43:39 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:43:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:43:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ebc297669986efcd1e')}
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ebc297669986efcd1f')}
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ebc297669986efcd20')}
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ebc297669986efcd21')}
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582ebc297669986efcd22')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd23')}
2019-10-27 14:43:40 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd24')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd25')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd26')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd27')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd28')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd29')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd2a')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd2b')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd2c')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd2d')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db582ecc297669986efcd2e')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582ecc297669986efcd2f')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db582ecc297669986efcd30')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd31')}
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd32')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd33')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd34')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd35')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd36')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582ecc297669986efcd37')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db582ecc297669986efcd38')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd39')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd3a')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd3b')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd3c')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd3d')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd3e')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd3f')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd40')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd41')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd42')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd43')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd44')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582edc297669986efcd45')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/889>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/616>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd46')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd47')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd48')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582edc297669986efcd49')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd4a')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db582eec297669986efcd4b')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd4c')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd4d')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd4e')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd4f')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd50')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd51')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd52')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern / Стажер по направлению «Анализ данных»', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd53')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd54')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db582eec297669986efcd55')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd56')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': ['Поисковые системы'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd57')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмента Авто)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd58')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмент Коммерческий транспорт и мелкий опт)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', 'Математический анализ', 'Математическое моделирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd59')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd5a')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd5b')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db582eec297669986efcd5c')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': 'Руководитель отдела аналитики', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', 'Анализ данных', 'SQL', 'Статистический анализ', 'Математический анализ', 'Базы данных'], 'salary': 'от 350\xa0000 руб. на руки', '_id': ObjectId('5db582efc297669986efcd5d')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd5e')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd5f')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': 'Разработчик BI (QlikView, Qlik Sense)', 'company_name': 'ООО Клевер Солюшнс', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', 'ООП', 'Оптимизация запросов', 'Анализ данных', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', 'Управление проектами', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', 'Работа с базами данных', 'Cognos BI'], 'salary': 'от 30\xa0000 до 110\xa0000 руб. на руки', '_id': ObjectId('5db582efc297669986efcd60')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': 'Специалист по анализу изображений', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd61')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd62')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', 'Бизнес-анализ', 'Системный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd63')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd64')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd65')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': 'ООО HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db582efc297669986efcd66')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd67')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd68')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/25>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd69')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd6a')}
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd6b')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd6c')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', 'Математическая статистика', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', 'Английский язык', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd6d')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', 'Английский язык', 'MS Visio'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd6e')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd6f')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': 'Инженер-аналитик', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd70')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' Рексофт', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd71')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd72')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd73')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd74')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' з/п не указана', '_id': ObjectId('5db582efc297669986efcd75')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': 'от 260\xa0000 до 370\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582efc297669986efcd76')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd77')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': 'Системный аналитик (аналитическая система)', 'company_name': 'ООО ЦРПТ (Центр развития перспективных технологий)', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd78')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd79')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': 'Руководитель группы сервисов / Tech Lead', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', 'Управление проектами', 'Руководство коллективом', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd7a')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': 'Системный аналитик [id20580]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', 'Системная интеграция'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd7b')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd7c')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': 'Системный инженер / Стажер Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd7d')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': 'Разработчик научного ПО', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 100\xa0000 до 120\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582f0c297669986efcd7e')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd7f')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd80')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': 'Менеджер по аналитике', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Управление проектами', 'Ведение переговоров', 'Работа в команде', 'SQL', 'ORACLE'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd81')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd82')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd83')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd84')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd85')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', 'Английский язык', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd86')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd87')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': 'Ведущий аналитик (проект по DWH и BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['Бизнес-анализ', 'Оптимизация бизнес-процессов', 'Бизнес-планирование', 'Руководство коллективом', 'Аналитические исследования', 'MS Visio', 'Управление проектами', 'Ведение переговоров', 'Big Data', 'Data Science', 'Моделирование бизнес процессов'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd88')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master –  Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd89')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd8a')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f0c297669986efcd8b')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db582f1c297669986efcd8c')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd8d')}
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer / Старший разработчик', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db582f1c297669986efcd8e')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd8f')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd90')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-специалист', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', 'Яндекс.Метрика', 'Google Analytics', 'SEO оптимизация', 'Лидогенерация', 'Составление семантического ядра', 'Внутренняя оптимизация сайта'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd91')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/162>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd92')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': 'Специалист по веб-аналитике', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', 'Английский язык', 'Google Tag Manager', 'Анализ данных'], 'salary': 'от 60\xa0000 до 120\xa0000 руб. на руки', '_id': ObjectId('5db582f1c297669986efcd93')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', 'микросервисы'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd94')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd95')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' ЭР-Телеком', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd96')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': ['Английский язык', 'Математическая статистика', 'Математический анализ', 'языки программирования'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd97')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd98')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd99')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) / Ассистент по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd9a')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', 'Английский язык', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd9b')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd9c')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', 'Ведение переговоров', 'DWH', 'Управление проектами', 'Business Development', 'ETL', 'AWS', 'Английский язык', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd9d')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcd9e')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Компьютерное моделирование и прогнозирование в финансовой сфере)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582f1c297669986efcd9f')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': 'Бухгалтер / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['МСФО', 'Бухгалтерская отчетность', 'Английский язык', 'Основные средства', '1C: Бухгалтерия', 'Бухгалтерский учет', 'Первичная бухгалтерская документация', 'Расчет затрат', 'Российские стандарты бухгалтерского учета', 'Консолидированная бухгалтерская отчетность', 'Работа с поставщиками', 'Учет затрат'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcda0')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcda1')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Машинное обучение и анализ данных в социальных и экономических системах)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db582f1c297669986efcda2')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcda3')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR, Группа Компаний', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', 'Машинное обучение', 'компьютерное зрение', 'искусственный интеллект', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcda4')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', 'Английский язык', 'Умение принимать решения', 'Analytical skills', 'Управление талантами', 'Time management', 'Управление персоналом', 'Подбор персонала', 'Ведение переговоров'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f1c297669986efcda5')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': 'Координатор онлайн-курсов', 'company_name': 'ООО Технологии в образовании', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': ['Информационные технологии', 'Клиентоориентированность', 'Консультирование клиентов', 'Консультирование клиентов по телефону', 'Ведение переписки'], 'salary': 'до 40\xa0000 руб. на руки', '_id': ObjectId('5db582f2c297669986efcda6')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/139>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': 'от 2\xa0500 до 3\xa0500 USD на руки', '_id': ObjectId('5db582f2c297669986efcda7')}
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcda8')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Навыки межличностного общения', 'MS Excel', 'Английский язык', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcda9')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/Старший) специалист по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdaa')}
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' «Nissan», Конструкторский центр', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdab')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdac')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdad')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-разработчик', 'company_name': ' Альфа-Банк', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdae')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdaf')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdb0')}
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdb1')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdb2')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer / Программист Python', 'company_name': ' Санкт-Петербургский филиал компании ГАЗПРОМ Германия ГмбХ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdb3')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web аналитик', 'company_name': 'ООО Гаудеамус', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', 'Разработка технических заданий', 'СУБД'], 'salary': 'от 90\xa0000 руб. на руки', '_id': ObjectId('5db582f2c297669986efcdb4')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' Криптонит', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': ['Разработка ПО', 'SCALA', 'Python', 'Spark', 'Веб-программирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdb5')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/854>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db582f2c297669986efcdb6')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db582f2c297669986efcdb7')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 14:44:14 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:44:14 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:44:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:44:14 [scrapy.extensions.telnet] INFO: Telnet Password: bcb6a95f99e29ecc
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:44:14 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:44:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:44:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:44:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:45:17 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5834d163bfec383bbce21')}
2019-10-27 14:46:13 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:46:13 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:46:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:46:13 [scrapy.extensions.telnet] INFO: Telnet Password: fc76d632266d49a0
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:46:13 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:46:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:46:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:46:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:46:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58389b851706e78ab869b')}
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5838ab851706e78ab869c')}
2019-10-27 14:46:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5838bb851706e78ab869d')}
2019-10-27 14:46:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5838cb851706e78ab869e')}
2019-10-27 14:46:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5838cb851706e78ab869f')}
2019-10-27 14:46:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5838db851706e78ab86a0')}
2019-10-27 14:46:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5838eb851706e78ab86a1')}
2019-10-27 14:46:50 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:46:50 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:46:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:46:50 [scrapy.extensions.telnet] INFO: Telnet Password: 733dffea90835520
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:46:50 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:46:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:46:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:46:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:46:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db583af67b788ad4504e4d8')}
2019-10-27 14:46:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db583af67b788ad4504e4d9')}
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b067b788ad4504e4da')}
2019-10-27 14:46:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583b167b788ad4504e4db')}
2019-10-27 14:46:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b167b788ad4504e4dc')}
2019-10-27 14:46:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b167b788ad4504e4dd')}
2019-10-27 14:46:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b267b788ad4504e4de')}
2019-10-27 14:46:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b367b788ad4504e4df')}
2019-10-27 14:46:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b367b788ad4504e4e0')}
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b467b788ad4504e4e1')}
2019-10-27 14:47:00 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:47:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b467b788ad4504e4e2')}
2019-10-27 14:47:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b467b788ad4504e4e3')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b567b788ad4504e4e4')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db583b567b788ad4504e4e5')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583b567b788ad4504e4e6')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b567b788ad4504e4e7')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db583b667b788ad4504e4e8')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b667b788ad4504e4e9')}
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b667b788ad4504e4ea')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b667b788ad4504e4eb')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b667b788ad4504e4ec')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b767b788ad4504e4ed')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583b767b788ad4504e4ee')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b767b788ad4504e4ef')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b767b788ad4504e4f0')}
2019-10-27 14:47:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b867b788ad4504e4f1')}
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583b867b788ad4504e4f2')}
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b867b788ad4504e4f3')}
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db583b867b788ad4504e4f4')}
2019-10-27 14:48:11 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:48:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:48:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:48:11 [scrapy.extensions.telnet] INFO: Telnet Password: 06e8230bc5e9ee22
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:48:11 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:48:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:48:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe08a')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe08b')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583fcb72f8ad41bffe08c')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe08d')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe08e')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe08f')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe090')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe091')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe092')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe093')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe094')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe095')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe096')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe097')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:48:12 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe098')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db583fcb72f8ad41bffe099')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db583fcb72f8ad41bffe09a')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fcb72f8ad41bffe09b')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583fdb72f8ad41bffe09c')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe09d')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe09e')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe09f')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a0')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a1')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583fdb72f8ad41bffe0a2')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a3')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a4')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a5')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583fdb72f8ad41bffe0a6')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a7')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a8')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0a9')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0aa')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0ab')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0ac')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0ad')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0ae')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0af')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0b0')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db583fdb72f8ad41bffe0b1')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0b2')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0b3')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0b4')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0b5')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0b6')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0b7')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db583feb72f8ad41bffe0b8')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0b9')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0ba')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0bb')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0bc')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0bd')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern / Стажер по направлению «Анализ данных»', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0be')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db583feb72f8ad41bffe0bf')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c0')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': ['Поисковые системы'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c1')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c2')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмент Коммерческий транспорт и мелкий опт)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', 'Математический анализ', 'Математическое моделирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c3')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c4')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмента Авто)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c5')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c6')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', 'Математическая статистика', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', 'Английский язык', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c7')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c8')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0c9')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0ca')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0cb')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': 'Руководитель отдела аналитики', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', 'Анализ данных', 'SQL', 'Статистический анализ', 'Математический анализ', 'Базы данных'], 'salary': 'от 350\xa0000 руб. на руки', '_id': ObjectId('5db583feb72f8ad41bffe0cc')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': 'Специалист по анализу изображений', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0cd')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db583feb72f8ad41bffe0ce')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': 'Разработчик BI (QlikView, Qlik Sense)', 'company_name': 'ООО Клевер Солюшнс', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', 'ООП', 'Оптимизация запросов', 'Анализ данных', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', 'Управление проектами', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', 'Работа с базами данных', 'Cognos BI'], 'salary': 'от 30\xa0000 до 110\xa0000 руб. на руки', '_id': ObjectId('5db583feb72f8ad41bffe0cf')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', 'Бизнес-анализ', 'Системный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d0')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d1')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d2')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d3')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': 'ООО HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db583ffb72f8ad41bffe0d4')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d5')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d6')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d7')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d8')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/15478> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0d9')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', 'Английский язык', 'MS Visio'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0da')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' Рексофт', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0db')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0dc')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0dd')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0de')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0df')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e0')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e1')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': 'от 260\xa0000 до 370\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583ffb72f8ad41bffe0e2')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': 'Руководитель группы сервисов / Tech Lead', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', 'Управление проектами', 'Руководство коллективом', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e3')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e4')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': 'Системный аналитик [id20580]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', 'Системная интеграция'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e5')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': 'Системный инженер / Стажер Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e6')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e7')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': 'Разработчик научного ПО', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 100\xa0000 до 120\xa0000 руб. до вычета налогов', '_id': ObjectId('5db583ffb72f8ad41bffe0e8')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': 'Системный аналитик (аналитическая система)', 'company_name': 'ООО ЦРПТ (Центр развития перспективных технологий)', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0e9')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0ea')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': 'Менеджер по аналитике', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Управление проектами', 'Ведение переговоров', 'Работа в команде', 'SQL', 'ORACLE'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0eb')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' з/п не указана', '_id': ObjectId('5db583ffb72f8ad41bffe0ec')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': 'Инженер-аналитик', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0ed')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0ee')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0ef')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', 'Английский язык', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f0')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': 'Ведущий аналитик (проект по DWH и BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['Бизнес-анализ', 'Оптимизация бизнес-процессов', 'Бизнес-планирование', 'Руководство коллективом', 'Аналитические исследования', 'MS Visio', 'Управление проектами', 'Ведение переговоров', 'Big Data', 'Data Science', 'Моделирование бизнес процессов'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f1')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f2')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f3')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master –  Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f4')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f5')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f6')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db58400b72f8ad41bffe0f7')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0f8')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer / Старший разработчик', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db58400b72f8ad41bffe0f9')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0fa')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-специалист', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', 'Яндекс.Метрика', 'Google Analytics', 'SEO оптимизация', 'Лидогенерация', 'Составление семантического ядра', 'Внутренняя оптимизация сайта'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0fb')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0fc')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': 'Специалист по веб-аналитике', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', 'Английский язык', 'Google Tag Manager', 'Анализ данных'], 'salary': 'от 60\xa0000 до 120\xa0000 руб. на руки', '_id': ObjectId('5db58400b72f8ad41bffe0fd')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0fe')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', 'микросервисы'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe0ff')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe100')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' ЭР-Телеком', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe101')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR, Группа Компаний', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', 'Машинное обучение', 'компьютерное зрение', 'искусственный интеллект', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe102')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', 'Английский язык', 'Умение принимать решения', 'Analytical skills', 'Управление талантами', 'Time management', 'Управление персоналом', 'Подбор персонала', 'Ведение переговоров'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe103')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': 'Координатор онлайн-курсов', 'company_name': 'ООО Технологии в образовании', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': ['Информационные технологии', 'Клиентоориентированность', 'Консультирование клиентов', 'Консультирование клиентов по телефону', 'Ведение переписки'], 'salary': 'до 40\xa0000 руб. на руки', '_id': ObjectId('5db58400b72f8ad41bffe104')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe105')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe106')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': 'от 2\xa0500 до 3\xa0500 USD на руки', '_id': ObjectId('5db58400b72f8ad41bffe107')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe108')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) / Ассистент по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58400b72f8ad41bffe109')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Навыки межличностного общения', 'MS Excel', 'Английский язык', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe10a')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', 'Английский язык', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe10b')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': ['Английский язык', 'Математическая статистика', 'Математический анализ', 'языки программирования'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe10c')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe10d')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe10e')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Компьютерное моделирование и прогнозирование в финансовой сфере)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db58401b72f8ad41bffe10f')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', 'Ведение переговоров', 'DWH', 'Управление проектами', 'Business Development', 'ETL', 'AWS', 'Английский язык', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe110')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe111')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe112')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': 'Бухгалтер / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['МСФО', 'Бухгалтерская отчетность', 'Английский язык', 'Основные средства', '1C: Бухгалтерия', 'Бухгалтерский учет', 'Первичная бухгалтерская документация', 'Расчет затрат', 'Российские стандарты бухгалтерского учета', 'Консолидированная бухгалтерская отчетность', 'Работа с поставщиками', 'Учет затрат'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe113')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe114')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Машинное обучение и анализ данных в социальных и экономических системах)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db58401b72f8ad41bffe115')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/Старший) специалист по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe116')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' «Nissan», Конструкторский центр', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe117')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe118')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe119')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe11a')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-разработчик', 'company_name': ' Альфа-Банк', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe11b')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe11c')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe11d')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe11e')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe11f')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web аналитик', 'company_name': 'ООО Гаудеамус', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', 'Разработка технических заданий', 'СУБД'], 'salary': 'от 90\xa0000 руб. на руки', '_id': ObjectId('5db58401b72f8ad41bffe120')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' Криптонит', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': ['Разработка ПО', 'SCALA', 'Python', 'Spark', 'Веб-программирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe121')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db58401b72f8ad41bffe122')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31189716?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db58401b72f8ad41bffe123')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer / Программист Python', 'company_name': ' Санкт-Петербургский филиал компании ГАЗПРОМ Германия ГмбХ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' з/п не указана', '_id': ObjectId('5db58401b72f8ad41bffe124')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33497437?query=data%20science>
{'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', 'Управление проектами', 'MS SQL Server', 'Business English'], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe125')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34031778?query=data%20science>
{'vacancy_title': 'Администратор-ассистент (комплексные решения IoT, Ml)', 'company_name': ' КОМПЛИТ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe126')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33964564?query=data%20science>
{'vacancy_title': 'Market Researcher (аналитик рынков)', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Маркетинговые исследования', 'Английский язык', 'Маркетинговый анализ', 'Работа в команде', 'Аналитические исследования', 'конкурентный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe127')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/21963813?query=data%20science>
{'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe128')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813402?query=data%20science>
{'vacancy_title': 'Разработчик .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': 'от 1\xa0800 USD на руки', '_id': ObjectId('5db58402b72f8ad41bffe129')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33998747?query=data%20science>
{'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe12a')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813435?query=data%20science>
{'vacancy_title': 'Тестировщик ПО', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': 'от 650 USD на руки', '_id': ObjectId('5db58402b72f8ad41bffe12b')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3832> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33354237?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933692?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe12c')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135809?query=data%20science>
{'vacancy_title': 'Senior iOS Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe12d')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32422629?query=data%20science>
{'vacancy_title': 'Senior .NET Developer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe12e')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3358623> (referer: https://spb.hh.ru/vacancy/31015468?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/97598> (referer: https://spb.hh.ru/vacancy/33921789?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/14153> (referer: https://spb.hh.ru/vacancy/33497437?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933697?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58402b72f8ad41bffe12f')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3491569> (referer: https://spb.hh.ru/vacancy/33912707?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2558659> (referer: https://spb.hh.ru/vacancy/33813402?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1587> (referer: https://spb.hh.ru/vacancy/34031778?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31252237?query=data%20science>
{'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': 'от 100\xa0000 руб. на руки', '_id': ObjectId('5db58402b72f8ad41bffe130')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2097195> (referer: https://spb.hh.ru/vacancy/31252237?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 14:48:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 113959,
 'downloader/request_count': 265,
 'downloader/request_method_count/GET': 265,
 'downloader/response_bytes': 10886536,
 'downloader/response_count': 265,
 'downloader/response_status_count/200': 265,
 'dupefilter/filtered': 79,
 'elapsed_time_seconds': 6.789583,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 11, 48, 18, 556228),
 'item_scraped_count': 167,
 'log_count/DEBUG': 433,
 'log_count/INFO': 10,
 'request_depth_max': 10,
 'response_received_count': 265,
 'scheduler/dequeued': 265,
 'scheduler/dequeued/memory': 265,
 'scheduler/enqueued': 265,
 'scheduler/enqueued/memory': 265,
 'start_time': datetime.datetime(2019, 10, 27, 11, 48, 11, 766645)}
2019-10-27 14:48:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 14:49:22 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:49:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:49:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:49:22 [scrapy.extensions.telnet] INFO: Telnet Password: 9ff044b791d60db5
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:49:22 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:49:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:49:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db584438cdd0a411a80d282')}
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db584438cdd0a411a80d283')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db584438cdd0a411a80d284')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db584438cdd0a411a80d285')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db584438cdd0a411a80d286')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db584438cdd0a411a80d287')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db584438cdd0a411a80d288')}
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db584448cdd0a411a80d289')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db584448cdd0a411a80d28a')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db584448cdd0a411a80d28b')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db584448cdd0a411a80d28c')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db584448cdd0a411a80d28d')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db584448cdd0a411a80d28e')}
2019-10-27 14:49:24 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db584448cdd0a411a80d28f')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db584448cdd0a411a80d290')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db584448cdd0a411a80d291')}
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:46 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:53:46 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:53:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:53:46 [scrapy.extensions.telnet] INFO: Telnet Password: c49b68b47c9a7b0e
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:53:46 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:53:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:53:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd07a')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd07b')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd07c')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5854b1b419135735dd07d')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd07e')}
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd07f')}
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd080')}
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd081')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd082')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd083')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd084')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd085')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd086')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd087')}
2019-10-27 14:53:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854b1b419135735dd088')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db5854c1b419135735dd089')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5854c1b419135735dd08a')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd08b')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db5854c1b419135735dd08c')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd08d')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd08e')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5854c1b419135735dd08f')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd090')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5854c1b419135735dd091')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd092')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd093')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd094')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd095')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd096')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd097')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd098')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd099')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd09a')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd09b')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd09c')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd09d')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd09e')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5854c1b419135735dd09f')}
2019-10-27 14:53:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:53:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 14:53:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5854d1b419135735dd0a0')}
2019-10-27 14:54:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db585801b419135735dd0a1')}
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db585801b419135735dd0a2')}
2019-10-27 14:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db585871b419135735dd0a3')}
2019-10-27 14:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db585881b419135735dd0a4')}
2019-10-27 14:54:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:49 [scrapy.extensions.logstats] INFO: Crawled 59 pages (at 59 pages/min), scraped 43 items (at 43 items/min)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:54:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/22863>
{'company_url': 'http://www.protei.ru', '_id': ObjectId('5db585891b419135735dd0a5')}
2019-10-27 14:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/826770>
{'company_url': 'http://www.weareworldquant.com', '_id': ObjectId('5db5858a1b419135735dd0a6')}
2019-10-27 14:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/813602>
{'company_url': 'http://www.kofax.com', '_id': ObjectId('5db5858b1b419135735dd0a7')}
2019-10-27 14:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/889>
{'company_url': 'http://www.kodeks.ru', '_id': ObjectId('5db5858c1b419135735dd0a8')}
2019-10-27 14:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5858d1b419135735dd0a9')}
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', '_id': ObjectId('5db5858d1b419135735dd0aa')}
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5858d1b419135735dd0ab')}
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/245657>
{'company_url': 'http://www.gctrials.com/', '_id': ObjectId('5db5858d1b419135735dd0ac')}
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', '_id': ObjectId('5db5858e1b419135735dd0ad')}
2019-10-27 14:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4934> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/63225>
{'company_url': 'https://vk.com', '_id': ObjectId('5db5858e1b419135735dd0ae')}
2019-10-27 14:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2416804>
{'company_url': 'https://www.welltory.com', '_id': ObjectId('5db5858e1b419135735dd0af')}
2019-10-27 14:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/168421>
{'company_url': 'http://www.wandisco.com', '_id': ObjectId('5db5858f1b419135735dd0b0')}
2019-10-27 14:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1040131>
{'company_url': 'http://segmento.ru', '_id': ObjectId('5db585901b419135735dd0b1')}
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3931730>
{'company_url': 'http://GamesFactoryTalents.com', '_id': ObjectId('5db585911b419135735dd0b2')}
2019-10-27 14:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1245452>
{'company_url': 'http://www.cloudlinux.com', '_id': ObjectId('5db585921b419135735dd0b3')}
2019-10-27 14:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 14:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/834307>
{'company_url': 'http://www.dm-matrix.com', '_id': ObjectId('5db585931b419135735dd0b4')}
2019-10-27 14:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1263188>
{'company_url': 'http://www.weigandt-consulting.com', '_id': ObjectId('5db585931b419135735dd0b5')}
2019-10-27 14:55:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2625959>
{'company_url': 'http://www.idrnd.ai', '_id': ObjectId('5db585941b419135735dd0b6')}
2019-10-27 14:55:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/616>
{'company_url': 'http://lancktele.com', '_id': ObjectId('5db585941b419135735dd0b7')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db585951b419135735dd0b8')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0b9')}
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0ba')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0bb')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0bc')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0bd')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0be')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern / Стажер по направлению «Анализ данных»', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0bf')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': ['Поисковые системы'], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0c0')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0c1')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db585951b419135735dd0c2')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмент Коммерческий транспорт и мелкий опт)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', 'Математический анализ', 'Математическое моделирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db585951b419135735dd0c3')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db585961b419135735dd0c4')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585961b419135735dd0c5')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмента Авто)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' з/п не указана', '_id': ObjectId('5db585961b419135735dd0c6')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' з/п не указана', '_id': ObjectId('5db585961b419135735dd0c7')}
2019-10-27 14:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585961b419135735dd0c8')}
2019-10-27 14:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/45743>
{'company_url': 'http://www.oct-clinicaltrials.com/', '_id': ObjectId('5db585961b419135735dd0c9')}
2019-10-27 14:55:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/9281>
{'company_url': 'http://www.jetbrains.com', '_id': ObjectId('5db585981b419135735dd0ca')}
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/6769>
{'company_url': 'http://www.epam-group.ru', '_id': ObjectId('5db585991b419135735dd0cb')}
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/314182>
{'company_url': 'http://www.saber3d.com', '_id': ObjectId('5db585991b419135735dd0cc')}
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': 'Специалист по анализу изображений', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585991b419135735dd0cd')}
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585991b419135735dd0ce')}
2019-10-27 14:55:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0cf')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0d0')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': 'Руководитель отдела аналитики', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', 'Анализ данных', 'SQL', 'Статистический анализ', 'Математический анализ', 'Базы данных'], 'salary': 'от 350\xa0000 руб. на руки', '_id': ObjectId('5db5859a1b419135735dd0d1')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0d2')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': 'Разработчик BI (QlikView, Qlik Sense)', 'company_name': 'ООО Клевер Солюшнс', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', 'ООП', 'Оптимизация запросов', 'Анализ данных', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', 'Управление проектами', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', 'Работа с базами данных', 'Cognos BI'], 'salary': 'от 30\xa0000 до 110\xa0000 руб. на руки', '_id': ObjectId('5db5859a1b419135735dd0d3')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0d4')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0d5')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', 'Бизнес-анализ', 'Системный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0d6')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0d7')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0d8')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': 'ООО HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db5859a1b419135735dd0d9')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0da')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/15478> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0db')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0dc')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', 'Математическая статистика', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', 'Английский язык', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0dd')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0de')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0df')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e0')}
2019-10-27 14:55:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/15478> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e1')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e2')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e3')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e4')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e5')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': 'от 260\xa0000 до 370\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5859a1b419135735dd0e6')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e7')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e8')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': 'Руководитель группы сервисов / Tech Lead', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', 'Управление проектами', 'Руководство коллективом', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0e9')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': 'Системный аналитик [id20580]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', 'Системная интеграция'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0ea')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0eb')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': 'Разработчик научного ПО', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 100\xa0000 до 120\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5859a1b419135735dd0ec')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': 'Системный инженер / Стажер Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0ed')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': 'Системный аналитик (аналитическая система)', 'company_name': 'ООО ЦРПТ (Центр развития перспективных технологий)', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0ee')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0ef')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': 'Менеджер по аналитике', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Управление проектами', 'Ведение переговоров', 'Работа в команде', 'SQL', 'ORACLE'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0f0')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' з/п не указана', '_id': ObjectId('5db5859a1b419135735dd0f1')}
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2160858>
{'company_url': 'http://qleversolutions.ru/', '_id': ObjectId('5db5859b1b419135735dd0f2')}
2019-10-27 14:56:36 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:56:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:56:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:56:37 [scrapy.extensions.telnet] INFO: Telnet Password: a33f6aa7f3cec5d6
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:56:37 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:56:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:56:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:56:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:56:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db585fcfbe96efd8f7b2b89')}
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58601fbe96efd8f7b2b8a')}
2019-10-27 14:56:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db58603fbe96efd8f7b2b8b')}
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db58604fbe96efd8f7b2b8c')}
2019-10-27 14:56:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db58605fbe96efd8f7b2b8d')}
2019-10-27 14:56:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db58609fbe96efd8f7b2b8e')}
2019-10-27 14:56:57 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/3529> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:56:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5860afbe96efd8f7b2b8f')}
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5860bfbe96efd8f7b2b90')}
2019-10-27 14:56:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5860bfbe96efd8f7b2b91')}
2019-10-27 14:57:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5860cfbe96efd8f7b2b92')}
2019-10-27 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5860dfbe96efd8f7b2b93')}
2019-10-27 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5860dfbe96efd8f7b2b94')}
2019-10-27 14:57:12 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:57:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:57:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:57:12 [scrapy.extensions.telnet] INFO: Telnet Password: 9876dabdcd5d6e1f
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:57:12 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:57:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:57:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:57:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:57:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5861b98f6d6670452863a')}
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:47 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:57:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:57:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:57:47 [scrapy.extensions.telnet] INFO: Telnet Password: a5769d86ce870c23
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:57:47 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:57:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:57:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:57:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5863cb495880b03d182ed')}
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5864cb495880b03d182ee')}
2019-10-27 14:58:08 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5864db495880b03d182ef')}
2019-10-27 14:58:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db58650b495880b03d182f0')}
2019-10-27 14:58:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db58653b495880b03d182f1')}
2019-10-27 14:58:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58654b495880b03d182f2')}
2019-10-27 14:58:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db58656b495880b03d182f3')}
2019-10-27 14:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db58657b495880b03d182f4')}
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5865bb495880b03d182f5')}
2019-10-27 14:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5865bb495880b03d182f6')}
2019-10-27 14:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5865cb495880b03d182f7')}
2019-10-27 14:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db5865db495880b03d182f8')}
2019-10-27 14:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db5865eb495880b03d182f9')}
2019-10-27 14:58:27 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5865fb495880b03d182fa')}
2019-10-27 14:58:28 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:58:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58664b495880b03d182fb')}
2019-10-27 14:58:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db58666b495880b03d182fc')}
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db58668b495880b03d182fd')}
2019-10-27 14:58:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db5866bb495880b03d182fe')}
2019-10-27 14:58:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5866db495880b03d182ff')}
2019-10-27 14:58:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5866eb495880b03d18300')}
2019-10-27 14:58:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db5866fb495880b03d18301')}
2019-10-27 14:58:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db58671b495880b03d18302')}
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db58673b495880b03d18303')}
2019-10-27 14:58:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db58674b495880b03d18304')}
2019-10-27 14:58:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58675b495880b03d18305')}
2019-10-27 14:58:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58676b495880b03d18306')}
2019-10-27 14:58:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db58679b495880b03d18307')}
2019-10-27 14:58:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db58679b495880b03d18308')}
2019-10-27 14:58:52 [scrapy.extensions.logstats] INFO: Crawled 40 pages (at 40 pages/min), scraped 28 items (at 28 items/min)
2019-10-27 14:58:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db5867cb495880b03d18309')}
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5867db495880b03d1830a')}
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5867eb495880b03d1830b')}
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5867eb495880b03d1830c')}
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db5867eb495880b03d1830d')}
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db5867eb495880b03d1830e')}
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db5867fb495880b03d1830f')}
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5867fb495880b03d18310')}
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db5867fb495880b03d18311')}
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5867fb495880b03d18312')}
2019-10-27 14:58:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5867fb495880b03d18313')}
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db58680b495880b03d18314')}
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db58680b495880b03d18315')}
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db58680b495880b03d18316')}
2019-10-27 14:58:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db58681b495880b03d18317')}
2019-10-27 14:58:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:58:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:58:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:58:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:58:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:01:09 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:01:09 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:01:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:01:09 [scrapy.extensions.telnet] INFO: Telnet Password: a8e4d1b9c93841f5
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:01:09 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:01:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:01:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:01:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db587066d4ea946f1314960')}
2019-10-27 15:01:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5870a6d4ea946f1314961')}
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5870b6d4ea946f1314962')}
2019-10-27 15:01:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5870c6d4ea946f1314963')}
2019-10-27 15:01:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5870c6d4ea946f1314964')}
2019-10-27 15:01:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db5870d6d4ea946f1314965')}
2019-10-27 15:01:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5870d6d4ea946f1314966')}
2019-10-27 15:01:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5870e6d4ea946f1314967')}
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db5870e6d4ea946f1314968')}
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5870e6d4ea946f1314969')}
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5870e6d4ea946f131496a')}
2019-10-27 15:01:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5870e6d4ea946f131496b')}
2019-10-27 15:01:39 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:01:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:01:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:01:39 [scrapy.extensions.telnet] INFO: Telnet Password: 30d9584b88ca5166
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:01:39 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:01:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:01:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872465033d79ea161202')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872465033d79ea161203')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872465033d79ea161204')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872465033d79ea161205')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5872465033d79ea161206')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5872465033d79ea161207')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5872465033d79ea161208')}
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea161209')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea16120a')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea16120b')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea16120c')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea16120d')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea16120e')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea16120f')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea161210')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5872565033d79ea161211')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db5872565033d79ea161212')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db5872565033d79ea161213')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea161214')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea161215')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db5872565033d79ea161216')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5872565033d79ea161217')}
2019-10-27 15:02:36 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:02:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:02:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:02:36 [scrapy.extensions.telnet] INFO: Telnet Password: 434922feabf4e261
2019-10-27 15:02:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:02:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:02:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:02:37 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:02:37 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:02:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:02:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a08')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a09')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a0a')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a0b')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a0c')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a0d')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5875e2126e491169e6a0e')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a0f')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a10')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a11')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a12')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a13')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a14')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a15')}
2019-10-27 15:02:38 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a16')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5875e2126e491169e6a17')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db5875e2126e491169e6a18')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db5875e2126e491169e6a19')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a1a')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a1b')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a1c')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a1d')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5875e2126e491169e6a1e')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a1f')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a20')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a21')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a22')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a23')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a24')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a25')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875e2126e491169e6a26')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875f2126e491169e6a27')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875f2126e491169e6a28')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875f2126e491169e6a29')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875f2126e491169e6a2a')}
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5875f2126e491169e6a2b')}
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875f2126e491169e6a2c')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5875f2126e491169e6a2d')}
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5875f2126e491169e6a2e')}
2019-10-27 15:07:43 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:07:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:07:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:07:43 [scrapy.extensions.telnet] INFO: Telnet Password: 549f100310a7c8ad
2019-10-27 15:07:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:07:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:07:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:07:44 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:07:44 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:07:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:07:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588905ba1f5c468194cd7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588915ba1f5c468194cd8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1947330>, '_id': ObjectId('5db588915ba1f5c468194cd9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1947330>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588915ba1f5c468194cda')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588915ba1f5c468194cdb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588915ba1f5c468194cdc')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588915ba1f5c468194cdd')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588915ba1f5c468194cde')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, '_id': ObjectId('5db588915ba1f5c468194cdf')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/44458>, '_id': ObjectId('5db588915ba1f5c468194ce0')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44458>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/826770>, '_id': ObjectId('5db588915ba1f5c468194ce1')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/826770>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588915ba1f5c468194ce2')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2768>, '_id': ObjectId('5db588915ba1f5c468194ce3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2768>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2061860>, '_id': ObjectId('5db588915ba1f5c468194ce4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2061860>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/83746>, '_id': ObjectId('5db588915ba1f5c468194ce5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/83746>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588915ba1f5c468194ce6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/22863>, '_id': ObjectId('5db588915ba1f5c468194ce7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/22863>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/889>, '_id': ObjectId('5db588915ba1f5c468194ce8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/889>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/1528668>, '_id': ObjectId('5db588915ba1f5c468194ce9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1528668>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/813602>, '_id': ObjectId('5db588915ba1f5c468194cea')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/813602>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588915ba1f5c468194ceb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588915ba1f5c468194cec')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588915ba1f5c468194ced')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/616>, '_id': ObjectId('5db588915ba1f5c468194cee')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/616>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588925ba1f5c468194cef')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/63225>, '_id': ObjectId('5db588925ba1f5c468194cf0')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/63225>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588925ba1f5c468194cf1')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588925ba1f5c468194cf2')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588925ba1f5c468194cf3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2416804>, '_id': ObjectId('5db588925ba1f5c468194cf4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2416804>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3931730>, '_id': ObjectId('5db588925ba1f5c468194cf5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3931730>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/168421>, '_id': ObjectId('5db588925ba1f5c468194cf6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168421>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588925ba1f5c468194cf7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1245452>, '_id': ObjectId('5db588925ba1f5c468194cf8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1245452>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2625959>, '_id': ObjectId('5db588925ba1f5c468194cf9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2625959>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588925ba1f5c468194cfa')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588925ba1f5c468194cfb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588925ba1f5c468194cfc')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588925ba1f5c468194cfd')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/168307>, '_id': ObjectId('5db588925ba1f5c468194cfe')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/314182>, '_id': ObjectId('5db588925ba1f5c468194cff')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/314182>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588925ba1f5c468194d00')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2618793>, '_id': ObjectId('5db588925ba1f5c468194d01')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2618793>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588925ba1f5c468194d02')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588925ba1f5c468194d03')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588925ba1f5c468194d04')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588935ba1f5c468194d05')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/681575>, '_id': ObjectId('5db588935ba1f5c468194d06')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/681575>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588935ba1f5c468194d07')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588935ba1f5c468194d08')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588935ba1f5c468194d09')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analytics Intern / Стажер по направлению «Анализ данных»', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588935ba1f5c468194d0a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': ['Поисковые системы'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2232>, '_id': ObjectId('5db588935ba1f5c468194d0b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2232>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588935ba1f5c468194d0c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588935ba1f5c468194d0d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead (Сегмент Коммерческий транспорт и мелкий опт)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', 'Математический анализ', 'Математическое моделирование'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588935ba1f5c468194d0e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead (Сегмента Авто)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588935ba1f5c468194d0f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588935ba1f5c468194d10')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588935ba1f5c468194d11')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588935ba1f5c468194d12')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d13')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Руководитель отдела аналитики', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', 'Анализ данных', 'SQL', 'Статистический анализ', 'Математический анализ', 'Базы данных'], 'salary': 'от 350\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588935ba1f5c468194d14')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d15')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Специалист по анализу изображений', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588935ba1f5c468194d16')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Разработчик BI (QlikView, Qlik Sense)', 'company_name': 'ООО Клевер Солюшнс', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', 'ООП', 'Оптимизация запросов', 'Анализ данных', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', 'Управление проектами', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', 'Работа с базами данных', 'Cognos BI'], 'salary': 'от 30\xa0000 до 110\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2160858>, '_id': ObjectId('5db588935ba1f5c468194d17')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2160858>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588935ba1f5c468194d18')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d19')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', 'Бизнес-анализ', 'Системный анализ'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588935ba1f5c468194d1a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Chief  Officer', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588935ba1f5c468194d1b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d1c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588945ba1f5c468194d1d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior C#. Net Developer', 'company_name': 'ООО HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': 'от 3\xa0000 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/2661749>, '_id': ObjectId('5db588945ba1f5c468194d1e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2661749>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588945ba1f5c468194d1f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Head Of Growth', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588945ba1f5c468194d20')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', 'Математическая статистика', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', 'Английский язык', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3425814>, '_id': ObjectId('5db588945ba1f5c468194d21')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3425814>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588945ba1f5c468194d22')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588945ba1f5c468194d23')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/130986>, '_id': ObjectId('5db588945ba1f5c468194d24')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/130986>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588945ba1f5c468194d25')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588945ba1f5c468194d26')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588945ba1f5c468194d27')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588945ba1f5c468194d28')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': 'от 260\xa0000 до 370\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588945ba1f5c468194d29')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/638130>, '_id': ObjectId('5db588945ba1f5c468194d2a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/638130>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588945ba1f5c468194d2b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Руководитель группы сервисов / Tech Lead', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', 'Управление проектами', 'Руководство коллективом', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588945ba1f5c468194d2c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588945ba1f5c468194d2d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Системный аналитик (аналитическая система)', 'company_name': 'ООО ЦРПТ (Центр развития перспективных технологий)', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3156389>, '_id': ObjectId('5db588945ba1f5c468194d2e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3156389>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Системный инженер / Стажер Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588945ba1f5c468194d2f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6301>, '_id': ObjectId('5db588945ba1f5c468194d30')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6301>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Системный аналитик [id20580]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', 'Системная интеграция'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588945ba1f5c468194d31')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588945ba1f5c468194d32')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Менеджер по аналитике', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Управление проектами', 'Ведение переговоров', 'Работа в команде', 'SQL', 'ORACLE'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588945ba1f5c468194d33')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Разработчик научного ПО', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 100\xa0000 до 120\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588955ba1f5c468194d34')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588955ba1f5c468194d35')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', 'Английский язык', 'MS Visio'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/213004>, '_id': ObjectId('5db588955ba1f5c468194d36')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/213004>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer', 'company_name': ' Рексофт', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', 'Английский язык'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3984>, '_id': ObjectId('5db588955ba1f5c468194d37')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3984>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588955ba1f5c468194d38')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588955ba1f5c468194d39')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Инженер-аналитик', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588955ba1f5c468194d3a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/86298>, '_id': ObjectId('5db588955ba1f5c468194d3b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/86298>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588955ba1f5c468194d3c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', 'Английский язык', 'C/C++'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6500>, '_id': ObjectId('5db588955ba1f5c468194d3d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6500>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Ведущий аналитик (проект по DWH и BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['Бизнес-анализ', 'Оптимизация бизнес-процессов', 'Бизнес-планирование', 'Руководство коллективом', 'Аналитические исследования', 'MS Visio', 'Управление проектами', 'Ведение переговоров', 'Big Data', 'Data Science', 'Моделирование бизнес процессов'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/139>, '_id': ObjectId('5db588955ba1f5c468194d3e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/139>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588955ba1f5c468194d3f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product IT Manager/ SCRUM Master –  Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1186>, '_id': ObjectId('5db588955ba1f5c468194d40')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1186>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588955ba1f5c468194d41')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588955ba1f5c468194d42')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588955ba1f5c468194d43')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588955ba1f5c468194d44')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d45')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d46')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET developer / Старший разработчик', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d47')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior SEO-специалист', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', 'Яндекс.Метрика', 'Google Analytics', 'SEO оптимизация', 'Лидогенерация', 'Составление семантического ядра', 'Внутренняя оптимизация сайта'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/633069>, '_id': ObjectId('5db588955ba1f5c468194d48')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/633069>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d49')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588955ba1f5c468194d4a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588955ba1f5c468194d4b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Специалист по веб-аналитике', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', 'Английский язык', 'Google Tag Manager', 'Анализ данных'], 'salary': 'от 60\xa0000 до 120\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2559110>, '_id': ObjectId('5db588965ba1f5c468194d4c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2559110>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', 'микросервисы'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/654435>, '_id': ObjectId('5db588965ba1f5c468194d4d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/654435>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': ['Английский язык', 'Математическая статистика', 'Математический анализ', 'языки программирования'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3455245>, '_id': ObjectId('5db588965ba1f5c468194d4e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3455245>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps engineer', 'company_name': ' ЭР-Телеком', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/44272>, '_id': ObjectId('5db588965ba1f5c468194d4f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44272>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588965ba1f5c468194d50')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588965ba1f5c468194d51')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Trials Assistant (CTA) / Ассистент по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588965ba1f5c468194d52')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2617114>, '_id': ObjectId('5db588965ba1f5c468194d53')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2617114>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', 'Английский язык', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588965ba1f5c468194d54')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588965ba1f5c468194d55')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Бухгалтер / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['МСФО', 'Бухгалтерская отчетность', 'Английский язык', 'Основные средства', '1C: Бухгалтерия', 'Бухгалтерский учет', 'Первичная бухгалтерская документация', 'Расчет затрат', 'Российские стандарты бухгалтерского учета', 'Консолидированная бухгалтерская отчетность', 'Работа с поставщиками', 'Учет затрат'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588965ba1f5c468194d56')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Научный сотрудник (Компьютерное моделирование и прогнозирование в финансовой сфере)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': 'от 100\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588965ba1f5c468194d57')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', 'Ведение переговоров', 'DWH', 'Управление проектами', 'Business Development', 'ETL', 'AWS', 'Английский язык', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588965ba1f5c468194d58')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/854>, '_id': ObjectId('5db588965ba1f5c468194d59')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/854>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Научный сотрудник (Машинное обучение и анализ данных в социальных и экономических системах)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': 'от 100\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588965ba1f5c468194d5a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588965ba1f5c468194d5b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR, Группа Компаний', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', 'Машинное обучение', 'компьютерное зрение', 'искусственный интеллект', 'Анализ данных'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/217737>, '_id': ObjectId('5db588965ba1f5c468194d5c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/217737>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588965ba1f5c468194d5d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', 'Английский язык', 'Умение принимать решения', 'Analytical skills', 'Управление талантами', 'Time management', 'Управление персоналом', 'Подбор персонала', 'Ведение переговоров'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588965ba1f5c468194d5e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Координатор онлайн-курсов', 'company_name': 'ООО Технологии в образовании', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': ['Информационные технологии', 'Клиентоориентированность', 'Консультирование клиентов', 'Консультирование клиентов по телефону', 'Ведение переписки'], 'salary': 'до 40\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2869446>, '_id': ObjectId('5db588965ba1f5c468194d5f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2869446>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': 'от 2\xa0500 до 3\xa0500 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/3289413>, '_id': ObjectId('5db588965ba1f5c468194d60')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3289413>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/62126>, '_id': ObjectId('5db588965ba1f5c468194d61')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/62126>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Finance Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Навыки межличностного общения', 'MS Excel', 'Английский язык', 'SQL'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588965ba1f5c468194d62')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate / (Senior/Старший) специалист по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588965ba1f5c468194d63')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' «Nissan», Конструкторский центр', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, '_id': ObjectId('5db588975ba1f5c468194d64')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d65')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Android Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588975ba1f5c468194d66')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588975ba1f5c468194d67')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588975ba1f5c468194d68')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java-разработчик', 'company_name': ' Альфа-Банк', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/80>, '_id': ObjectId('5db588975ba1f5c468194d69')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/80>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588975ba1f5c468194d6a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588975ba1f5c468194d6b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Web аналитик', 'company_name': 'ООО Гаудеамус', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', 'Разработка технических заданий', 'СУБД'], 'salary': 'от 90\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/97598>, '_id': ObjectId('5db588975ba1f5c468194d6c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/97598>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d6d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python developer / Программист Python', 'company_name': ' Санкт-Петербургский филиал компании ГАЗПРОМ Германия ГмбХ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3358623>, '_id': ObjectId('5db588975ba1f5c468194d6e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3358623>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': 'от 200\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d6f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scala Developer', 'company_name': ' Криптонит', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': ['Разработка ПО', 'SCALA', 'Python', 'Spark', 'Веб-программирование'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3491569>, '_id': ObjectId('5db588975ba1f5c468194d70')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3491569>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', 'Управление проектами', 'MS SQL Server', 'Business English'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/14153>, '_id': ObjectId('5db588975ba1f5c468194d71')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/14153>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d72')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Администратор-ассистент (комплексные решения IoT, Ml)', 'company_name': ' КОМПЛИТ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1587>, '_id': ObjectId('5db588975ba1f5c468194d73')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1587>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588975ba1f5c468194d74')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Разработчик .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': 'от 1\xa0800 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588975ba1f5c468194d75')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Market Researcher (аналитик рынков)', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Маркетинговые исследования', 'Английский язык', 'Маркетинговый анализ', 'Работа в команде', 'Аналитические исследования', 'конкурентный анализ'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588975ba1f5c468194d76')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Тестировщик ПО', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': 'от 650 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588975ba1f5c468194d77')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior iOS Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588975ba1f5c468194d78')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588975ba1f5c468194d79')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588975ba1f5c468194d7a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET Developer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588975ba1f5c468194d7b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588975ba1f5c468194d7c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:52 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': 'от 100\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2097195>, '_id': ObjectId('5db588985ba1f5c468194d7d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2097195>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 15:07:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 81070,
 'downloader/request_count': 176,
 'downloader/request_method_count/GET': 176,
 'downloader/response_bytes': 6407646,
 'downloader/response_count': 176,
 'downloader/response_status_count/200': 176,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 7.996969,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 12, 7, 52, 42862),
 'log_count/DEBUG': 177,
 'log_count/ERROR': 167,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 176,
 'scheduler/dequeued': 176,
 'scheduler/dequeued/memory': 176,
 'scheduler/enqueued': 176,
 'scheduler/enqueued/memory': 176,
 'start_time': datetime.datetime(2019, 10, 27, 12, 7, 44, 45893)}
2019-10-27 15:07:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 15:09:09 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:09:09 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:09:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:09:09 [scrapy.extensions.telnet] INFO: Telnet Password: 2fdfaeefb442fbb4
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:09:09 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:09:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:09:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:09:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588e6e31518ac4c5634e3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588e6e31518ac4c5634e4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588e6e31518ac4c5634e5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588e6e31518ac4c5634e6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1947330>, '_id': ObjectId('5db588e6e31518ac4c5634e7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1947330>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e6e31518ac4c5634e8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588e6e31518ac4c5634e9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588e6e31518ac4c5634ea')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/44458>, '_id': ObjectId('5db588e6e31518ac4c5634eb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44458>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2061860>, '_id': ObjectId('5db588e6e31518ac4c5634ec')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2061860>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, '_id': ObjectId('5db588e6e31518ac4c5634ed')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2768>, '_id': ObjectId('5db588e6e31518ac4c5634ee')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2768>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/826770>, '_id': ObjectId('5db588e6e31518ac4c5634ef')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/826770>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/83746>, '_id': ObjectId('5db588e6e31518ac4c5634f0')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/83746>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588e6e31518ac4c5634f1')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/22863>, '_id': ObjectId('5db588e6e31518ac4c5634f2')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/22863>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588e6e31518ac4c5634f3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588e6e31518ac4c5634f4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/889>, '_id': ObjectId('5db588e6e31518ac4c5634f5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/889>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/1528668>, '_id': ObjectId('5db588e7e31518ac4c5634f6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1528668>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/813602>, '_id': ObjectId('5db588e7e31518ac4c5634f7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/813602>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588e7e31518ac4c5634f8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/616>, '_id': ObjectId('5db588e7e31518ac4c5634f9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/616>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588e7e31518ac4c5634fa')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1245452>, '_id': ObjectId('5db588e7e31518ac4c5634fb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1245452>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588e7e31518ac4c5634fc')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2416804>, '_id': ObjectId('5db588e7e31518ac4c5634fd')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2416804>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588e7e31518ac4c5634fe')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/63225>, '_id': ObjectId('5db588e7e31518ac4c5634ff')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/63225>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588e7e31518ac4c563500')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3931730>, '_id': ObjectId('5db588e7e31518ac4c563501')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3931730>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/168421>, '_id': ObjectId('5db588e7e31518ac4c563502')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168421>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2625959>, '_id': ObjectId('5db588e7e31518ac4c563503')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2625959>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588e7e31518ac4c563504')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e7e31518ac4c563505')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588e7e31518ac4c563506')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588e7e31518ac4c563507')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e7e31518ac4c563508')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/168307>, '_id': ObjectId('5db588e7e31518ac4c563509')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588e7e31518ac4c56350a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588e7e31518ac4c56350b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/314182>, '_id': ObjectId('5db588e7e31518ac4c56350c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/314182>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588e8e31518ac4c56350d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588e8e31518ac4c56350e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588e8e31518ac4c56350f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588e8e31518ac4c563510')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/681575>, '_id': ObjectId('5db588e8e31518ac4c563511')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/681575>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588e8e31518ac4c563512')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588e8e31518ac4c563513')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588e8e31518ac4c563514')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588e8e31518ac4c563515')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588e8e31518ac4c563516')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2618793>, '_id': ObjectId('5db588e8e31518ac4c563517')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2618793>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588e8e31518ac4c563518')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588e8e31518ac4c563519')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analytics Intern / Стажер по направлению «Анализ данных»', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588e8e31518ac4c56351a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead (Сегмента Авто)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e8e31518ac4c56351b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': ['Поисковые системы'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2232>, '_id': ObjectId('5db588e8e31518ac4c56351c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2232>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead (Сегмент Коммерческий транспорт и мелкий опт)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', 'Математический анализ', 'Математическое моделирование'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e8e31518ac4c56351d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588e8e31518ac4c56351e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588e8e31518ac4c56351f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Руководитель отдела аналитики', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', 'Анализ данных', 'SQL', 'Статистический анализ', 'Математический анализ', 'Базы данных'], 'salary': 'от 350\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588e8e31518ac4c563520')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Разработчик BI (QlikView, Qlik Sense)', 'company_name': 'ООО Клевер Солюшнс', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', 'ООП', 'Оптимизация запросов', 'Анализ данных', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', 'Управление проектами', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', 'Работа с базами данных', 'Cognos BI'], 'salary': 'от 30\xa0000 до 110\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2160858>, '_id': ObjectId('5db588e8e31518ac4c563521')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2160858>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588e8e31518ac4c563522')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Специалист по анализу изображений', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588e8e31518ac4c563523')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563524')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563525')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563526')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563527')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Chief  Officer', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588e9e31518ac4c563528')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', 'Бизнес-анализ', 'Системный анализ'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588e9e31518ac4c563529')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c56352a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c56352b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Head Of Growth', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588e9e31518ac4c56352c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior C#. Net Developer', 'company_name': 'ООО HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': 'от 3\xa0000 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/2661749>, '_id': ObjectId('5db588e9e31518ac4c56352d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2661749>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588e9e31518ac4c56352e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/130986>, '_id': ObjectId('5db588e9e31518ac4c56352f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/130986>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', 'Математическая статистика', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', 'Английский язык', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3425814>, '_id': ObjectId('5db588e9e31518ac4c563530')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3425814>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588e9e31518ac4c563531')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer', 'company_name': ' Рексофт', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', 'Английский язык'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3984>, '_id': ObjectId('5db588e9e31518ac4c563532')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3984>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588e9e31518ac4c563533')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/86298>, '_id': ObjectId('5db588e9e31518ac4c563534')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/86298>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588e9e31518ac4c563535')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588e9e31518ac4c563536')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/638130>, '_id': ObjectId('5db588e9e31518ac4c563537')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/638130>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': 'от 260\xa0000 до 370\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588e9e31518ac4c563538')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588e9e31518ac4c563539')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Системный аналитик (аналитическая система)', 'company_name': 'ООО ЦРПТ (Центр развития перспективных технологий)', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3156389>, '_id': ObjectId('5db588eae31518ac4c56353a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3156389>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588eae31518ac4c56353b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588eae31518ac4c56353c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Руководитель группы сервисов / Tech Lead', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', 'Управление проектами', 'Руководство коллективом', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588eae31518ac4c56353d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Системный аналитик [id20580]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', 'Системная интеграция'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588eae31518ac4c56353e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Системный инженер / Стажер Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588eae31518ac4c56353f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588eae31518ac4c563540')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Разработчик научного ПО', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 100\xa0000 до 120\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588eae31518ac4c563541')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6301>, '_id': ObjectId('5db588eae31518ac4c563542')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6301>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Менеджер по аналитике', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Управление проектами', 'Ведение переговоров', 'Работа в команде', 'SQL', 'ORACLE'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588eae31518ac4c563543')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588eae31518ac4c563544')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Инженер-аналитик', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588eae31518ac4c563545')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', 'Английский язык', 'MS Visio'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/213004>, '_id': ObjectId('5db588eae31518ac4c563546')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/213004>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', 'Английский язык', 'C/C++'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/6500>, '_id': ObjectId('5db588eae31518ac4c563547')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6500>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Ведущий аналитик (проект по DWH и BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['Бизнес-анализ', 'Оптимизация бизнес-процессов', 'Бизнес-планирование', 'Руководство коллективом', 'Аналитические исследования', 'MS Visio', 'Управление проектами', 'Ведение переговоров', 'Big Data', 'Data Science', 'Моделирование бизнес процессов'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/139>, '_id': ObjectId('5db588eae31518ac4c563548')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/139>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588eae31518ac4c563549')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588eae31518ac4c56354a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588eae31518ac4c56354b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588eae31518ac4c56354c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588eae31518ac4c56354d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588eae31518ac4c56354e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588eae31518ac4c56354f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product IT Manager/ SCRUM Master –  Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1186>, '_id': ObjectId('5db588ebe31518ac4c563550')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1186>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588ebe31518ac4c563551')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588ebe31518ac4c563552')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET developer / Старший разработчик', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588ebe31518ac4c563553')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ebe31518ac4c563554')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Специалист по веб-аналитике', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', 'Английский язык', 'Google Tag Manager', 'Анализ данных'], 'salary': 'от 60\xa0000 до 120\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2559110>, '_id': ObjectId('5db588ebe31518ac4c563555')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2559110>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588ebe31518ac4c563556')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps engineer', 'company_name': ' ЭР-Телеком', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/44272>, '_id': ObjectId('5db588ebe31518ac4c563557')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44272>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Finance Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Навыки межличностного общения', 'MS Excel', 'Английский язык', 'SQL'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588ebe31518ac4c563558')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588ebe31518ac4c563559')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior SEO-специалист', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', 'Яндекс.Метрика', 'Google Analytics', 'SEO оптимизация', 'Лидогенерация', 'Составление семантического ядра', 'Внутренняя оптимизация сайта'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/633069>, '_id': ObjectId('5db588ebe31518ac4c56355a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/633069>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588ebe31518ac4c56355b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', 'микросервисы'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/654435>, '_id': ObjectId('5db588ebe31518ac4c56355c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/654435>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ebe31518ac4c56355d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Trials Assistant (CTA) / Ассистент по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588ebe31518ac4c56355e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', 'Английский язык', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588ebe31518ac4c56355f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Научный сотрудник (Компьютерное моделирование и прогнозирование в финансовой сфере)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': 'от 100\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588ebe31518ac4c563560')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': ['Английский язык', 'Математическая статистика', 'Математический анализ', 'языки программирования'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3455245>, '_id': ObjectId('5db588ebe31518ac4c563561')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3455245>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2617114>, '_id': ObjectId('5db588ebe31518ac4c563562')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2617114>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588ebe31518ac4c563563')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/854>, '_id': ObjectId('5db588ebe31518ac4c563564')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/854>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ebe31518ac4c563565')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', 'Ведение переговоров', 'DWH', 'Управление проектами', 'Business Development', 'ETL', 'AWS', 'Английский язык', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588ebe31518ac4c563566')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR, Группа Компаний', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', 'Машинное обучение', 'компьютерное зрение', 'искусственный интеллект', 'Анализ данных'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/217737>, '_id': ObjectId('5db588ece31518ac4c563567')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/217737>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', 'Английский язык', 'Умение принимать решения', 'Analytical skills', 'Управление талантами', 'Time management', 'Управление персоналом', 'Подбор персонала', 'Ведение переговоров'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588ece31518ac4c563568')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Научный сотрудник (Машинное обучение и анализ данных в социальных и экономических системах)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': 'от 100\xa0000 руб. до вычета налогов', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588ece31518ac4c563569')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588ece31518ac4c56356a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Бухгалтер / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['МСФО', 'Бухгалтерская отчетность', 'Английский язык', 'Основные средства', '1C: Бухгалтерия', 'Бухгалтерский учет', 'Первичная бухгалтерская документация', 'Расчет затрат', 'Российские стандарты бухгалтерского учета', 'Консолидированная бухгалтерская отчетность', 'Работа с поставщиками', 'Учет затрат'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588ece31518ac4c56356b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate / (Senior/Старший) специалист по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588ece31518ac4c56356c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': 'от 2\xa0500 до 3\xa0500 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/3289413>, '_id': ObjectId('5db588ece31518ac4c56356d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3289413>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/62126>, '_id': ObjectId('5db588ece31518ac4c56356e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/62126>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Координатор онлайн-курсов', 'company_name': 'ООО Технологии в образовании', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': ['Информационные технологии', 'Клиентоориентированность', 'Консультирование клиентов', 'Консультирование клиентов по телефону', 'Ведение переписки'], 'salary': 'до 40\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2869446>, '_id': ObjectId('5db588ece31518ac4c56356f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2869446>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Android Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588ece31518ac4c563570')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588ece31518ac4c563571')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' «Nissan», Конструкторский центр', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, '_id': ObjectId('5db588ece31518ac4c563572')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java-разработчик', 'company_name': ' Альфа-Банк', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/80>, '_id': ObjectId('5db588ece31518ac4c563573')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/80>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c563574')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588ece31518ac4c563575')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588ece31518ac4c563576')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588ece31518ac4c563577')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python developer / Программист Python', 'company_name': ' Санкт-Петербургский филиал компании ГАЗПРОМ Германия ГмбХ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3358623>, '_id': ObjectId('5db588ece31518ac4c563578')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3358623>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Web аналитик', 'company_name': 'ООО Гаудеамус', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', 'Разработка технических заданий', 'СУБД'], 'salary': 'от 90\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/97598>, '_id': ObjectId('5db588ece31518ac4c563579')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/97598>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scala Developer', 'company_name': ' Криптонит', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': ['Разработка ПО', 'SCALA', 'Python', 'Spark', 'Веб-программирование'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/3491569>, '_id': ObjectId('5db588ece31518ac4c56357a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3491569>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c56357b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c56357c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': 'от 200\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c56357d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', 'Управление проектами', 'MS SQL Server', 'Business English'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/14153>, '_id': ObjectId('5db588ece31518ac4c56357e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/14153>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Администратор-ассистент (комплексные решения IoT, Ml)', 'company_name': ' КОМПЛИТ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1587>, '_id': ObjectId('5db588ece31518ac4c56357f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1587>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ece31518ac4c563580')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Разработчик .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': 'от 1\xa0800 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588ede31518ac4c563581')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Market Researcher (аналитик рынков)', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Маркетинговые исследования', 'Английский язык', 'Маркетинговый анализ', 'Работа в команде', 'Аналитические исследования', 'конкурентный анализ'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588ede31518ac4c563582')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Тестировщик ПО', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': 'от 650 USD на руки', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588ede31518ac4c563583')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588ede31518ac4c563584')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET Developer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588ede31518ac4c563585')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588ede31518ac4c563586')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior iOS Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588ede31518ac4c563587')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588ede31518ac4c563588')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': 'от 100\xa0000 руб. на руки', 'company_url': <GET https://spb.hh.ru/employer/2097195>, '_id': ObjectId('5db588ede31518ac4c563589')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2097195>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 15:09:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 81070,
 'downloader/request_count': 176,
 'downloader/request_method_count/GET': 176,
 'downloader/response_bytes': 6407539,
 'downloader/response_count': 176,
 'downloader/response_status_count/200': 176,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 7.966885,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 12, 9, 17, 291520),
 'log_count/DEBUG': 177,
 'log_count/ERROR': 167,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 176,
 'scheduler/dequeued': 176,
 'scheduler/dequeued/memory': 176,
 'scheduler/enqueued': 176,
 'scheduler/enqueued/memory': 176,
 'start_time': datetime.datetime(2019, 10, 27, 12, 9, 9, 324635)}
2019-10-27 15:09:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 15:10:55 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:10:55 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:10:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:10:55 [scrapy.extensions.telnet] INFO: Telnet Password: 3914ae08bfaeee9d
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:10:55 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:10:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:10:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:10:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c381')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c382')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c383')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c384')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db589507f59ace01f76c385')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c386')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c387')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c388')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c389')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c38a')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c38b')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db589507f59ace01f76c38c')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c38d')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db589507f59ace01f76c38e')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c38f')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c390')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db589507f59ace01f76c391')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c392')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c393')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:10:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589507f59ace01f76c394')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db589507f59ace01f76c395')}
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c396')}
2019-10-27 15:11:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c397')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c398')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c399')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c39a')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db589587f59ace01f76c39b')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c39c')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c39d')}
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c39e')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c39f')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c3a0')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c3a1')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db589587f59ace01f76c3a2')}
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589597f59ace01f76c3a3')}
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db589597f59ace01f76c3a4')}
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db589597f59ace01f76c3a5')}
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db589597f59ace01f76c3a6')}
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589817f59ace01f76c3a7')}
2019-10-27 15:11:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db589817f59ace01f76c3a8')}
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', '_id': ObjectId('5db589817f59ace01f76c3a9')}
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/22863>
{'company_url': 'http://www.protei.ru', '_id': ObjectId('5db589817f59ace01f76c3aa')}
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 15:11:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db589877f59ace01f76c3ab')}
2019-10-27 15:12:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/826770>
{'company_url': 'http://www.weareworldquant.com', '_id': ObjectId('5db5899f7f59ace01f76c3ac')}
2019-10-27 15:12:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/813602>
{'company_url': 'http://www.kofax.com', '_id': ObjectId('5db589a07f59ace01f76c3ad')}
2019-10-27 15:12:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/889>
{'company_url': 'http://www.kodeks.ru', '_id': ObjectId('5db589a07f59ace01f76c3ae')}
2019-10-27 15:12:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db589a17f59ace01f76c3af')}
2019-10-27 15:12:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db589a17f59ace01f76c3b0')}
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.extensions.logstats] INFO: Crawled 74 pages (at 74 pages/min), scraped 48 items (at 48 items/min)
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589a27f59ace01f76c3b1')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db589a27f59ace01f76c3b2')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', '_id': ObjectId('5db589a27f59ace01f76c3b3')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589a27f59ace01f76c3b4')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', '_id': ObjectId('5db589a27f59ace01f76c3b5')}
2019-10-27 15:12:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db589a27f59ace01f76c3b6')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589a27f59ace01f76c3b7')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1040131>
{'company_url': 'http://segmento.ru', '_id': ObjectId('5db589a27f59ace01f76c3b8')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/834307>
{'company_url': 'http://www.dm-matrix.com', '_id': ObjectId('5db589a27f59ace01f76c3b9')}
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/616>
{'company_url': 'http://lancktele.com', '_id': ObjectId('5db589a27f59ace01f76c3ba')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/63225>
{'company_url': 'https://vk.com', '_id': ObjectId('5db589a27f59ace01f76c3bb')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589a37f59ace01f76c3bc')}
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', '_id': ObjectId('5db589a37f59ace01f76c3bd')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db589a37f59ace01f76c3be')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2625959>
{'company_url': 'http://www.idrnd.ai', '_id': ObjectId('5db589a37f59ace01f76c3bf')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1263188>
{'company_url': 'http://www.weigandt-consulting.com', '_id': ObjectId('5db589a37f59ace01f76c3c0')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3931730>
{'company_url': 'http://GamesFactoryTalents.com', '_id': ObjectId('5db589a37f59ace01f76c3c1')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2416804>
{'company_url': 'https://www.welltory.com', '_id': ObjectId('5db589a37f59ace01f76c3c2')}
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 15:12:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:12:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/168421>
{'company_url': 'http://www.wandisco.com', '_id': ObjectId('5db589a47f59ace01f76c3c3')}
2019-10-27 15:58:17 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:58:17 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:58:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:58:17 [scrapy.extensions.telnet] INFO: Telnet Password: 4fc9a7d0917cecd9
2019-10-27 15:58:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:58:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:58:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:58:18 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:58:18 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:58:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:58:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' Кофакс', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', 'Английский язык', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e005')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist (Нижний Новгород)', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e006')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist в NLP лабораторию СКБ Контур', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e007')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist (Команда знаний)', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e008')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e009')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e00a')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e00b')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Синтез речи', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', 'нейронный сети', 'synthesis', 'speech synthesis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e00c')}
2019-10-27 15:58:19 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Сбербанк для экспертов', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e00d')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', 'Английский язык', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e00e')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': 'от 150\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5946b280a695c8af9e00f')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e010')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': 'Аналитик /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e011')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' Протей, НТЦ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': 'до 100\xa0000 руб. на руки', '_id': ObjectId('5db5946b280a695c8af9e012')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e013')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e014')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist (Аналитик антифрод)', 'company_name': 'ООО Смарт Лайн', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': 'от 100\xa0000 до 150\xa0000 руб. на руки', '_id': ObjectId('5db5946b280a695c8af9e015')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e016')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', 'Математическая статистика', 'MySQL', 'MS SQL', 'Английский язык', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e017')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5946b280a695c8af9e018')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/813602>
{'company_url': 'http://www.kofax.com', '_id': ObjectId('5db5946b280a695c8af9e019')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Консорциум Кодекс', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': 'до 130\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5946b280a695c8af9e01a')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db5946b280a695c8af9e01b')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', 'Математическая статистика', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e01c')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e01d')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', 'Статистический анализ', 'Математическое моделирование', 'Машинное обучение', 'Нейронные сети'], 'salary': 'от 150\xa0000 до 200\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5946b280a695c8af9e01e')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e01f')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-разработчик для медиаконтента', 'company_name': ' ВКонтакте', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e020')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e021')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': 'Аналитик данных', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e022')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', 'Анализ данных', 'PostgreSQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e023')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946b280a695c8af9e024')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e025')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e026')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', 'Анализ данных', 'Статистический анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e027')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', 'Английский язык', 'Коммуникабельность', 'Сбор и анализ информации', 'Управление отношениями с клиентами', 'Ведение переговоров', 'Деловая переписка'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e028')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e029')}
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead группы машинного обучения (computer vision)', 'company_name': 'ООО DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', 'Организаторские навыки', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e02a')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db5946c280a695c8af9e02b')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e02c')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e02d')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e02e')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/22863>
{'company_url': 'http://www.protei.ru', '_id': ObjectId('5db5946c280a695c8af9e02f')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e030')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/826770>
{'company_url': 'http://www.weareworldquant.com', '_id': ObjectId('5db5946c280a695c8af9e031')}
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db5946c280a695c8af9e032')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/889>
{'company_url': 'http://www.kodeks.ru', '_id': ObjectId('5db5946c280a695c8af9e033')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/245657>
{'company_url': 'http://www.gctrials.com/', '_id': ObjectId('5db5946c280a695c8af9e034')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/63225>
{'company_url': 'https://vk.com', '_id': ObjectId('5db5946c280a695c8af9e035')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/616>
{'company_url': 'http://lancktele.com', '_id': ObjectId('5db5946c280a695c8af9e036')}
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/34028801?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': 'Исследователь в области машинного обучения и анализа данных', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 70\xa0000 до 90\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5946c280a695c8af9e037')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/834307>
{'company_url': 'http://www.dm-matrix.com', '_id': ObjectId('5db5946c280a695c8af9e038')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/34028801?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1040131>
{'company_url': 'http://segmento.ru', '_id': ObjectId('5db5946c280a695c8af9e039')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмента Авто)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e03a')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e03b')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (Сегмент Коммерческий транспорт и мелкий опт)', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', 'Математический анализ', 'Математическое моделирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e03c')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e03d')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e03e')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Английский язык', 'Data Analysis', 'Big Data'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e03f')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1245452>
{'company_url': 'http://www.cloudlinux.com', '_id': ObjectId('5db5946c280a695c8af9e040')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': 'ООО Перфект Арт', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e041')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': ' Дата Матрикс', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e042')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2416804>
{'company_url': 'https://www.welltory.com', '_id': ObjectId('5db5946c280a695c8af9e043')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3931730>
{'company_url': 'http://GamesFactoryTalents.com', '_id': ObjectId('5db5946c280a695c8af9e044')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/168421>
{'company_url': 'http://www.wandisco.com', '_id': ObjectId('5db5946c280a695c8af9e045')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1263188>
{'company_url': 'http://www.weigandt-consulting.com', '_id': ObjectId('5db5946c280a695c8af9e046')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2625959>
{'company_url': 'http://www.idrnd.ai', '_id': ObjectId('5db5946c280a695c8af9e047')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946c280a695c8af9e048')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db5946c280a695c8af9e049')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e04a')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist в Контур.Фокус', 'company_name': ' СКБ Контур', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e04b')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e04c')}
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e04d')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern / Стажер по направлению «Анализ данных»', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e04e')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e04f')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': ['Поисковые системы'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e050')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e051')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/314182>
{'company_url': 'http://www.saber3d.com', '_id': ObjectId('5db5946d280a695c8af9e052')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db5946d280a695c8af9e053')}
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e054')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': 'ООО HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db5946d280a695c8af9e055')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e056')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e057')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e058')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', 'Математическая статистика', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', 'Английский язык', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e059')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e05a')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e05b')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': 'Менеджер по аналитике', 'company_name': ' Билайн', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Управление проектами', 'Ведение переговоров', 'Работа в команде', 'SQL', 'ORACLE'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e05c')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e05d')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e05e')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': 'Специалист по анализу изображений', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e05f')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': 'Разработчик BI (QlikView, Qlik Sense)', 'company_name': 'ООО Клевер Солюшнс', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', 'ООП', 'Оптимизация запросов', 'Анализ данных', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', 'Управление проектами', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', 'Работа с базами данных', 'Cognos BI'], 'salary': 'от 30\xa0000 до 110\xa0000 руб. на руки', '_id': ObjectId('5db5946d280a695c8af9e060')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e061')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e062')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': 'Руководитель отдела аналитики', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', 'Анализ данных', 'SQL', 'Статистический анализ', 'Математический анализ', 'Базы данных'], 'salary': 'от 350\xa0000 руб. на руки', '_id': ObjectId('5db5946d280a695c8af9e063')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e064')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/45743>
{'company_url': 'http://www.oct-clinicaltrials.com/', '_id': ObjectId('5db5946d280a695c8af9e065')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', 'Бизнес-анализ', 'Системный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e066')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/9281>
{'company_url': 'http://www.jetbrains.com', '_id': ObjectId('5db5946d280a695c8af9e067')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e068')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e069')}
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946d280a695c8af9e06a')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/6769>
{'company_url': 'http://www.epam-group.ru', '_id': ObjectId('5db5946d280a695c8af9e06b')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2610798>
{'company_url': 'http://www.melscience.com', '_id': ObjectId('5db5946d280a695c8af9e06c')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3425814>
{'company_url': 'http://www.corning.com', '_id': ObjectId('5db5946e280a695c8af9e06d')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/130986>
{'company_url': 'http://www.weatherford.ru', '_id': ObjectId('5db5946e280a695c8af9e06e')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2160858>
{'company_url': 'http://qleversolutions.ru/', '_id': ObjectId('5db5946e280a695c8af9e06f')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/208902>
{'company_url': 'https://softwarecountry.ru', '_id': ObjectId('5db5946e280a695c8af9e070')}
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', 'Английский язык', 'MS Visio'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e071')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': 'Инженер-аналитик', 'company_name': ' Mail.Ru Group, Социальные сети, Одноклассники', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e072')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' Рексофт', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', 'Английский язык'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e073')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e074')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e075')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e076')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': 'Разработчик научного ПО', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': 'от 100\xa0000 до 120\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5946e280a695c8af9e077')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e078')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e079')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e07a')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e07b')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e07c')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': 'от 260\xa0000 до 370\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5946e280a695c8af9e07d')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e07e')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': 'Системный аналитик (аналитическая система)', 'company_name': 'ООО ЦРПТ (Центр развития перспективных технологий)', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e07f')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e080')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': 'Руководитель группы сервисов / Tech Lead', 'company_name': ' ЦРТ | Группа компаний', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', 'Управление проектами', 'Руководство коллективом', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e081')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': 'Системный аналитик [id20580]', 'company_name': ' ПАО «Газпром нефть»', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', 'Системная интеграция'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e082')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': 'Системный инженер / Стажер Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e083')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/213004>
{'company_url': 'http://akvelon-ivanovo.ru/', '_id': ObjectId('5db5946e280a695c8af9e084')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e085')}
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': 'Специалист по веб-аналитике', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', 'Английский язык', 'Google Tag Manager', 'Анализ данных'], 'salary': 'от 60\xa0000 до 120\xa0000 руб. на руки', '_id': ObjectId('5db5946e280a695c8af9e086')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', 'микросервисы'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e087')}
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e088')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e089')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': 'ООО ЭдСервер', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e08a')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e08b')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['знание английского языка', 'готовность к командировкам'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e08c')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': 'Ведущий аналитик (проект по DWH и BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['Бизнес-анализ', 'Оптимизация бизнес-процессов', 'Бизнес-планирование', 'Руководство коллективом', 'Аналитические исследования', 'MS Visio', 'Управление проектами', 'Ведение переговоров', 'Big Data', 'Data Science', 'Моделирование бизнес процессов'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e08d')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', 'Английский язык', 'C/C++'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946e280a695c8af9e08e')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' ЭР-Телеком', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e08f')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master –  Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e090')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e091')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e092')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db5946f280a695c8af9e093')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e094')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e095')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e096')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/914078>
{'company_url': 'https://ix.co/', '_id': ObjectId('5db5946f280a695c8af9e097')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e098')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer / Старший разработчик', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': 'от 3\xa0000 USD на руки', '_id': ObjectId('5db5946f280a695c8af9e099')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-специалист', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', 'Яндекс.Метрика', 'Google Analytics', 'SEO оптимизация', 'Лидогенерация', 'Составление семантического ядра', 'Внутренняя оптимизация сайта'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e09a')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3156389>
{'company_url': 'http://www.crpt.ru', '_id': ObjectId('5db5946f280a695c8af9e09b')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/638130>
{'company_url': 'http://www.firstlinesoftware.ru', '_id': ObjectId('5db5946f280a695c8af9e09c')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/574211>
{'company_url': 'http://www.netwrix.com', '_id': ObjectId('5db5946f280a695c8af9e09d')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1574063>
{'company_url': 'http://www.clearscale.com', '_id': ObjectId('5db5946f280a695c8af9e09e')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2559110>
{'company_url': 'https://insightwhale.com', '_id': ObjectId('5db5946f280a695c8af9e09f')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/654435>
{'company_url': 'http://spiceit.ru', '_id': ObjectId('5db5946f280a695c8af9e0a0')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/139>
{'company_url': 'http://www.ibs.ru', '_id': ObjectId('5db5946f280a695c8af9e0a1')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': 'Координатор онлайн-курсов', 'company_name': 'ООО Технологии в образовании', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': ['Информационные технологии', 'Клиентоориентированность', 'Консультирование клиентов', 'Консультирование клиентов по телефону', 'Ведение переписки'], 'salary': 'до 40\xa0000 руб. на руки', '_id': ObjectId('5db5946f280a695c8af9e0a2')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': 'от 2\xa0500 до 3\xa0500 USD на руки', '_id': ObjectId('5db5946f280a695c8af9e0a3')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': ['Английский язык', 'Математическая статистика', 'Математический анализ', 'языки программирования'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0a4')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0a5')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0a6')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Навыки межличностного общения', 'MS Excel', 'Английский язык', 'SQL'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0a7')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0a8')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', 'Английский язык', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0a9')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0aa')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Компьютерное моделирование и прогнозирование в финансовой сфере)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db5946f280a695c8af9e0ab')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0ac')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': 'ООО ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', 'Ведение переговоров', 'DWH', 'Управление проектами', 'Business Development', 'ETL', 'AWS', 'Английский язык', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0ad')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) / Ассистент по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0ae')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/6500>
{'company_url': 'http://www.synopsys.com', '_id': ObjectId('5db5946f280a695c8af9e0af')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': 'Бухгалтер / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['МСФО', 'Бухгалтерская отчетность', 'Английский язык', 'Основные средства', '1C: Бухгалтерия', 'Бухгалтерский учет', 'Первичная бухгалтерская документация', 'Расчет затрат', 'Российские стандарты бухгалтерского учета', 'Консолидированная бухгалтерская отчетность', 'Работа с поставщиками', 'Учет затрат'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0b0')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0b1')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0b2')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/79595>
{'company_url': 'http://www.broadridge.com', '_id': ObjectId('5db5946f280a695c8af9e0b3')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR, Группа Компаний', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', 'Машинное обучение', 'компьютерное зрение', 'искусственный интеллект', 'Анализ данных'], 'salary': ' з/п не указана', '_id': ObjectId('5db5946f280a695c8af9e0b4')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', 'Английский язык', 'Умение принимать решения', 'Analytical skills', 'Управление талантами', 'Time management', 'Управление персоналом', 'Подбор персонала', 'Ведение переговоров'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0b5')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': 'Научный сотрудник (Машинное обучение и анализ данных в социальных и экономических системах)', 'company_name': ' Университет ИТМО', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': 'от 100\xa0000 руб. до вычета налогов', '_id': ObjectId('5db59470280a695c8af9e0b6')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2869446>
{'company_url': 'http://skillfactory.ru', '_id': ObjectId('5db59470280a695c8af9e0b7')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3455245>
{'company_url': 'https://www.alberblanc.com/', '_id': ObjectId('5db59470280a695c8af9e0b8')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0b9')}
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2617114>
{'company_url': 'https://itretail.com/', '_id': ObjectId('5db59470280a695c8af9e0ba')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31189716?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': 'от 200\xa0000 руб. на руки', '_id': ObjectId('5db59470280a695c8af9e0bb')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/21963813?query=data%20science>
{'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', 'Управление продуктом'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0bc')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33497437?query=data%20science>
{'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', 'Управление проектами', 'MS SQL Server', 'Business English'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0bd')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34031778?query=data%20science>
{'vacancy_title': 'Администратор-ассистент (комплексные решения IoT, Ml)', 'company_name': ' КОМПЛИТ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0be')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33964564?query=data%20science>
{'vacancy_title': 'Market Researcher (аналитик рынков)', 'company_name': ' IQ Орtiоn Sоftwаre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Маркетинговые исследования', 'Английский язык', 'Маркетинговый анализ', 'Работа в команде', 'Аналитические исследования', 'конкурентный анализ'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0bf')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33998747?query=data%20science>
{'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c0')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/Старший) специалист по клиническим исследованиям', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c1')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' «Nissan», Конструкторский центр', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c2')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c3')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c4')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-разработчик', 'company_name': ' Альфа-Банк', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c5')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c6')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c7')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c8')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0c9')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer / Программист Python', 'company_name': ' Санкт-Петербургский филиал компании ГАЗПРОМ Германия ГмбХ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0ca')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/11687>
{'company_url': 'http://www.jobs.iqvia.com', '_id': ObjectId('5db59470280a695c8af9e0cb')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' Криптонит', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': ['Разработка ПО', 'SCALA', 'Python', 'Spark', 'Веб-программирование'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0cc')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1587> (referer: https://spb.hh.ru/vacancy/34031778?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web аналитик', 'company_name': 'ООО Гаудеамус', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', 'Разработка технических заданий', 'СУБД'], 'salary': 'от 90\xa0000 руб. на руки', '_id': ObjectId('5db59470280a695c8af9e0cd')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0ce')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': 'от 150\xa0000 руб. на руки', '_id': ObjectId('5db59470280a695c8af9e0cf')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33998747?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/14153> (referer: https://spb.hh.ru/vacancy/33497437?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/217737>
{'company_url': 'http://ru.dsr-corporation.com', '_id': ObjectId('5db59470280a695c8af9e0d0')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1587>
{'company_url': 'http://www.complete.ru', '_id': ObjectId('5db59470280a695c8af9e0d1')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813435?query=data%20science>
{'vacancy_title': 'Тестировщик ПО', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': 'от 650 USD на руки', '_id': ObjectId('5db59470280a695c8af9e0d2')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813402?query=data%20science>
{'vacancy_title': 'Разработчик .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': 'от 1\xa0800 USD на руки', '_id': ObjectId('5db59470280a695c8af9e0d3')}
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33998747?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/14153>
{'company_url': 'https://www.emergn.com/', '_id': ObjectId('5db59470280a695c8af9e0d4')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135809?query=data%20science>
{'vacancy_title': 'Senior iOS Developer', 'company_name': 'ООО МЕЛ Саенс', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0d5')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933692?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0d6')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/97598> (referer: https://spb.hh.ru/vacancy/33921789?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3358623> (referer: https://spb.hh.ru/vacancy/31015468?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32422629?query=data%20science>
{'vacancy_title': 'Senior .NET Developer', 'company_name': 'АО Аркадия', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0d7')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3491569> (referer: https://spb.hh.ru/vacancy/33912707?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933697?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (Российский бренд Эксперт-Система)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' з/п не указана', '_id': ObjectId('5db59470280a695c8af9e0d8')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/97598>
{'company_url': 'https://www.reveltime.ru', '_id': ObjectId('5db59470280a695c8af9e0d9')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2558659> (referer: https://spb.hh.ru/vacancy/33813435?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31252237?query=data%20science>
{'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': 'от 100\xa0000 руб. на руки', '_id': ObjectId('5db59470280a695c8af9e0da')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3358623>
{'company_url': 'https://www.gazprom-germania.de/home.html', '_id': ObjectId('5db59470280a695c8af9e0db')}
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3491569>
{'company_url': 'http://kryptonite.ru', '_id': ObjectId('5db59471280a695c8af9e0dc')}
2019-10-27 15:58:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2558659>
{'company_url': 'https://www.univintel.com', '_id': ObjectId('5db59471280a695c8af9e0dd')}
2019-10-27 15:58:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2097195> (referer: https://spb.hh.ru/vacancy/31252237?query=data%20science)
2019-10-27 15:58:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2097195>
{'company_url': 'http://devim.team', '_id': ObjectId('5db59471280a695c8af9e0de')}
2019-10-27 15:58:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 15:58:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 114079,
 'downloader/request_count': 265,
 'downloader/request_method_count/GET': 265,
 'downloader/response_bytes': 10887812,
 'downloader/response_count': 265,
 'downloader/response_status_count/200': 265,
 'dupefilter/filtered': 79,
 'elapsed_time_seconds': 7.082154,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 12, 58, 25, 235847),
 'item_scraped_count': 218,
 'log_count/DEBUG': 484,
 'log_count/ERROR': 38,
 'log_count/INFO': 10,
 'request_depth_max': 10,
 'response_received_count': 265,
 'scheduler/dequeued': 265,
 'scheduler/dequeued/memory': 265,
 'scheduler/enqueued': 265,
 'scheduler/enqueued/memory': 265,
 'spider_exceptions/IndexError': 38,
 'start_time': datetime.datetime(2019, 10, 27, 12, 58, 18, 153693)}
2019-10-27 15:58:25 [scrapy.core.engine] INFO: Spider closed (finished)
