2019-10-21 20:32:45 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:32:45 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:32:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:32:45 [scrapy.extensions.telnet] INFO: Telnet Password: 26582551d748b1e3
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:32:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:32:45 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:32:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:32:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:32:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:32:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 11, in parse
    prof = response.css('span.g-user-content a').text
AttributeError: 'SelectorList' object has no attribute 'text'
2019-10-21 20:32:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:32:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 303,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49920,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.844354,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 32, 46, 161710),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 32, 45, 317356)}
2019-10-21 20:32:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:32:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:32:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:32:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:32:54 [scrapy.extensions.telnet] INFO: Telnet Password: 3e00087a49e41ea7
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:32:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:32:54 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:32:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:32:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:32:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:32:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 11, in parse
    prof = response.css('span.g-user-content a').text
AttributeError: 'SelectorList' object has no attribute 'text'
2019-10-21 20:32:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:32:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 303,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 49901,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.866951,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 32, 55, 211552),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 32, 54, 344601)}
2019-10-21 20:32:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:33:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:33:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:33:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:33:23 [scrapy.extensions.telnet] INFO: Telnet Password: 63451b5519bff80d
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:33:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:33:23 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:33:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:33:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:33:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:33:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:33:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:33:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:33:56 [scrapy.extensions.telnet] INFO: Telnet Password: 7f05405f2761c291
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:33:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:33:56 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:33:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:33:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:33:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:35:20 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:35:20 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:35:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:35:20 [scrapy.extensions.telnet] INFO: Telnet Password: 50fa158b8c222b36
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:35:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:35:20 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:35:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:35:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:35:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:35:49 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:35:49 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:35:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:35:49 [scrapy.extensions.telnet] INFO: Telnet Password: dc2edd1bdcbbc55d
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:35:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:35:49 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:35:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:35:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:35:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:36:04 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:36:04 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:36:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:36:04 [scrapy.extensions.telnet] INFO: Telnet Password: cb0c6ceec5e55e03
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:36:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:36:04 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:36:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:36:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:36:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy> (referer: None)
2019-10-21 20:41:02 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:41:02 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:41:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:41:02 [scrapy.extensions.telnet] INFO: Telnet Password: 420070c5508d18a4
2019-10-21 20:41:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:41:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:41:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:41:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:41:03 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:41:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:41:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:41:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:43:34 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:43:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:43:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:43:34 [scrapy.extensions.telnet] INFO: Telnet Password: 4b5905bb1825be20
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:43:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:43:34 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:43:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:43:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:43:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:44:11 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:44:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:44:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:44:11 [scrapy.extensions.telnet] INFO: Telnet Password: 4e450383d57cfc47
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:44:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:44:11 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:44:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:44:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:44:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:44:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:44:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:44:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:44:23 [scrapy.extensions.telnet] INFO: Telnet Password: f042aa94f0867e00
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:44:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:44:23 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:44:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:44:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:44:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:44:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:44:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:44:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:44:58 [scrapy.extensions.telnet] INFO: Telnet Password: 90eb21663e429b2b
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:44:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:44:58 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:44:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:44:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:44:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:45:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:45:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:45:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:45:32 [scrapy.extensions.telnet] INFO: Telnet Password: 691c9d7456cc668b
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:45:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:45:32 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:45:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:45:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:45:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:46:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:46:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:46:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:46:56 [scrapy.extensions.telnet] INFO: Telnet Password: 17641c7ef0c084fd
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:46:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:46:56 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:46:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:46:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:46:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:47:41 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:47:41 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:47:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:47:41 [scrapy.extensions.telnet] INFO: Telnet Password: 8d851a207dcb26b0
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:47:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:47:41 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:47:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:47:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:47:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:48:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:48:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:48:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:48:32 [scrapy.extensions.telnet] INFO: Telnet Password: 4dfb82d76362dc44
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:48:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:48:32 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:48:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:48:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:48:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:49:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:49:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:49:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:49:58 [scrapy.extensions.telnet] INFO: Telnet Password: 5729d65d902c5c20
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:49:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:49:58 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:49:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:49:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:49:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:50:34 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:50:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:50:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:50:34 [scrapy.extensions.telnet] INFO: Telnet Password: b7b88c9c8cb60924
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:50:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:50:34 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:50:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:50:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:50:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:50:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('span.pager-item-not-in-short-range a::attr')
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 81, in xpath_pseudo_element
    % pseudo_element)
cssselect.xpath.ExpressionError: The pseudo-element ::attr is unknown
2019-10-21 20:50:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:50:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42965,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.599956,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 50, 35, 224160),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 50, 34, 624204)}
2019-10-21 20:50:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:50:40 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:50:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:50:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:50:40 [scrapy.extensions.telnet] INFO: Telnet Password: 6209a54712c78494
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:50:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:50:40 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:50:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:50:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:50:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:50:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('span.pager-item-not-in-short-range a::attr')
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in <genexpr>
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 222, in selector_to_xpath
    xpath = self.xpath_pseudo_element(xpath, selector.pseudo_element)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 81, in xpath_pseudo_element
    % pseudo_element)
cssselect.xpath.ExpressionError: The pseudo-element ::attr is unknown
2019-10-21 20:50:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:50:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43006,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.573457,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 50, 41, 547334),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ExpressionError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 50, 40, 973877)}
2019-10-21 20:50:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:50:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:50:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:50:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:50:59 [scrapy.extensions.telnet] INFO: Telnet Password: acb47b24eea4212f
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:50:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:50:59 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:50:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:50:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:50:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:51:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:51:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:51:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:51:22 [scrapy.extensions.telnet] INFO: Telnet Password: c9261a97220d19fc
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:51:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:51:22 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:51:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:51:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:51:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:52:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:52:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:52:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:52:58 [scrapy.extensions.telnet] INFO: Telnet Password: 3d0846234adb1b9a
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:52:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:52:58 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:52:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:52:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:52:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:55:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:55:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:55:12 [scrapy.extensions.telnet] INFO: Telnet Password: b08e6479c137ba56
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:55:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:55:12 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:55:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:55:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:55:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 17, in parse
    print(pagination[-1])
IndexError: list index out of range
2019-10-21 20:55:12 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:55:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43149,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.637381,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 55, 12, 992806),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 55, 12, 355425)}
2019-10-21 20:55:12 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:55:48 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:55:48 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:55:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:55:48 [scrapy.extensions.telnet] INFO: Telnet Password: c8bdf400dc0e9ba2
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:55:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:55:48 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:55:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:55:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:55:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('a."bloko-button HH-Pager-Control" a::attr(href)').extract()
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 415, in parse
    return list(parse_selector_group(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 428, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 436, in parse_selector
    result, pseudo_element = parse_simple_selector(stream)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 492, in parse_simple_selector
    result = Class(result, stream.next_ident())
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 819, in next_ident
    raise SelectorSyntaxError('Expected ident, got %s' % (next,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected ident, got <STRING 'bloko-button HH-Pager-Control' at 2>
2019-10-21 20:55:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:55:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42982,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.757368,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 55, 49, 335132),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/SelectorSyntaxError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 55, 48, 577764)}
2019-10-21 20:55:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:55:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:55:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:55:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:55:56 [scrapy.extensions.telnet] INFO: Telnet Password: 2ca2fe503d852254
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:55:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:55:57 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:55:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:55:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:55:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:55:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('a."bloko-button HH-Pager-Control" a::attr(href)').extract()
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 415, in parse
    return list(parse_selector_group(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 428, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 436, in parse_selector
    result, pseudo_element = parse_simple_selector(stream)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 492, in parse_simple_selector
    result = Class(result, stream.next_ident())
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 819, in next_ident
    raise SelectorSyntaxError('Expected ident, got %s' % (next,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected ident, got <STRING 'bloko-button HH-Pager-Control' at 2>
2019-10-21 20:55:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:55:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42990,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.61138,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 55, 57, 673213),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/SelectorSyntaxError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 55, 57, 61833)}
2019-10-21 20:55:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:56:11 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:56:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:56:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:56:11 [scrapy.extensions.telnet] INFO: Telnet Password: 2e33f697e05bbc61
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:56:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:56:11 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:56:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:56:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:56:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:56:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 13, in parse
    pagination=response.css('a.(bloko-button HH-Pager-Control) a::attr(href)').extract()
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\scrapy\http\response\text.py", line 122, in css
    return self.selector.css(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 264, in css
    return self.xpath(self._css2xpath(query))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 267, in _css2xpath
    return self._csstranslator.css_to_xpath(query)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\csstranslator.py", line 109, in css_to_xpath
    return super(HTMLTranslator, self).css_to_xpath(css, prefix)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\xpath.py", line 192, in css_to_xpath
    for selector in parse(css))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 415, in parse
    return list(parse_selector_group(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 428, in parse_selector_group
    yield Selector(*parse_selector(stream))
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 436, in parse_selector
    result, pseudo_element = parse_simple_selector(stream)
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 492, in parse_simple_selector
    result = Class(result, stream.next_ident())
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\cssselect\parser.py", line 819, in next_ident
    raise SelectorSyntaxError('Expected ident, got %s' % (next,))
  File "<string>", line None
cssselect.parser.SelectorSyntaxError: Expected ident, got <DELIM '(' at 2>
2019-10-21 20:56:12 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:56:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43118,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.598932,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 56, 12, 384847),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/SelectorSyntaxError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 56, 11, 785915)}
2019-10-21 20:56:12 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:56:24 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:56:24 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:56:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:56:24 [scrapy.extensions.telnet] INFO: Telnet Password: c345b5679afbb285
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:56:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:56:24 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:56:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:56:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:56:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:56:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 17, in parse
    print(pagination[-1])
IndexError: list index out of range
2019-10-21 20:56:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 20:56:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42923,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.62601,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 17, 56, 25, 71841),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 17, 56, 24, 445831)}
2019-10-21 20:56:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 20:57:00 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:57:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:57:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:57:00 [scrapy.extensions.telnet] INFO: Telnet Password: 2305d4d195a36854
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:57:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:57:00 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:57:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:57:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:57:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:57:44 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:57:44 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:57:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:57:44 [scrapy.extensions.telnet] INFO: Telnet Password: 2b1ac1bd7966fa1c
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:57:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:57:44 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:57:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:57:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:57:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 20:58:31 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 20:58:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 20:58:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 20:58:31 [scrapy.extensions.telnet] INFO: Telnet Password: 03fda66eeb3b16f6
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 20:58:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 20:58:31 [scrapy.core.engine] INFO: Spider opened
2019-10-21 20:58:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 20:58:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 20:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:07:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:07:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:07:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:07:52 [scrapy.extensions.telnet] INFO: Telnet Password: d444d230bdbefcc7
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:07:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:07:52 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:07:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:07:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:07:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 18, in parse
    print(pagination[-1])
IndexError: list index out of range
2019-10-21 21:07:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:07:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43013,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.59302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 7, 53, 174670),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 18, 7, 52, 581650)}
2019-10-21 21:07:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:08:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:08:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:08:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:08:22 [scrapy.extensions.telnet] INFO: Telnet Password: 9438cb94c1de0486
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:08:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:08:22 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:08:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:08:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:08:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 19, in parse
    print(pagination[3])
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\parsel\selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
2019-10-21 21:08:23 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:08:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 42968,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.574549,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 8, 23, 224253),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 18, 8, 22, 649704)}
2019-10-21 21:08:23 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:08:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:08:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:08:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:08:54 [scrapy.extensions.telnet] INFO: Telnet Password: a9027e9fefb517ee
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:08:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:08:54 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:08:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:08:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:08:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:08:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\lesson3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 19, in parse
    print(pagination[3])
IndexError: list index out of range
2019-10-21 21:08:54 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:08:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 43048,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.572722,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 8, 54, 791839),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 10, 21, 18, 8, 54, 219117)}
2019-10-21 21:08:54 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:09:06 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:09:06 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:09:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:09:06 [scrapy.extensions.telnet] INFO: Telnet Password: 02ba7a4eae034b0d
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:09:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:09:06 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:09:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:09:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:09:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:09:55 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:09:55 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:09:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:09:55 [scrapy.extensions.telnet] INFO: Telnet Password: c85356c11ff57b6b
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:09:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:09:55 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:09:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:09:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:09:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:10:40 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:10:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:10:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:10:40 [scrapy.extensions.telnet] INFO: Telnet Password: 4a5f72623bc661d5
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:10:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:10:40 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:10:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:10:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:10:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:11:00 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:11:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:11:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:11:00 [scrapy.extensions.telnet] INFO: Telnet Password: c0c8224dabcdea43
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:11:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:11:00 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:11:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:11:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:11:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:12:06 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:12:06 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:12:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:12:06 [scrapy.extensions.telnet] INFO: Telnet Password: ed08d963d21be3c4
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:12:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:12:06 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:12:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:12:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:12:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:12:30 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:12:30 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:12:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:12:30 [scrapy.extensions.telnet] INFO: Telnet Password: 7a51aac067d05717
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:12:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:12:30 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:12:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:12:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:12:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?text=Data+science> (referer: None)
2019-10-21 21:13:31 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:13:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?text=Data+science)
2019-10-21 21:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-21 21:13:37 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:13:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:13:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1277,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128179,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 72.127122,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 13, 42, 267199),
 'log_count/DEBUG': 4,
 'log_count/INFO': 11,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 12, 30, 140077)}
2019-10-21 21:13:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:15:44 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:15:44 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:15:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:15:44 [scrapy.extensions.telnet] INFO: Telnet Password: c310e7515ec97d93
2019-10-21 21:15:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:15:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:15:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:15:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:15:45 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:15:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:15:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:15:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:15:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:15:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-21 21:15:59 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:16:03 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:16:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1391,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128230,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.005512,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 16, 3, 57801),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 15, 45, 52289)}
2019-10-21 21:16:03 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:16:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:16:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:16:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:16:23 [scrapy.extensions.telnet] INFO: Telnet Password: 80e067fb2e4cd07d
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:16:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:16:23 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:16:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:16:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:16:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:16:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:17:31 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:17:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:17:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:17:31 [scrapy.extensions.telnet] INFO: Telnet Password: fb1aea8658affad8
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:17:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:17:31 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:17:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:17:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:17:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:17:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:17:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-21 21:17:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-21 21:17:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8)
2019-10-21 21:17:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10)
2019-10-21 21:17:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12)
2019-10-21 21:17:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14)
2019-10-21 21:18:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16)
2019-10-21 21:18:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18)
2019-10-21 21:18:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20)
2019-10-21 21:18:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22)
2019-10-21 21:18:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24)
2019-10-21 21:18:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26)
2019-10-21 21:18:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28)
2019-10-21 21:18:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30)
2019-10-21 21:18:31 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:18:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32)
2019-10-21 21:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34)
2019-10-21 21:18:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36)
2019-10-21 21:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38)
2019-10-21 21:18:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:18:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:18:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:18:59 [scrapy.extensions.telnet] INFO: Telnet Password: a6e194558d06cbf2
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:18:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:18:59 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:18:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:19:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:19:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:19:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-21 21:19:12 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:19:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:19:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1391,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128888,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 18.158348,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 19, 17, 895238),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 18, 59, 736890)}
2019-10-21 21:19:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:30:21 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:30:21 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:30:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:30:21 [scrapy.extensions.telnet] INFO: Telnet Password: e1c54ebe81309300
2019-10-21 21:30:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:30:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:30:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:30:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:30:22 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:30:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:30:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:30:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:30:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-21 21:30:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-21 21:31:09 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:31:09 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:31:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:31:09 [scrapy.extensions.telnet] INFO: Telnet Password: 9a02b8552549bd34
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:31:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:31:09 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:31:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:31:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:31:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science> (referer: None)
2019-10-21 21:31:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science)
2019-10-21 21:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-21 21:31:25 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=2> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:31:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:31:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1391,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128049,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 22.776688,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 31, 32, 48122),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 31, 9, 271434)}
2019-10-21 21:31:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-21 21:32:40 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:32:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:32:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:32:40 [scrapy.extensions.telnet] INFO: Telnet Password: 3b46aeddd56b7482
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:32:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:32:40 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:32:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:32:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:32:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-21 21:32:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: hh)
2019-10-21 21:32:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-21 21:32:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-21 21:32:56 [scrapy.extensions.telnet] INFO: Telnet Password: 9d3787c4620d12ab
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-21 21:32:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-21 21:32:56 [scrapy.core.engine] INFO: Spider opened
2019-10-21 21:32:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-21 21:32:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-21 21:32:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-21 21:33:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-21 21:33:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-21 21:33:05 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-21 21:33:08 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-21 21:33:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1405,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128849,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 11.721174,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 21, 18, 33, 8, 323523),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 21, 18, 32, 56, 602349)}
2019-10-21 21:33:08 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 19:15:11 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:15:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:15:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:15:11 [scrapy.extensions.telnet] INFO: Telnet Password: e3568a9844dbf9d0
2019-10-23 19:15:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:15:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:15:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:15:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:15:12 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:15:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:15:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:15:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:15:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:15:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-23 19:15:20 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 19:15:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-23 19:15:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1405,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128232,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 7.953121,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 23, 16, 15, 20, 13120),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 23, 16, 15, 12, 59999)}
2019-10-23 19:15:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 19:31:47 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:31:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:31:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:31:47 [scrapy.extensions.telnet] INFO: Telnet Password: 3806715424bc0a88
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:31:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:31:47 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:31:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:31:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:31:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:31:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:31:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-23 19:31:56 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=1> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 19:31:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-23 19:31:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1405,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 128456,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 8.479215,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 23, 16, 31, 56, 235945),
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'request_depth_max': 3,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 10, 23, 16, 31, 47, 756730)}
2019-10-23 19:31:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 19:32:31 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:32:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:32:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:32:31 [scrapy.extensions.telnet] INFO: Telnet Password: 4917a3e4dcd8a654
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:32:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:32:31 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:32:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:32:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:32:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:32:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 19:33:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-23 19:33:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8)
2019-10-23 19:50:11 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:50:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:50:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:50:11 [scrapy.extensions.telnet] INFO: Telnet Password: eb0e736bacff18a1
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:50:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:50:12 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:50:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:50:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:50:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:50:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 19:50:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 19:51:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:51:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:51:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:51:19 [scrapy.extensions.telnet] INFO: Telnet Password: 5717250efa1cdd15
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:51:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:51:19 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:51:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:51:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:51:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:52:25 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 19:52:25 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 19:52:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 19:52:25 [scrapy.extensions.telnet] INFO: Telnet Password: afed227caabec4a0
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 19:52:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 19:52:25 [scrapy.core.engine] INFO: Spider opened
2019-10-23 19:52:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 19:52:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 19:52:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 19:52:26 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 20:28:28 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 20:28:28 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 20:28:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 20:28:28 [scrapy.extensions.telnet] INFO: Telnet Password: 68a6d204f0219d47
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 20:28:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 20:28:28 [scrapy.core.engine] INFO: Spider opened
2019-10-23 20:28:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:28:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 20:28:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 20:28:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 20:28:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-23 20:28:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 20:28:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-23 20:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-23 20:29:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 20:29:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 20:29:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 20:29:19 [scrapy.extensions.telnet] INFO: Telnet Password: 5b376d9d1ac93168
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 20:29:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 20:29:19 [scrapy.core.engine] INFO: Spider opened
2019-10-23 20:29:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:29:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 20:29:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-23 20:29:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-23 20:29:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-23 20:29:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-23 20:29:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-23 20:29:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-23 20:29:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-23 20:29:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=9> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=8)
2019-10-23 20:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=9)
2019-10-23 20:29:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=11> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=10)
2019-10-23 20:29:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=11)
2019-10-23 20:30:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=13> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=12)
2019-10-23 20:30:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=13)
2019-10-23 20:30:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=15> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=14)
2019-10-23 20:30:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=15)
2019-10-23 20:30:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=17> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=16)
2019-10-23 20:30:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=17)
2019-10-23 20:30:19 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 17 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:30:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=19> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=18)
2019-10-23 20:30:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=19)
2019-10-23 20:30:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=21> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=20)
2019-10-23 20:30:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=21)
2019-10-23 20:30:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=23> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=22)
2019-10-23 20:30:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=23)
2019-10-23 20:30:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=25> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=24)
2019-10-23 20:30:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=25)
2019-10-23 20:30:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=27> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=26)
2019-10-23 20:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=27)
2019-10-23 20:31:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=29> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=28)
2019-10-23 20:31:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=29)
2019-10-23 20:31:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=31> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=30)
2019-10-23 20:31:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=31)
2019-10-23 20:31:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=33> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=32)
2019-10-23 20:31:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=33)
2019-10-23 20:31:19 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:31:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=35> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=34)
2019-10-23 20:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=35)
2019-10-23 20:31:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=37> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=36)
2019-10-23 20:31:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=37)
2019-10-23 20:31:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=39> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=38)
2019-10-23 20:31:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=39)
2019-10-23 20:31:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=41> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40)
2019-10-23 20:31:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=42> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=41)
2019-10-23 20:31:46 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=40> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-23 20:31:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-23 20:31:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20850,
 'downloader/request_count': 41,
 'downloader/request_method_count/GET': 41,
 'downloader/response_bytes': 1738900,
 'downloader/response_count': 41,
 'downloader/response_status_count/200': 41,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 146.689369,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 23, 17, 31, 46, 97076),
 'log_count/DEBUG': 42,
 'log_count/INFO': 12,
 'request_depth_max': 41,
 'response_received_count': 41,
 'scheduler/dequeued': 41,
 'scheduler/dequeued/memory': 41,
 'scheduler/enqueued': 41,
 'scheduler/enqueued/memory': 41,
 'start_time': datetime.datetime(2019, 10, 23, 17, 29, 19, 407707)}
2019-10-23 20:31:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-23 20:33:04 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-23 20:33:04 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-23 20:33:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-23 20:33:04 [scrapy.extensions.telnet] INFO: Telnet Password: f0de803168ed8888
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-23 20:33:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-23 20:33:04 [scrapy.core.engine] INFO: Spider opened
2019-10-23 20:33:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-23 20:33:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-23 20:33:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 20:31:37 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 20:31:37 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 20:31:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 20:31:37 [scrapy.extensions.telnet] INFO: Telnet Password: 6127b1c7ae6bee43
2019-10-24 20:31:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 20:31:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 20:31:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 20:31:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 20:31:38 [scrapy.core.engine] INFO: Spider opened
2019-10-24 20:31:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 20:31:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 20:31:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 20:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 20:32:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 20:32:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 20:32:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 20:32:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 20:32:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 20:32:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 20:32:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 20:32:13 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 20:32:16 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 20:32:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361113,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 38.055391,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 17, 32, 16, 64971),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 17, 31, 38, 9580)}
2019-10-24 20:32:16 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 20:34:08 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 20:34:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 20:34:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 20:34:08 [scrapy.extensions.telnet] INFO: Telnet Password: 639eabe89ac050b0
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 20:34:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 20:34:08 [scrapy.core.engine] INFO: Spider opened
2019-10-24 20:34:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 20:34:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 20:34:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 20:34:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 20:34:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 20:34:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 20:34:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 20:34:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 20:34:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 20:34:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 20:34:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 20:34:18 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 20:34:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 20:34:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361232,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 10.45162,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 17, 34, 18, 874664),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 17, 34, 8, 423044)}
2019-10-24 20:34:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 21:11:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:11:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:11:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:11:19 [scrapy.extensions.telnet] INFO: Telnet Password: 9882409fa41a21af
2019-10-24 21:11:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:11:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:11:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:11:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:11:20 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:11:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:11:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:11:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:11:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:11:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 21:11:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 21:11:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 21:11:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 21:11:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 21:11:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 21:11:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 21:11:30 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 21:11:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 21:11:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361203,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 10.332518,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 18, 11, 30, 354262),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 18, 11, 20, 21744)}
2019-10-24 21:11:30 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 21:11:58 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:11:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:11:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:11:58 [scrapy.extensions.telnet] INFO: Telnet Password: ecc296afb34097d6
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:11:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:11:58 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:11:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:11:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:11:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:11:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:12:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1)
2019-10-24 21:12:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=2)
2019-10-24 21:12:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=3)
2019-10-24 21:12:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=4)
2019-10-24 21:12:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=5)
2019-10-24 21:12:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=6)
2019-10-24 21:12:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7)
2019-10-24 21:12:49 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-24 21:12:49 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-24 21:12:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4584,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 361408,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 51.802708,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 24, 18, 12, 49, 929097),
 'log_count/DEBUG': 10,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2019, 10, 24, 18, 11, 58, 126389)}
2019-10-24 21:12:49 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-24 21:12:57 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:12:57 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:12:57 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:12:57 [scrapy.extensions.telnet] INFO: Telnet Password: 8fcf2087a6fd9c20
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:12:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:12:57 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:12:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:12:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:12:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:16:34 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:16:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:16:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:16:34 [scrapy.extensions.telnet] INFO: Telnet Password: c2363520ae973865
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:16:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:16:34 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:16:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:16:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:16:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:18:52 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:18:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:18:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:18:52 [scrapy.extensions.telnet] INFO: Telnet Password: a093ec02e72f3827
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:18:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:18:53 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:18:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:18:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:19:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:25 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:20:25 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:20:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:20:25 [scrapy.extensions.telnet] INFO: Telnet Password: a8c1692e65b3db7f
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:20:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:20:25 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:20:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:20:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:20:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:20:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:21:56 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:21:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:21:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:21:56 [scrapy.extensions.telnet] INFO: Telnet Password: 8cb8748d03ec26eb
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:21:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:21:56 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:21:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:21:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:21:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:21:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:25:08 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:25:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:25:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:25:08 [scrapy.extensions.telnet] INFO: Telnet Password: 1bbef3d4ebbf8b6e
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:25:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:25:08 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:25:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:25:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:25:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:25:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:25:59 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:25:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:25:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:25:59 [scrapy.extensions.telnet] INFO: Telnet Password: 83c0debd8e1c3ddb
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:25:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:25:59 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:25:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:25:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:25:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:26:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:34:46 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:34:46 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:34:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:34:46 [scrapy.extensions.telnet] INFO: Telnet Password: 10baddf48c2095ac
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:34:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:34:46 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:34:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:34:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:34:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:34:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:34:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:35:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:35:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:35:19 [scrapy.extensions.telnet] INFO: Telnet Password: 513f1139bc7ca114
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:35:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:35:19 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:35:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:35:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:35:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:35:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:35:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:17 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:36:17 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:36:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:36:17 [scrapy.extensions.telnet] INFO: Telnet Password: ee28b19a62bdaf74
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:36:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:36:17 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:36:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:36:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:36:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:54 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:36:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:36:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:36:54 [scrapy.extensions.telnet] INFO: Telnet Password: 7d975548396161ed
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:36:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:36:54 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:36:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:36:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:36:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:36:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:36:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:37:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:37:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:37:59 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:37:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:37:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:37:59 [scrapy.extensions.telnet] INFO: Telnet Password: a8f5d7586c9219b6
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:37:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:37:59 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:37:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:37:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:38:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:38:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:32 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:38:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:38:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:38:32 [scrapy.extensions.telnet] INFO: Telnet Password: 266452011b91eec9
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:38:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:38:32 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:38:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:38:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:38:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:38:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:38:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:39:44 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:39:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:39:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:40:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:40:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:40:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:41:59 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-24 21:41:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-24 21:41:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-24 21:41:59 [scrapy.extensions.telnet] INFO: Telnet Password: a1dacafe6a7d38d5
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-24 21:41:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-24 21:41:59 [scrapy.core.engine] INFO: Spider opened
2019-10-24 21:41:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 21:41:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-24 21:41:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0> (referer: None)
2019-10-24 21:42:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 21:42:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:16 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2019-10-24 22:05:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/34261089?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-24 22:05:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/28481921?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-24 22:05:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/33753181?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-24 22:05:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
2019-10-24 22:05:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/vacancy/32322427?query=Data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Data+science&page=0)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 29, in parse_vacancy_page
    print(vacancy_title)
OSError: [Errno 22] Invalid argument
2019-10-27 14:03:29 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:03:29 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:03:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:03:29 [scrapy.extensions.telnet] INFO: Telnet Password: 28987511e2f5fcd0
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:03:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-27 14:03:30 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:03:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:03:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:03:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:03:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:03:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:35 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:04:35 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:04:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:04:35 [scrapy.extensions.telnet] INFO: Telnet Password: 6847d3d8865f1bed
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:04:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-10-27 14:04:35 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:04:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:04:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:04:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:04:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:04:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:05:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:05:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:05:02 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 14:05:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof>: HTTP status code is not handled or not allowed
2019-10-27 14:05:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:20 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:05:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>: HTTP status code is not handled or not allowed
2019-10-27 14:05:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:05:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330>: HTTP status code is not handled or not allowed
2019-10-27 14:05:22 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:05:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873>: HTTP status code is not handled or not allowed
2019-10-27 14:05:24 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:05:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862>: HTTP status code is not handled or not allowed
2019-10-27 14:05:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:05:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585>: HTTP status code is not handled or not allowed
2019-10-27 14:05:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:05:26 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662>: HTTP status code is not handled or not allowed
2019-10-27 14:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:05:28 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel>: HTTP status code is not handled or not allowed
2019-10-27 14:05:29 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:05:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458>: HTTP status code is not handled or not allowed
2019-10-27 14:05:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:05:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter>: HTTP status code is not handled or not allowed
2019-10-27 14:05:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:05:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768>: HTTP status code is not handled or not allowed
2019-10-27 14:05:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:05:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770>: HTTP status code is not handled or not allowed
2019-10-27 14:05:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:05:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860>: HTTP status code is not handled or not allowed
2019-10-27 14:05:35 [scrapy.extensions.logstats] INFO: Crawled 50 pages (at 50 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:05:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:05:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746>: HTTP status code is not handled or not allowed
2019-10-27 14:05:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:05:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863>: HTTP status code is not handled or not allowed
2019-10-27 14:05:38 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:05:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/889>: HTTP status code is not handled or not allowed
2019-10-27 14:05:39 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:05:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668>: HTTP status code is not handled or not allowed
2019-10-27 14:05:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:05:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602>: HTTP status code is not handled or not allowed
2019-10-27 14:05:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:05:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976>: HTTP status code is not handled or not allowed
2019-10-27 14:05:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:05:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:05:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:05:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657>: HTTP status code is not handled or not allowed
2019-10-27 14:05:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:05:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/616>: HTTP status code is not handled or not allowed
2019-10-27 14:05:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:05:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072>: HTTP status code is not handled or not allowed
2019-10-27 14:05:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:05:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225>: HTTP status code is not handled or not allowed
2019-10-27 14:05:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:05:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307>: HTTP status code is not handled or not allowed
2019-10-27 14:05:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:05:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131>: HTTP status code is not handled or not allowed
2019-10-27 14:05:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:05:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:05:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804>: HTTP status code is not handled or not allowed
2019-10-27 14:05:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:05:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452>: HTTP status code is not handled or not allowed
2019-10-27 14:05:58 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:05:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730>: HTTP status code is not handled or not allowed
2019-10-27 14:05:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:06:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556>: HTTP status code is not handled or not allowed
2019-10-27 14:06:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:06:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421>: HTTP status code is not handled or not allowed
2019-10-27 14:06:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:21 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:06:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188>: HTTP status code is not handled or not allowed
2019-10-27 14:06:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:06:23 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959>: HTTP status code is not handled or not allowed
2019-10-27 14:06:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:06:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:06:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:06:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793>: HTTP status code is not handled or not allowed
2019-10-27 14:06:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:06:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281>: HTTP status code is not handled or not allowed
2019-10-27 14:06:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:06:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743>: HTTP status code is not handled or not allowed
2019-10-27 14:06:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:06:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358>: HTTP status code is not handled or not allowed
2019-10-27 14:06:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:06:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575>: HTTP status code is not handled or not allowed
2019-10-27 14:06:35 [scrapy.extensions.logstats] INFO: Crawled 101 pages (at 51 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:06:36 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32463248?query=data%20science)
2019-10-27 14:06:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721>: HTTP status code is not handled or not allowed
2019-10-27 14:06:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:06:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232>: HTTP status code is not handled or not allowed
2019-10-27 14:06:39 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:06:39 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769>: HTTP status code is not handled or not allowed
2019-10-27 14:06:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:06:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/25>: HTTP status code is not handled or not allowed
2019-10-27 14:06:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:06:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184>: HTTP status code is not handled or not allowed
2019-10-27 14:06:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:06:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182>: HTTP status code is not handled or not allowed
2019-10-27 14:06:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:06:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:07:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269>: HTTP status code is not handled or not allowed
2019-10-27 14:07:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:07:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307>: HTTP status code is not handled or not allowed
2019-10-27 14:07:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:07:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:07:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902>: HTTP status code is not handled or not allowed
2019-10-27 14:07:33 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:07:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749>: HTTP status code is not handled or not allowed
2019-10-27 14:07:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:35 [scrapy.extensions.logstats] INFO: Crawled 148 pages (at 47 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:07:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:37 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:07:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798>: HTTP status code is not handled or not allowed
2019-10-27 14:07:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:07:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986>: HTTP status code is not handled or not allowed
2019-10-27 14:07:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 14:07:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814>: HTTP status code is not handled or not allowed
2019-10-27 14:07:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:07:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:07:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od>: HTTP status code is not handled or not allowed
2019-10-27 14:07:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:07:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858>: HTTP status code is not handled or not allowed
2019-10-27 14:07:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 14:07:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004>: HTTP status code is not handled or not allowed
2019-10-27 14:07:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 14:07:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984>: HTTP status code is not handled or not allowed
2019-10-27 14:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:07:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 14:07:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298>: HTTP status code is not handled or not allowed
2019-10-27 14:07:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 14:07:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078>: HTTP status code is not handled or not allowed
2019-10-27 14:08:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 14:08:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130>: HTTP status code is not handled or not allowed
2019-10-27 14:08:03 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 14:08:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063>: HTTP status code is not handled or not allowed
2019-10-27 14:08:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 14:08:05 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211>: HTTP status code is not handled or not allowed
2019-10-27 14:08:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 14:08:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389>: HTTP status code is not handled or not allowed
2019-10-27 14:08:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:22 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 14:08:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301>: HTTP status code is not handled or not allowed
2019-10-27 14:08:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:08:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:34 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 14:08:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500>: HTTP status code is not handled or not allowed
2019-10-27 14:08:35 [scrapy.extensions.logstats] INFO: Crawled 197 pages (at 49 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:08:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 14:08:35 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/139>: HTTP status code is not handled or not allowed
2019-10-27 14:08:36 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 14:08:36 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/162>: HTTP status code is not handled or not allowed
2019-10-27 14:08:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 14:08:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186>: HTTP status code is not handled or not allowed
2019-10-27 14:08:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 14:08:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595>: HTTP status code is not handled or not allowed
2019-10-27 14:08:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:08:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 14:08:57 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069>: HTTP status code is not handled or not allowed
2019-10-27 14:08:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 14:08:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110>: HTTP status code is not handled or not allowed
2019-10-27 14:08:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 14:08:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435>: HTTP status code is not handled or not allowed
2019-10-27 14:09:00 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 14:09:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272>: HTTP status code is not handled or not allowed
2019-10-27 14:09:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:07 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 14:09:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245>: HTTP status code is not handled or not allowed
2019-10-27 14:09:08 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 14:09:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114>: HTTP status code is not handled or not allowed
2019-10-27 14:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:11 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 14:09:11 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687>: HTTP status code is not handled or not allowed
2019-10-27 14:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:19 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 14:09:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/854>: HTTP status code is not handled or not allowed
2019-10-27 14:09:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 14:09:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/217737>: HTTP status code is not handled or not allowed
2019-10-27 14:09:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 14:09:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2869446>: HTTP status code is not handled or not allowed
2019-10-27 14:09:31 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 14:09:31 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3289413>: HTTP status code is not handled or not allowed
2019-10-27 14:09:32 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 14:09:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/62126>: HTTP status code is not handled or not allowed
2019-10-27 14:09:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:09:35 [scrapy.extensions.logstats] INFO: Crawled 247 pages (at 50 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:09:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33847716?query=data%20science)
2019-10-27 14:09:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1304>: HTTP status code is not handled or not allowed
2019-10-27 14:09:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
2019-10-27 14:09:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/80>: HTTP status code is not handled or not allowed
2019-10-27 14:09:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3832?dpt=3832-3832-const> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
2019-10-27 14:09:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3832?dpt=3832-3832-const>: HTTP status code is not handled or not allowed
2019-10-27 14:09:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:09:47 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3358623> (referer: https://spb.hh.ru/vacancy/31015468?query=data%20science)
2019-10-27 14:09:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3358623>: HTTP status code is not handled or not allowed
2019-10-27 14:09:49 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/97598> (referer: https://spb.hh.ru/vacancy/33921789?query=data%20science)
2019-10-27 14:09:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/97598>: HTTP status code is not handled or not allowed
2019-10-27 14:09:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3491569> (referer: https://spb.hh.ru/vacancy/33912707?query=data%20science)
2019-10-27 14:09:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3491569>: HTTP status code is not handled or not allowed
2019-10-27 14:09:51 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/14153> (referer: https://spb.hh.ru/vacancy/33497437?query=data%20science)
2019-10-27 14:09:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/14153>: HTTP status code is not handled or not allowed
2019-10-27 14:09:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1587> (referer: https://spb.hh.ru/vacancy/34031778?query=data%20science)
2019-10-27 14:09:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1587>: HTTP status code is not handled or not allowed
2019-10-27 14:09:54 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2558659> (referer: https://spb.hh.ru/vacancy/33813402?query=data%20science)
2019-10-27 14:09:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2558659>: HTTP status code is not handled or not allowed
2019-10-27 14:09:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
2019-10-27 14:09:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/52729>: HTTP status code is not handled or not allowed
2019-10-27 14:09:56 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2097195> (referer: https://spb.hh.ru/vacancy/31252237?query=data%20science)
2019-10-27 14:09:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2097195>: HTTP status code is not handled or not allowed
2019-10-27 14:09:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 14:09:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 115681,
 'downloader/request_count': 265,
 'downloader/request_method_count/GET': 265,
 'downloader/response_bytes': 7544079,
 'downloader/response_count': 265,
 'downloader/response_status_count/200': 176,
 'downloader/response_status_count/404': 89,
 'dupefilter/filtered': 79,
 'elapsed_time_seconds': 321.248594,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 11, 9, 56, 752102),
 'httperror/response_ignored_count': 89,
 'httperror/response_ignored_status_count/404': 89,
 'log_count/DEBUG': 266,
 'log_count/INFO': 104,
 'request_depth_max': 10,
 'response_received_count': 265,
 'scheduler/dequeued': 265,
 'scheduler/dequeued/memory': 265,
 'scheduler/enqueued': 265,
 'scheduler/enqueued/memory': 265,
 'start_time': datetime.datetime(2019, 10, 27, 11, 4, 35, 503508)}
2019-10-27 14:09:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 14:21:00 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:21:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:21:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:21:00 [scrapy.extensions.telnet] INFO: Telnet Password: 625f1a4e3692ef70
2019-10-27 14:21:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:21:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:21:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:21:01 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:21:01 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:21:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:21:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': [' /  ']}
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:21:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': [' /  ']}
2019-10-27 14:21:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': [' 150\xa0000 .   ']}
2019-10-27 14:21:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': [' /  ']}
2019-10-27 14:29:03 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:29:03 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:29:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:29:03 [scrapy.extensions.telnet] INFO: Telnet Password: e66a13b7837a2c9d
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:29:03 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:29:03 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:29:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:29:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:29:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:29:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': [' /  '], '_id': ObjectId('5db57f80a2de5eb72a9c98a0')}
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:17 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 16 pages/min), scraped 1 items (at 1 items/min)
2019-10-27 14:30:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': [' 150\xa0000 .   '], '_id': ObjectId('5db57fc9a2de5eb72a9c98a1')}
2019-10-27 14:30:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': [' /  '], '_id': ObjectId('5db57fcca2de5eb72a9c98a2')}
2019-10-27 14:30:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': [' /  '], '_id': ObjectId('5db57fcda2de5eb72a9c98a3')}
2019-10-27 14:30:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': [' /  '], '_id': ObjectId('5db57fcda2de5eb72a9c98a4')}
2019-10-27 14:30:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': [' /  '], '_id': ObjectId('5db57fcea2de5eb72a9c98a5')}
2019-10-27 14:30:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': [' /  '], '_id': ObjectId('5db57fcea2de5eb72a9c98a6')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': [' /  '], '_id': ObjectId('5db57fcea2de5eb72a9c98a7')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': [' /  '], '_id': ObjectId('5db57fcfa2de5eb72a9c98a8')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': [' /  '], '_id': ObjectId('5db57fcfa2de5eb72a9c98a9')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': [' /  '], '_id': ObjectId('5db57fcfa2de5eb72a9c98aa')}
2019-10-27 14:30:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': [' /  '], '_id': ObjectId('5db57fcfa2de5eb72a9c98ab')}
2019-10-27 14:30:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': [' /  '], '_id': ObjectId('5db57fd0a2de5eb72a9c98ac')}
2019-10-27 14:30:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': [' /  '], '_id': ObjectId('5db57fd0a2de5eb72a9c98ad')}
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:30:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': [' /  '], '_id': ObjectId('5db57fd0a2de5eb72a9c98ae')}
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:30:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': [' /  '], '_id': ObjectId('5db57fd0a2de5eb72a9c98af')}
2019-10-27 14:30:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': [' 130\xa0000 .   '], '_id': ObjectId('5db57fd1a2de5eb72a9c98b0')}
2019-10-27 14:30:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': [' 100\xa0000  150\xa0000 .  '], '_id': ObjectId('5db57fd1a2de5eb72a9c98b1')}
2019-10-27 14:31:36 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:31:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:31:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:31:36 [scrapy.extensions.telnet] INFO: Telnet Password: dc506c537b6d77e9
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:31:36 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:31:36 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:31:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:31:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:31:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580199d1fdde02f064891')}
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db580259d1fdde02f064892')}
2019-10-27 14:31:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580259d1fdde02f064893')}
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580269d1fdde02f064894')}
2019-10-27 14:31:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db580269d1fdde02f064895')}
2019-10-27 14:31:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db580269d1fdde02f064896')}
2019-10-27 14:31:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db580269d1fdde02f064897')}
2019-10-27 14:31:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db580289d1fdde02f064898')}
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:31:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db580289d1fdde02f064899')}
2019-10-27 14:31:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db580289d1fdde02f06489a')}
2019-10-27 14:32:47 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:32:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:32:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:32:47 [scrapy.extensions.telnet] INFO: Telnet Password: 75bbc2711534d2a7
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:32:47 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:32:47 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:32:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:32:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289da')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289db')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289dc')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db580604d640ee4f41289dd')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289de')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289df')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e0')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e1')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e2')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e3')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e4')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e5')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e6')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e7')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289e8')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db580604d640ee4f41289e9')}
2019-10-27 14:32:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db580604d640ee4f41289ea')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db580604d640ee4f41289eb')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289ec')}
2019-10-27 14:32:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db580604d640ee4f41289ed')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289ee')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289ef')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db580614d640ee4f41289f0')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f1')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f2')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f3')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f4')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f5')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f6')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f7')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f8')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289f9')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289fa')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289fb')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289fc')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289fd')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f41289fe')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', '_id': ObjectId('5db580614d640ee4f41289ff')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (     )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a00')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a01')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a02')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a03')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a04')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a05')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a06')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a07')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a08')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db580614d640ee4f4128a09')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a0a')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a0b')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a0c')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a0d')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a0e')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': [' '], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a0f')}
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:32:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern /     ', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a10')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a11')}
2019-10-27 14:32:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db580614d640ee4f4128a12')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db580624d640ee4f4128a13')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a14')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a15')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a16')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a17')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', ' ', 'SQL', ' ', ' ', ' '], 'salary': ' 350\xa0000 .  ', '_id': ObjectId('5db580624d640ee4f4128a18')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a19')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a1a')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': ' BI (QlikView, Qlik Sense)', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', '', ' ', ' ', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', ' ', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', '   ', 'Cognos BI'], 'salary': ' 30\xa0000  110\xa0000 .  ', '_id': ObjectId('5db580624d640ee4f4128a1b')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': '   ', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a1c')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a1d')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a1e')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', '-', ' '], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a1f')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a20')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a21')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a22')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a23')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': ' HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db580624d640ee4f4128a24')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a25')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a26')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a27')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a28')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', ' ', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', ' ', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a29')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a2a')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a2b')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a2c')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a2d')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a2e')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': ' 260\xa0000  370\xa0000 .   ', '_id': ObjectId('5db580624d640ee4f4128a2f')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a30')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': '   / Tech Lead', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', ' ', ' ', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a31')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a32')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': '  ( )', 'company_name': '  (   )', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a33')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': '  [id20580]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', ' '], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a34')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': '  /  Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a35')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a36')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a37')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': '  ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 100\xa0000  120\xa0000 .   ', '_id': ObjectId('5db580624d640ee4f4128a38')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a39')}
2019-10-27 14:32:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [' ', ' ', '  ', 'SQL', 'ORACLE'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a3a')}
2019-10-27 14:32:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', ' ', 'MS Visio'], 'salary': ' /  ', '_id': ObjectId('5db580624d640ee4f4128a3b')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', ' '], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a3c')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': '-', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a3d')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer /  ', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db580634d640ee4f4128a3e')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a3f')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': '  -', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', ' ', 'Google Tag Manager', ' '], 'salary': ' 60\xa0000  120\xa0000 .  ', '_id': ObjectId('5db580634d640ee4f4128a40')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', '.', 'Google Analytics', 'SEO ', '', '  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a41')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a42')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', ''], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a43')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a44')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a45')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a46')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', ' ', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a47')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': '  (  DWH  BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['-', ' -', '-', ' ', ' ', 'MS Visio', ' ', ' ', 'Big Data', 'Data Science', '  '], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a48')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a49')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master   Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a4a')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a4b')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a4c')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a4d')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a4e')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a4f')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db580634d640ee4f4128a50')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a51')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['  ', 'MS Excel', ' ', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a52')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': ' 2\xa0500  3\xa0500 USD  ', '_id': ObjectId('5db580634d640ee4f4128a53')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a54')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a55')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a56')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': [' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a57')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a58')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) /    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a59')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a5a')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': '  (      )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db580634d640ee4f4128a5b')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a5c')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', ' ', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a5d')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', ' ', 'DWH', ' ', 'Business Development', 'ETL', 'AWS', ' ', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a5e')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a5f')}
2019-10-27 14:32:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a60')}
2019-10-27 14:32:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', ' ', '  ', 'Analytical skills', ' ', 'Time management', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580634d640ee4f4128a61')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR,  ', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', ' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a62')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': ' / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['', ' ', ' ', ' ', '1C: ', ' ', '  ', ' ', '   ', '  ', '  ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a63')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': '  (         )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db580644d640ee4f4128a64')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': ' -', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': [' ', '', ' ', '   ', ' '], 'salary': ' 40\xa0000 .  ', '_id': ObjectId('5db580644d640ee4f4128a65')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33964564?query=data%20science>
{'vacancy_title': 'Market Researcher ( )', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [' ', ' ', ' ', '  ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a66')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/21963813?query=data%20science>
{'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a67')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33998747?query=data%20science>
{'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a68')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' Nissan,  ', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a69')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/)    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a6a')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a6b')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a6c')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a6d')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a6e')}
2019-10-27 14:32:52 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a6f')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a70')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer /  Python', 'company_name': ' -     ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a71')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a72')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', '  ', ''], 'salary': ' 90\xa0000 .  ', '_id': ObjectId('5db580644d640ee4f4128a73')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a74')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31189716?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db580644d640ee4f4128a75')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': [' ', 'SCALA', 'Python', 'Spark', '-'], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a76')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db580644d640ee4f4128a77')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33497437?query=data%20science>
{'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', ' ', 'MS SQL Server', 'Business English'], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a78')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34031778?query=data%20science>
{'vacancy_title': '- (  IoT, Ml)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a79')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32422629?query=data%20science>
{'vacancy_title': 'Senior .NET Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a7a')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933697?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a7b')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813402?query=data%20science>
{'vacancy_title': ' .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': ' 1\xa0800 USD  ', '_id': ObjectId('5db580644d640ee4f4128a7c')}
2019-10-27 14:32:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31252237?query=data%20science>
{'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db580644d640ee4f4128a7d')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813435?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': ' 650 USD  ', '_id': ObjectId('5db580644d640ee4f4128a7e')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933692?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a7f')}
2019-10-27 14:32:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135809?query=data%20science>
{'vacancy_title': 'Senior iOS Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db580644d640ee4f4128a80')}
2019-10-27 14:32:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 14:32:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 81070,
 'downloader/request_count': 176,
 'downloader/request_method_count/GET': 176,
 'downloader/response_bytes': 6407997,
 'downloader/response_count': 176,
 'downloader/response_status_count/200': 176,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 4.918857,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 11, 32, 52, 726385),
 'item_scraped_count': 167,
 'log_count/DEBUG': 344,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 176,
 'scheduler/dequeued': 176,
 'scheduler/dequeued/memory': 176,
 'scheduler/enqueued': 176,
 'scheduler/enqueued/memory': 176,
 'start_time': datetime.datetime(2019, 10, 27, 11, 32, 47, 807528)}
2019-10-27 14:32:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 14:42:10 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:42:10 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:42:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:42:10 [scrapy.extensions.telnet] INFO: Telnet Password: f25a07f900929909
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:42:10 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:42:10 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:42:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:42:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:42:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58293b7091f2069098927')}
2019-10-27 14:42:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db58293b7091f2069098928')}
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:42:33 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db582a9b7091f2069098929')}
2019-10-27 14:42:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db582aab7091f206909892a')}
2019-10-27 14:42:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db582bcb7091f206909892b')}
2019-10-27 14:42:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582bdb7091f206909892c')}
2019-10-27 14:42:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db582beb7091f206909892d')}
2019-10-27 14:42:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db582bfb7091f206909892e')}
2019-10-27 14:43:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582cbb7091f206909892f')}
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db582d6b7091f2069098930')}
2019-10-27 14:43:38 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:43:38 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:43:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:43:38 [scrapy.extensions.telnet] INFO: Telnet Password: 1f41df7e3fbc62de
2019-10-27 14:43:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:43:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:43:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:43:39 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:43:39 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:43:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:43:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ebc297669986efcd1e')}
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ebc297669986efcd1f')}
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db582ebc297669986efcd20')}
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db582ebc297669986efcd21')}
2019-10-27 14:43:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db582ebc297669986efcd22')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd23')}
2019-10-27 14:43:40 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd24')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd25')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd26')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd27')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd28')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd29')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd2a')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd2b')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd2c')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd2d')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2873>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/41862>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3529?dpt=3529-3529-prof>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db582ecc297669986efcd2e')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db582ecc297669986efcd2f')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db582ecc297669986efcd30')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd31')}
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/789662>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1947330>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4585>: HTTP status code is not handled or not allowed
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd32')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd33')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd34')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd35')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd36')}
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db582ecc297669986efcd37')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db582ecc297669986efcd38')}
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd39')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd3a')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd3b')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd3c')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd3d')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd3e')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd3f')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd40')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd41')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2061860>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/83746>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd42')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd43')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/4934?dpt=bil-4934-beel>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2768>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd44')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44458>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/662769?dpt=662769-662769-piter>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/826770>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', '_id': ObjectId('5db582edc297669986efcd45')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/22863>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/889>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1528668>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/813602>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/245657>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/63225>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/834307>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1040131>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/616>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1876072>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd46')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd47')}
2019-10-27 14:43:41 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2416804>: HTTP status code is not handled or not allowed
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd48')}
2019-10-27 14:43:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582edc297669986efcd49')}
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:43:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd4a')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2625959>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db582eec297669986efcd4b')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd4c')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1245452>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd4d')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/49556>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1263188>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3931730>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd4e')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/815269>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd4f')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd50')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168421>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/168307>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd51')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd52')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1795976>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3659721>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern /     ', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd53')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd54')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db582eec297669986efcd55')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/802184>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd56')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2618793>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': [' '], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd57')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/314182>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd58')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (     )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd59')}
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd5a')}
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/9281>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/45743>: HTTP status code is not handled or not allowed
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd5b')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db582eec297669986efcd5c')}
2019-10-27 14:43:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', ' ', 'SQL', ' ', ' ', ' '], 'salary': ' 350\xa0000 .  ', '_id': ObjectId('5db582efc297669986efcd5d')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd5e')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd5f')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': ' BI (QlikView, Qlik Sense)', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', '', ' ', ' ', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', ' ', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', '   ', 'Cognos BI'], 'salary': ' 30\xa0000  110\xa0000 .  ', '_id': ObjectId('5db582efc297669986efcd60')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': '   ', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd61')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd62')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3543358>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/681575>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', '-', ' '], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd63')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd64')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd65')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': ' HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db582efc297669986efcd66')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd67')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd68')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6769>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/25>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd69')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd6a')}
2019-10-27 14:43:43 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2232>: HTTP status code is not handled or not allowed
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd6b')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd6c')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', ' ', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', ' ', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd6d')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', ' ', 'MS Visio'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd6e')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd6f')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': '-', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd70')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', ' '], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd71')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd72')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd73')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd74')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' /  ', '_id': ObjectId('5db582efc297669986efcd75')}
2019-10-27 14:43:43 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': ' 260\xa0000  370\xa0000 .   ', '_id': ObjectId('5db582efc297669986efcd76')}
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd77')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': '  ( )', 'company_name': '  (   )', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd78')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd79')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': '   / Tech Lead', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', ' ', ' ', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd7a')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': '  [id20580]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', ' '], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd7b')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd7c')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': '  /  Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd7d')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': '  ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 100\xa0000  120\xa0000 .   ', '_id': ObjectId('5db582f0c297669986efcd7e')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/15478?dpt=mailru-15478-od>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2661749>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2160858>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/208902>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2610798>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd7f')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/130986>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd80')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [' ', ' ', '  ', 'SQL', 'ORACLE'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd81')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3425814>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/213004>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/86298>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3984>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/914078>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd82')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd83')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd84')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd85')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', ' ', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd86')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd87')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/638130>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1574063>: HTTP status code is not handled or not allowed
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': '  (  DWH  BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['-', ' -', '-', ' ', ' ', 'MS Visio', ' ', ' ', 'Big Data', 'Data Science', '  '], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd88')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master   Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd89')}
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd8a')}
2019-10-27 14:43:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' /  ', '_id': ObjectId('5db582f0c297669986efcd8b')}
2019-10-27 14:43:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3156389>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db582f1c297669986efcd8c')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd8d')}
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/574211>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer /  ', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db582f1c297669986efcd8e')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd8f')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd90')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', '.', 'Google Analytics', 'SEO ', '', '  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd91')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6301>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/6500>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/162>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd92')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': '  -', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', ' ', 'Google Tag Manager', ' '], 'salary': ' 60\xa0000  120\xa0000 .  ', '_id': ObjectId('5db582f1c297669986efcd93')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', ''], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd94')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd95')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd96')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': [' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd97')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd98')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd99')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) /    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd9a')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', ' ', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd9b')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd9c')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', ' ', 'DWH', ' ', 'Business Development', 'ETL', 'AWS', ' ', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd9d')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcd9e')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': '  (      )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db582f1c297669986efcd9f')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': ' / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['', ' ', ' ', ' ', '1C: ', ' ', '  ', ' ', '   ', '  ', '  ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcda0')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcda1')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': '  (         )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db582f1c297669986efcda2')}
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 14:43:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/1186>: HTTP status code is not handled or not allowed
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:43:45 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcda3')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR,  ', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', ' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcda4')}
2019-10-27 14:43:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', ' ', '  ', 'Analytical skills', ' ', 'Time management', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db582f1c297669986efcda5')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': ' -', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': [' ', '', ' ', '   ', ' '], 'salary': ' 40\xa0000 .  ', '_id': ObjectId('5db582f2c297669986efcda6')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/139>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': ' 2\xa0500  3\xa0500 USD  ', '_id': ObjectId('5db582f2c297669986efcda7')}
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcda8')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/79595>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['  ', 'MS Excel', ' ', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcda9')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/654435>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2559110>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/3455245>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/44272>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/633069>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/)    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdaa')}
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' Nissan,  ', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdab')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/2617114>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdac')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdad')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdae')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdaf')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdb0')}
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdb1')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdb2')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer /  Python', 'company_name': ' -     ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdb3')}
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/11687>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', '  ', ''], 'salary': ' 90\xa0000 .  ', '_id': ObjectId('5db582f2c297669986efcdb4')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': [' ', 'SCALA', 'Python', 'Spark', '-'], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdb5')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 14:43:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://spb.hh.ru/vacancy/spb.hh.ru/employer/854>: HTTP status code is not handled or not allowed
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db582f2c297669986efcdb6')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:43:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db582f2c297669986efcdb7')}
2019-10-27 14:43:46 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://spb.hh.ru/vacancy/spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 14:44:14 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:44:14 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:44:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:44:14 [scrapy.extensions.telnet] INFO: Telnet Password: bcb6a95f99e29ecc
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:44:14 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:44:14 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:44:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:44:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:44:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:45:17 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:45:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5834d163bfec383bbce21')}
2019-10-27 14:46:13 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:46:13 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:46:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:46:13 [scrapy.extensions.telnet] INFO: Telnet Password: fc76d632266d49a0
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:46:13 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:46:13 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:46:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:46:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:46:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:46:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58389b851706e78ab869b')}
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db5838ab851706e78ab869c')}
2019-10-27 14:46:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5838bb851706e78ab869d')}
2019-10-27 14:46:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db5838cb851706e78ab869e')}
2019-10-27 14:46:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db5838cb851706e78ab869f')}
2019-10-27 14:46:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db5838db851706e78ab86a0')}
2019-10-27 14:46:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5838eb851706e78ab86a1')}
2019-10-27 14:46:50 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:46:50 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:46:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:46:50 [scrapy.extensions.telnet] INFO: Telnet Password: 733dffea90835520
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:46:50 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:46:50 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:46:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:46:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:46:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:46:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db583af67b788ad4504e4d8')}
2019-10-27 14:46:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db583af67b788ad4504e4d9')}
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:46:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db583b067b788ad4504e4da')}
2019-10-27 14:46:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db583b167b788ad4504e4db')}
2019-10-27 14:46:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db583b167b788ad4504e4dc')}
2019-10-27 14:46:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db583b167b788ad4504e4dd')}
2019-10-27 14:46:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b267b788ad4504e4de')}
2019-10-27 14:46:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db583b367b788ad4504e4df')}
2019-10-27 14:46:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b367b788ad4504e4e0')}
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db583b467b788ad4504e4e1')}
2019-10-27 14:47:00 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:47:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b467b788ad4504e4e2')}
2019-10-27 14:47:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db583b467b788ad4504e4e3')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b567b788ad4504e4e4')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db583b567b788ad4504e4e5')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db583b567b788ad4504e4e6')}
2019-10-27 14:47:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db583b567b788ad4504e4e7')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db583b667b788ad4504e4e8')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db583b667b788ad4504e4e9')}
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b667b788ad4504e4ea')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b667b788ad4504e4eb')}
2019-10-27 14:47:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db583b667b788ad4504e4ec')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db583b767b788ad4504e4ed')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db583b767b788ad4504e4ee')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db583b767b788ad4504e4ef')}
2019-10-27 14:47:03 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b767b788ad4504e4f0')}
2019-10-27 14:47:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b867b788ad4504e4f1')}
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583b867b788ad4504e4f2')}
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db583b867b788ad4504e4f3')}
2019-10-27 14:47:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db583b867b788ad4504e4f4')}
2019-10-27 14:48:11 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:48:11 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:48:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:48:11 [scrapy.extensions.telnet] INFO: Telnet Password: 06e8230bc5e9ee22
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:48:11 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:48:11 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:48:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:48:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe08a')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe08b')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db583fcb72f8ad41bffe08c')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe08d')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe08e')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe08f')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe090')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe091')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe092')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe093')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe094')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe095')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe096')}
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe097')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:48:12 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe098')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db583fcb72f8ad41bffe099')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db583fcb72f8ad41bffe09a')}
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:48:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:48:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db583fcb72f8ad41bffe09b')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db583fdb72f8ad41bffe09c')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe09d')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe09e')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe09f')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a0')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a1')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', '_id': ObjectId('5db583fdb72f8ad41bffe0a2')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a3')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a4')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a5')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db583fdb72f8ad41bffe0a6')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a7')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a8')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0a9')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0aa')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0ab')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0ac')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0ad')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0ae')}
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0af')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0b0')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:48:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db583fdb72f8ad41bffe0b1')}
2019-10-27 14:48:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0b2')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0b3')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0b4')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0b5')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0b6')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0b7')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db583feb72f8ad41bffe0b8')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0b9')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0ba')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0bb')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0bc')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0bd')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern /     ', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0be')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db583feb72f8ad41bffe0bf')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c0')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': [' '], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c1')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c2')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (     )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c3')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c4')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c5')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c6')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', ' ', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', ' ', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c7')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c8')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0c9')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0ca')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0cb')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', ' ', 'SQL', ' ', ' ', ' '], 'salary': ' 350\xa0000 .  ', '_id': ObjectId('5db583feb72f8ad41bffe0cc')}
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': '   ', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0cd')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db583feb72f8ad41bffe0ce')}
2019-10-27 14:48:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': ' BI (QlikView, Qlik Sense)', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', '', ' ', ' ', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', ' ', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', '   ', 'Cognos BI'], 'salary': ' 30\xa0000  110\xa0000 .  ', '_id': ObjectId('5db583feb72f8ad41bffe0cf')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', '-', ' '], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d0')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d1')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d2')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d3')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': ' HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d4')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d5')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d6')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d7')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d8')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/15478> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0d9')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', ' ', 'MS Visio'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0da')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', ' '], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0db')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0dc')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0dd')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0de')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0df')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e0')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e1')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': ' 260\xa0000  370\xa0000 .   ', '_id': ObjectId('5db583ffb72f8ad41bffe0e2')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': '   / Tech Lead', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', ' ', ' ', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e3')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e4')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': '  [id20580]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', ' '], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e5')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': '  /  Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e6')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e7')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': '  ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 100\xa0000  120\xa0000 .   ', '_id': ObjectId('5db583ffb72f8ad41bffe0e8')}
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': '  ( )', 'company_name': '  (   )', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0e9')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0ea')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [' ', ' ', '  ', 'SQL', 'ORACLE'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0eb')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' /  ', '_id': ObjectId('5db583ffb72f8ad41bffe0ec')}
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 14:48:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': '-', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0ed')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0ee')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0ef')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', ' ', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f0')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': '  (  DWH  BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['-', ' -', '-', ' ', ' ', 'MS Visio', ' ', ' ', 'Big Data', 'Data Science', '  '], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f1')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f2')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f3')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master   Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f4')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f5')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f6')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db58400b72f8ad41bffe0f7')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0f8')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer /  ', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db58400b72f8ad41bffe0f9')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0fa')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', '.', 'Google Analytics', 'SEO ', '', '  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0fb')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0fc')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': '  -', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', ' ', 'Google Tag Manager', ' '], 'salary': ' 60\xa0000  120\xa0000 .  ', '_id': ObjectId('5db58400b72f8ad41bffe0fd')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0fe')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', ''], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe0ff')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe100')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe101')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR,  ', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', ' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe102')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', ' ', '  ', 'Analytical skills', ' ', 'Time management', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe103')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': ' -', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': [' ', '', ' ', '   ', ' '], 'salary': ' 40\xa0000 .  ', '_id': ObjectId('5db58400b72f8ad41bffe104')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe105')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe106')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': ' 2\xa0500  3\xa0500 USD  ', '_id': ObjectId('5db58400b72f8ad41bffe107')}
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe108')}
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) /    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58400b72f8ad41bffe109')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['  ', 'MS Excel', ' ', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe10a')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', ' ', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe10b')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': [' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe10c')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe10d')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe10e')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': '  (      )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db58401b72f8ad41bffe10f')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', ' ', 'DWH', ' ', 'Business Development', 'ETL', 'AWS', ' ', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe110')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe111')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe112')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': ' / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['', ' ', ' ', ' ', '1C: ', ' ', '  ', ' ', '   ', '  ', '  ', ' '], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe113')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe114')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': '  (         )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db58401b72f8ad41bffe115')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/)    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe116')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' Nissan,  ', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe117')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe118')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe119')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe11a')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe11b')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe11c')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe11d')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe11e')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe11f')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', '  ', ''], 'salary': ' 90\xa0000 .  ', '_id': ObjectId('5db58401b72f8ad41bffe120')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': [' ', 'SCALA', 'Python', 'Spark', '-'], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe121')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db58401b72f8ad41bffe122')}
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31189716?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db58401b72f8ad41bffe123')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer /  Python', 'company_name': ' -     ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' /  ', '_id': ObjectId('5db58401b72f8ad41bffe124')}
2019-10-27 14:48:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33497437?query=data%20science>
{'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', ' ', 'MS SQL Server', 'Business English'], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe125')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34031778?query=data%20science>
{'vacancy_title': '- (  IoT, Ml)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe126')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33964564?query=data%20science>
{'vacancy_title': 'Market Researcher ( )', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [' ', ' ', ' ', '  ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe127')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/21963813?query=data%20science>
{'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe128')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813402?query=data%20science>
{'vacancy_title': ' .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': ' 1\xa0800 USD  ', '_id': ObjectId('5db58402b72f8ad41bffe129')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33998747?query=data%20science>
{'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe12a')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813435?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': ' 650 USD  ', '_id': ObjectId('5db58402b72f8ad41bffe12b')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3832> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33354237?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933692?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe12c')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135809?query=data%20science>
{'vacancy_title': 'Senior iOS Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe12d')}
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32422629?query=data%20science>
{'vacancy_title': 'Senior .NET Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe12e')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3358623> (referer: https://spb.hh.ru/vacancy/31015468?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/97598> (referer: https://spb.hh.ru/vacancy/33921789?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/14153> (referer: https://spb.hh.ru/vacancy/33497437?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933697?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58402b72f8ad41bffe12f')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3491569> (referer: https://spb.hh.ru/vacancy/33912707?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2558659> (referer: https://spb.hh.ru/vacancy/33813402?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1587> (referer: https://spb.hh.ru/vacancy/34031778?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31252237?query=data%20science>
{'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db58402b72f8ad41bffe130')}
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2097195> (referer: https://spb.hh.ru/vacancy/31252237?query=data%20science)
2019-10-27 14:48:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 14:48:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 113959,
 'downloader/request_count': 265,
 'downloader/request_method_count/GET': 265,
 'downloader/response_bytes': 10886536,
 'downloader/response_count': 265,
 'downloader/response_status_count/200': 265,
 'dupefilter/filtered': 79,
 'elapsed_time_seconds': 6.789583,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 11, 48, 18, 556228),
 'item_scraped_count': 167,
 'log_count/DEBUG': 433,
 'log_count/INFO': 10,
 'request_depth_max': 10,
 'response_received_count': 265,
 'scheduler/dequeued': 265,
 'scheduler/dequeued/memory': 265,
 'scheduler/enqueued': 265,
 'scheduler/enqueued/memory': 265,
 'start_time': datetime.datetime(2019, 10, 27, 11, 48, 11, 766645)}
2019-10-27 14:48:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 14:49:22 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:49:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:49:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:49:22 [scrapy.extensions.telnet] INFO: Telnet Password: 9ff044b791d60db5
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:49:22 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:49:22 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:49:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:49:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db584438cdd0a411a80d282')}
2019-10-27 14:49:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db584438cdd0a411a80d283')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db584438cdd0a411a80d284')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db584438cdd0a411a80d285')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db584438cdd0a411a80d286')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db584438cdd0a411a80d287')}
2019-10-27 14:49:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db584438cdd0a411a80d288')}
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db584448cdd0a411a80d289')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db584448cdd0a411a80d28a')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db584448cdd0a411a80d28b')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db584448cdd0a411a80d28c')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db584448cdd0a411a80d28d')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db584448cdd0a411a80d28e')}
2019-10-27 14:49:24 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db584448cdd0a411a80d28f')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db584448cdd0a411a80d290')}
2019-10-27 14:49:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db584448cdd0a411a80d291')}
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:46 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:53:46 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:53:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:53:46 [scrapy.extensions.telnet] INFO: Telnet Password: c49b68b47c9a7b0e
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:53:46 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:53:46 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:53:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:53:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd07a')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd07b')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd07c')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db5854b1b419135735dd07d')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd07e')}
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd07f')}
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd080')}
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd081')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd082')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd083')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd084')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd085')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd086')}
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd087')}
2019-10-27 14:53:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:53:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854b1b419135735dd088')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db5854c1b419135735dd089')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db5854c1b419135735dd08a')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd08b')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db5854c1b419135735dd08c')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd08d')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd08e')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', '_id': ObjectId('5db5854c1b419135735dd08f')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd090')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db5854c1b419135735dd091')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd092')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd093')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd094')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd095')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd096')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd097')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd098')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd099')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd09a')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd09b')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd09c')}
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd09d')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd09e')}
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:53:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5854c1b419135735dd09f')}
2019-10-27 14:53:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:53:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:53:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 14:53:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5854d1b419135735dd0a0')}
2019-10-27 14:54:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db585801b419135735dd0a1')}
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:54:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db585801b419135735dd0a2')}
2019-10-27 14:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:54:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:54:47 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db585871b419135735dd0a3')}
2019-10-27 14:54:48 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db585881b419135735dd0a4')}
2019-10-27 14:54:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:49 [scrapy.extensions.logstats] INFO: Crawled 59 pages (at 59 pages/min), scraped 43 items (at 43 items/min)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 14:54:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/22863>
{'company_url': 'http://www.protei.ru', '_id': ObjectId('5db585891b419135735dd0a5')}
2019-10-27 14:54:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/826770>
{'company_url': 'http://www.weareworldquant.com', '_id': ObjectId('5db5858a1b419135735dd0a6')}
2019-10-27 14:54:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/813602>
{'company_url': 'http://www.kofax.com', '_id': ObjectId('5db5858b1b419135735dd0a7')}
2019-10-27 14:54:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/889>
{'company_url': 'http://www.kodeks.ru', '_id': ObjectId('5db5858c1b419135735dd0a8')}
2019-10-27 14:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 14:54:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 14:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5858d1b419135735dd0a9')}
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', '_id': ObjectId('5db5858d1b419135735dd0aa')}
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5858d1b419135735dd0ab')}
2019-10-27 14:54:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/245657>
{'company_url': 'http://www.gctrials.com/', '_id': ObjectId('5db5858d1b419135735dd0ac')}
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 14:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', '_id': ObjectId('5db5858e1b419135735dd0ad')}
2019-10-27 14:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4934> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/63225>
{'company_url': 'https://vk.com', '_id': ObjectId('5db5858e1b419135735dd0ae')}
2019-10-27 14:54:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2416804>
{'company_url': 'https://www.welltory.com', '_id': ObjectId('5db5858e1b419135735dd0af')}
2019-10-27 14:54:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/168421>
{'company_url': 'http://www.wandisco.com', '_id': ObjectId('5db5858f1b419135735dd0b0')}
2019-10-27 14:54:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1040131>
{'company_url': 'http://segmento.ru', '_id': ObjectId('5db585901b419135735dd0b1')}
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 14:54:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3931730>
{'company_url': 'http://GamesFactoryTalents.com', '_id': ObjectId('5db585911b419135735dd0b2')}
2019-10-27 14:54:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1245452>
{'company_url': 'http://www.cloudlinux.com', '_id': ObjectId('5db585921b419135735dd0b3')}
2019-10-27 14:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 14:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 14:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:54:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 14:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/834307>
{'company_url': 'http://www.dm-matrix.com', '_id': ObjectId('5db585931b419135735dd0b4')}
2019-10-27 14:54:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1263188>
{'company_url': 'http://www.weigandt-consulting.com', '_id': ObjectId('5db585931b419135735dd0b5')}
2019-10-27 14:55:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2625959>
{'company_url': 'http://www.idrnd.ai', '_id': ObjectId('5db585941b419135735dd0b6')}
2019-10-27 14:55:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/616>
{'company_url': 'http://lancktele.com', '_id': ObjectId('5db585941b419135735dd0b7')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db585951b419135735dd0b8')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0b9')}
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0ba')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0bb')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0bc')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0bd')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0be')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern /     ', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0bf')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': [' '], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0c0')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0c1')}
2019-10-27 14:55:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 14:55:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db585951b419135735dd0c2')}
2019-10-27 14:55:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (     )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db585951b419135735dd0c3')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', '_id': ObjectId('5db585961b419135735dd0c4')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585961b419135735dd0c5')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' /  ', '_id': ObjectId('5db585961b419135735dd0c6')}
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' /  ', '_id': ObjectId('5db585961b419135735dd0c7')}
2019-10-27 14:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585961b419135735dd0c8')}
2019-10-27 14:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/45743>
{'company_url': 'http://www.oct-clinicaltrials.com/', '_id': ObjectId('5db585961b419135735dd0c9')}
2019-10-27 14:55:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/9281>
{'company_url': 'http://www.jetbrains.com', '_id': ObjectId('5db585981b419135735dd0ca')}
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/6769>
{'company_url': 'http://www.epam-group.ru', '_id': ObjectId('5db585991b419135735dd0cb')}
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/314182>
{'company_url': 'http://www.saber3d.com', '_id': ObjectId('5db585991b419135735dd0cc')}
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': '   ', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585991b419135735dd0cd')}
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585991b419135735dd0ce')}
2019-10-27 14:55:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0cf')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0d0')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', ' ', 'SQL', ' ', ' ', ' '], 'salary': ' 350\xa0000 .  ', '_id': ObjectId('5db5859a1b419135735dd0d1')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0d2')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': ' BI (QlikView, Qlik Sense)', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', '', ' ', ' ', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', ' ', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', '   ', 'Cognos BI'], 'salary': ' 30\xa0000  110\xa0000 .  ', '_id': ObjectId('5db5859a1b419135735dd0d3')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0d4')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0d5')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', '-', ' '], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0d6')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0d7')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0d8')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': ' HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db5859a1b419135735dd0d9')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0da')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/15478> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0db')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0dc')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', ' ', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', ' ', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0dd')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0de')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0df')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e0')}
2019-10-27 14:55:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/15478> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e1')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e2')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e3')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e4')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e5')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': ' 260\xa0000  370\xa0000 .   ', '_id': ObjectId('5db5859a1b419135735dd0e6')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e7')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e8')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': '   / Tech Lead', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', ' ', ' ', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0e9')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': '  [id20580]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', ' '], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0ea')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0eb')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': '  ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 100\xa0000  120\xa0000 .   ', '_id': ObjectId('5db5859a1b419135735dd0ec')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': '  /  Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0ed')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': '  ( )', 'company_name': '  (   )', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0ee')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0ef')}
2019-10-27 14:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [' ', ' ', '  ', 'SQL', 'ORACLE'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0f0')}
2019-10-27 14:55:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' /  ', '_id': ObjectId('5db5859a1b419135735dd0f1')}
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 14:55:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2160858>
{'company_url': 'http://qleversolutions.ru/', '_id': ObjectId('5db5859b1b419135735dd0f2')}
2019-10-27 14:56:36 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:56:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:56:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:56:37 [scrapy.extensions.telnet] INFO: Telnet Password: a33f6aa7f3cec5d6
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:56:37 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:56:37 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:56:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:56:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:56:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:56:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db585fcfbe96efd8f7b2b89')}
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58601fbe96efd8f7b2b8a')}
2019-10-27 14:56:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db58603fbe96efd8f7b2b8b')}
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db58604fbe96efd8f7b2b8c')}
2019-10-27 14:56:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db58605fbe96efd8f7b2b8d')}
2019-10-27 14:56:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db58609fbe96efd8f7b2b8e')}
2019-10-27 14:56:57 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/3529> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:56:58 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5860afbe96efd8f7b2b8f')}
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:56:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db5860bfbe96efd8f7b2b90')}
2019-10-27 14:56:59 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db5860bfbe96efd8f7b2b91')}
2019-10-27 14:57:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5860cfbe96efd8f7b2b92')}
2019-10-27 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:57:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5860dfbe96efd8f7b2b93')}
2019-10-27 14:57:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5860dfbe96efd8f7b2b94')}
2019-10-27 14:57:12 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:57:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:57:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:57:12 [scrapy.extensions.telnet] INFO: Telnet Password: 9876dabdcd5d6e1f
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:57:12 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:57:12 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:57:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:57:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:57:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:57:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5861b98f6d6670452863a')}
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:47 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 14:57:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 14:57:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 14:57:47 [scrapy.extensions.telnet] INFO: Telnet Password: a5769d86ce870c23
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 14:57:47 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 14:57:47 [scrapy.core.engine] INFO: Spider opened
2019-10-27 14:57:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 14:57:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 14:57:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:57:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5863cb495880b03d182ed')}
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db5864cb495880b03d182ee')}
2019-10-27 14:58:08 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db5864db495880b03d182ef')}
2019-10-27 14:58:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db58650b495880b03d182f0')}
2019-10-27 14:58:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db58653b495880b03d182f1')}
2019-10-27 14:58:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58654b495880b03d182f2')}
2019-10-27 14:58:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db58656b495880b03d182f3')}
2019-10-27 14:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db58657b495880b03d182f4')}
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5865bb495880b03d182f5')}
2019-10-27 14:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5865bb495880b03d182f6')}
2019-10-27 14:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5865cb495880b03d182f7')}
2019-10-27 14:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db5865db495880b03d182f8')}
2019-10-27 14:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db5865eb495880b03d182f9')}
2019-10-27 14:58:27 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5865fb495880b03d182fa')}
2019-10-27 14:58:28 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 14:58:30 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58664b495880b03d182fb')}
2019-10-27 14:58:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 14:58:32 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db58666b495880b03d182fc')}
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db58668b495880b03d182fd')}
2019-10-27 14:58:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db5866bb495880b03d182fe')}
2019-10-27 14:58:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db5866db495880b03d182ff')}
2019-10-27 14:58:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5866eb495880b03d18300')}
2019-10-27 14:58:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5866fb495880b03d18301')}
2019-10-27 14:58:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:42 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db58671b495880b03d18302')}
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:44 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db58673b495880b03d18303')}
2019-10-27 14:58:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db58674b495880b03d18304')}
2019-10-27 14:58:46 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58675b495880b03d18305')}
2019-10-27 14:58:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58676b495880b03d18306')}
2019-10-27 14:58:49 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db58679b495880b03d18307')}
2019-10-27 14:58:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db58679b495880b03d18308')}
2019-10-27 14:58:52 [scrapy.extensions.logstats] INFO: Crawled 40 pages (at 40 pages/min), scraped 28 items (at 28 items/min)
2019-10-27 14:58:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db5867cb495880b03d18309')}
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db5867db495880b03d1830a')}
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 14:58:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db5867eb495880b03d1830b')}
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5867eb495880b03d1830c')}
2019-10-27 14:58:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db5867eb495880b03d1830d')}
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5867eb495880b03d1830e')}
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db5867fb495880b03d1830f')}
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5867fb495880b03d18310')}
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db5867fb495880b03d18311')}
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 14:58:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5867fb495880b03d18312')}
2019-10-27 14:58:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', '_id': ObjectId('5db5867fb495880b03d18313')}
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db58680b495880b03d18314')}
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db58680b495880b03d18315')}
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 14:58:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 14:58:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db58680b495880b03d18316')}
2019-10-27 14:58:57 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db58681b495880b03d18317')}
2019-10-27 14:58:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 14:58:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 14:58:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 14:58:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 14:58:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:01:09 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:01:09 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:01:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:01:09 [scrapy.extensions.telnet] INFO: Telnet Password: a8e4d1b9c93841f5
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:01:09 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:01:09 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:01:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:01:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:01:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db587066d4ea946f1314960')}
2019-10-27 15:01:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5870a6d4ea946f1314961')}
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db5870b6d4ea946f1314962')}
2019-10-27 15:01:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5870c6d4ea946f1314963')}
2019-10-27 15:01:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db5870c6d4ea946f1314964')}
2019-10-27 15:01:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db5870d6d4ea946f1314965')}
2019-10-27 15:01:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db5870d6d4ea946f1314966')}
2019-10-27 15:01:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:18 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5870e6d4ea946f1314967')}
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db5870e6d4ea946f1314968')}
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:01:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5870e6d4ea946f1314969')}
2019-10-27 15:01:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5870e6d4ea946f131496a')}
2019-10-27 15:01:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5870e6d4ea946f131496b')}
2019-10-27 15:01:39 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:01:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:01:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:01:39 [scrapy.extensions.telnet] INFO: Telnet Password: 30d9584b88ca5166
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:01:39 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:01:39 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:01:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:01:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db5872465033d79ea161202')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5872465033d79ea161203')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db5872465033d79ea161204')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db5872465033d79ea161205')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5872465033d79ea161206')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5872465033d79ea161207')}
2019-10-27 15:01:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db5872465033d79ea161208')}
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea161209')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea16120a')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea16120b')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea16120c')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea16120d')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea16120e')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea16120f')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea161210')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db5872565033d79ea161211')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db5872565033d79ea161212')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db5872565033d79ea161213')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea161214')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea161215')}
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5872565033d79ea161216')}
2019-10-27 15:01:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:01:41 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5872565033d79ea161217')}
2019-10-27 15:02:36 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:02:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:02:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:02:36 [scrapy.extensions.telnet] INFO: Telnet Password: 434922feabf4e261
2019-10-27 15:02:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:02:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:02:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:02:37 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:02:37 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:02:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:02:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a08')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a09')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a0a')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a0b')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a0c')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a0d')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db5875e2126e491169e6a0e')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a0f')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a10')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a11')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a12')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a13')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a14')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a15')}
2019-10-27 15:02:38 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a16')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db5875e2126e491169e6a17')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db5875e2126e491169e6a18')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db5875e2126e491169e6a19')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a1a')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a1b')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a1c')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a1d')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db5875e2126e491169e6a1e')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a1f')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a20')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a21')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a22')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a23')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a24')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a25')}
2019-10-27 15:02:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db5875e2126e491169e6a26')}
2019-10-27 15:02:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875f2126e491169e6a27')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db5875f2126e491169e6a28')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5875f2126e491169e6a29')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db5875f2126e491169e6a2a')}
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5875f2126e491169e6a2b')}
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db5875f2126e491169e6a2c')}
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5875f2126e491169e6a2d')}
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33753181?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 15:02:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:02:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5875f2126e491169e6a2e')}
2019-10-27 15:07:43 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:07:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:07:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:07:43 [scrapy.extensions.telnet] INFO: Telnet Password: 549f100310a7c8ad
2019-10-27 15:07:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:07:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:07:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:07:44 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:07:44 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:07:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:07:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588905ba1f5c468194cd7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588915ba1f5c468194cd8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1947330>, '_id': ObjectId('5db588915ba1f5c468194cd9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1947330>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588915ba1f5c468194cda')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588915ba1f5c468194cdb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588915ba1f5c468194cdc')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588915ba1f5c468194cdd')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588915ba1f5c468194cde')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, '_id': ObjectId('5db588915ba1f5c468194cdf')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/44458>, '_id': ObjectId('5db588915ba1f5c468194ce0')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44458>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/826770>, '_id': ObjectId('5db588915ba1f5c468194ce1')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/826770>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588915ba1f5c468194ce2')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2768>, '_id': ObjectId('5db588915ba1f5c468194ce3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2768>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2061860>, '_id': ObjectId('5db588915ba1f5c468194ce4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2061860>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/83746>, '_id': ObjectId('5db588915ba1f5c468194ce5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/83746>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588915ba1f5c468194ce6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/22863>, '_id': ObjectId('5db588915ba1f5c468194ce7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/22863>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/889>, '_id': ObjectId('5db588915ba1f5c468194ce8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/889>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/1528668>, '_id': ObjectId('5db588915ba1f5c468194ce9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1528668>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/813602>, '_id': ObjectId('5db588915ba1f5c468194cea')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/813602>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588915ba1f5c468194ceb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588915ba1f5c468194cec')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588915ba1f5c468194ced')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/616>, '_id': ObjectId('5db588915ba1f5c468194cee')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/616>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588925ba1f5c468194cef')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/63225>, '_id': ObjectId('5db588925ba1f5c468194cf0')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/63225>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588925ba1f5c468194cf1')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588925ba1f5c468194cf2')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588925ba1f5c468194cf3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2416804>, '_id': ObjectId('5db588925ba1f5c468194cf4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2416804>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3931730>, '_id': ObjectId('5db588925ba1f5c468194cf5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3931730>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/168421>, '_id': ObjectId('5db588925ba1f5c468194cf6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168421>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588925ba1f5c468194cf7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1245452>, '_id': ObjectId('5db588925ba1f5c468194cf8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1245452>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2625959>, '_id': ObjectId('5db588925ba1f5c468194cf9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2625959>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588925ba1f5c468194cfa')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588925ba1f5c468194cfb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588925ba1f5c468194cfc')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588925ba1f5c468194cfd')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/168307>, '_id': ObjectId('5db588925ba1f5c468194cfe')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/314182>, '_id': ObjectId('5db588925ba1f5c468194cff')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/314182>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588925ba1f5c468194d00')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2618793>, '_id': ObjectId('5db588925ba1f5c468194d01')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2618793>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588925ba1f5c468194d02')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588925ba1f5c468194d03')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:46 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588925ba1f5c468194d04')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588935ba1f5c468194d05')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/681575>, '_id': ObjectId('5db588935ba1f5c468194d06')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/681575>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588935ba1f5c468194d07')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588935ba1f5c468194d08')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588935ba1f5c468194d09')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analytics Intern /     ', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588935ba1f5c468194d0a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': [' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2232>, '_id': ObjectId('5db588935ba1f5c468194d0b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2232>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588935ba1f5c468194d0c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588935ba1f5c468194d0d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead (     )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588935ba1f5c468194d0e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588935ba1f5c468194d0f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588935ba1f5c468194d10')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588935ba1f5c468194d11')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588935ba1f5c468194d12')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d13')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', ' ', 'SQL', ' ', ' ', ' '], 'salary': ' 350\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588935ba1f5c468194d14')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d15')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '   ', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588935ba1f5c468194d16')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' BI (QlikView, Qlik Sense)', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', '', ' ', ' ', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', ' ', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', '   ', 'Cognos BI'], 'salary': ' 30\xa0000  110\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2160858>, '_id': ObjectId('5db588935ba1f5c468194d17')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2160858>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588935ba1f5c468194d18')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d19')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', '-', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588935ba1f5c468194d1a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Chief  Officer', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588935ba1f5c468194d1b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588935ba1f5c468194d1c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588945ba1f5c468194d1d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior C#. Net Developer', 'company_name': ' HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': ' 3\xa0000 USD  ', 'company_url': <GET https://spb.hh.ru/employer/2661749>, '_id': ObjectId('5db588945ba1f5c468194d1e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2661749>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588945ba1f5c468194d1f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Head Of Growth', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588945ba1f5c468194d20')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', ' ', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', ' ', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3425814>, '_id': ObjectId('5db588945ba1f5c468194d21')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3425814>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588945ba1f5c468194d22')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588945ba1f5c468194d23')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/130986>, '_id': ObjectId('5db588945ba1f5c468194d24')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/130986>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588945ba1f5c468194d25')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588945ba1f5c468194d26')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588945ba1f5c468194d27')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588945ba1f5c468194d28')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': ' 260\xa0000  370\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588945ba1f5c468194d29')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/638130>, '_id': ObjectId('5db588945ba1f5c468194d2a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/638130>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588945ba1f5c468194d2b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '   / Tech Lead', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', ' ', ' ', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588945ba1f5c468194d2c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588945ba1f5c468194d2d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ( )', 'company_name': '  (   )', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3156389>, '_id': ObjectId('5db588945ba1f5c468194d2e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3156389>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  /  Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588945ba1f5c468194d2f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6301>, '_id': ObjectId('5db588945ba1f5c468194d30')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6301>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  [id20580]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588945ba1f5c468194d31')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588945ba1f5c468194d32')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [' ', ' ', '  ', 'SQL', 'ORACLE'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588945ba1f5c468194d33')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 100\xa0000  120\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588955ba1f5c468194d34')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588955ba1f5c468194d35')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', ' ', 'MS Visio'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/213004>, '_id': ObjectId('5db588955ba1f5c468194d36')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/213004>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3984>, '_id': ObjectId('5db588955ba1f5c468194d37')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3984>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588955ba1f5c468194d38')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588955ba1f5c468194d39')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '-', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588955ba1f5c468194d3a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/86298>, '_id': ObjectId('5db588955ba1f5c468194d3b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/86298>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588955ba1f5c468194d3c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', ' ', 'C/C++'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6500>, '_id': ObjectId('5db588955ba1f5c468194d3d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6500>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (  DWH  BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['-', ' -', '-', ' ', ' ', 'MS Visio', ' ', ' ', 'Big Data', 'Data Science', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/139>, '_id': ObjectId('5db588955ba1f5c468194d3e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/139>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588955ba1f5c468194d3f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product IT Manager/ SCRUM Master   Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1186>, '_id': ObjectId('5db588955ba1f5c468194d40')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1186>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588955ba1f5c468194d41')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588955ba1f5c468194d42')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588955ba1f5c468194d43')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588955ba1f5c468194d44')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d45')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d46')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET developer /  ', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d47')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior SEO-', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', '.', 'Google Analytics', 'SEO ', '', '  ', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/633069>, '_id': ObjectId('5db588955ba1f5c468194d48')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/633069>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588955ba1f5c468194d49')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:49 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588955ba1f5c468194d4a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588955ba1f5c468194d4b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  -', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', ' ', 'Google Tag Manager', ' '], 'salary': ' 60\xa0000  120\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2559110>, '_id': ObjectId('5db588965ba1f5c468194d4c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2559110>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', ''], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/654435>, '_id': ObjectId('5db588965ba1f5c468194d4d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/654435>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': [' ', ' ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3455245>, '_id': ObjectId('5db588965ba1f5c468194d4e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3455245>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps engineer', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/44272>, '_id': ObjectId('5db588965ba1f5c468194d4f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44272>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588965ba1f5c468194d50')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588965ba1f5c468194d51')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Trials Assistant (CTA) /    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588965ba1f5c468194d52')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2617114>, '_id': ObjectId('5db588965ba1f5c468194d53')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2617114>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', ' ', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588965ba1f5c468194d54')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588965ba1f5c468194d55')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['', ' ', ' ', ' ', '1C: ', ' ', '  ', ' ', '   ', '  ', '  ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588965ba1f5c468194d56')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (      )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': ' 100\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588965ba1f5c468194d57')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', ' ', 'DWH', ' ', 'Business Development', 'ETL', 'AWS', ' ', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588965ba1f5c468194d58')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/854>, '_id': ObjectId('5db588965ba1f5c468194d59')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/854>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (         )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': ' 100\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588965ba1f5c468194d5a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588965ba1f5c468194d5b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR,  ', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', ' ', ' ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/217737>, '_id': ObjectId('5db588965ba1f5c468194d5c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/217737>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588965ba1f5c468194d5d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', ' ', '  ', 'Analytical skills', ' ', 'Time management', ' ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588965ba1f5c468194d5e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' -', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': [' ', '', ' ', '   ', ' '], 'salary': ' 40\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2869446>, '_id': ObjectId('5db588965ba1f5c468194d5f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2869446>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': ' 2\xa0500  3\xa0500 USD  ', 'company_url': <GET https://spb.hh.ru/employer/3289413>, '_id': ObjectId('5db588965ba1f5c468194d60')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3289413>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/62126>, '_id': ObjectId('5db588965ba1f5c468194d61')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/62126>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Finance Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['  ', 'MS Excel', ' ', 'SQL'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588965ba1f5c468194d62')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate / (Senior/)    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588965ba1f5c468194d63')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' Nissan,  ', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, '_id': ObjectId('5db588975ba1f5c468194d64')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d65')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Android Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588975ba1f5c468194d66')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588975ba1f5c468194d67')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588975ba1f5c468194d68')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java-', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/80>, '_id': ObjectId('5db588975ba1f5c468194d69')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/80>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588975ba1f5c468194d6a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588975ba1f5c468194d6b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Web ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', '  ', ''], 'salary': ' 90\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/97598>, '_id': ObjectId('5db588975ba1f5c468194d6c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/97598>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d6d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python developer /  Python', 'company_name': ' -     ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3358623>, '_id': ObjectId('5db588975ba1f5c468194d6e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3358623>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': ' 200\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d6f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scala Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': [' ', 'SCALA', 'Python', 'Spark', '-'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3491569>, '_id': ObjectId('5db588975ba1f5c468194d70')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3491569>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', ' ', 'MS SQL Server', 'Business English'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/14153>, '_id': ObjectId('5db588975ba1f5c468194d71')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/14153>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588975ba1f5c468194d72')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '- (  IoT, Ml)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1587>, '_id': ObjectId('5db588975ba1f5c468194d73')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1587>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588975ba1f5c468194d74')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': ' 1\xa0800 USD  ', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588975ba1f5c468194d75')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Market Researcher ( )', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [' ', ' ', ' ', '  ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588975ba1f5c468194d76')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' ', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': ' 650 USD  ', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588975ba1f5c468194d77')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior iOS Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588975ba1f5c468194d78')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588975ba1f5c468194d79')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588975ba1f5c468194d7a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588975ba1f5c468194d7b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:07:51 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588975ba1f5c468194d7c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:52 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': ' 100\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2097195>, '_id': ObjectId('5db588985ba1f5c468194d7d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2097195>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:07:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 15:07:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 81070,
 'downloader/request_count': 176,
 'downloader/request_method_count/GET': 176,
 'downloader/response_bytes': 6407646,
 'downloader/response_count': 176,
 'downloader/response_status_count/200': 176,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 7.996969,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 12, 7, 52, 42862),
 'log_count/DEBUG': 177,
 'log_count/ERROR': 167,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 176,
 'scheduler/dequeued': 176,
 'scheduler/dequeued/memory': 176,
 'scheduler/enqueued': 176,
 'scheduler/enqueued/memory': 176,
 'start_time': datetime.datetime(2019, 10, 27, 12, 7, 44, 45893)}
2019-10-27 15:07:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 15:09:09 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:09:09 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:09:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:09:09 [scrapy.extensions.telnet] INFO: Telnet Password: 2fdfaeefb442fbb4
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:09:09 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:09:09 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:09:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:09:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:09:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588e6e31518ac4c5634e3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588e6e31518ac4c5634e4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588e6e31518ac4c5634e5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588e6e31518ac4c5634e6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1947330>, '_id': ObjectId('5db588e6e31518ac4c5634e7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1947330>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e6e31518ac4c5634e8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588e6e31518ac4c5634e9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588e6e31518ac4c5634ea')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/44458>, '_id': ObjectId('5db588e6e31518ac4c5634eb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44458>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2061860>, '_id': ObjectId('5db588e6e31518ac4c5634ec')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2061860>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, '_id': ObjectId('5db588e6e31518ac4c5634ed')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2768>, '_id': ObjectId('5db588e6e31518ac4c5634ee')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2768>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/826770>, '_id': ObjectId('5db588e6e31518ac4c5634ef')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/826770>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/83746>, '_id': ObjectId('5db588e6e31518ac4c5634f0')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/83746>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588e6e31518ac4c5634f1')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/22863>, '_id': ObjectId('5db588e6e31518ac4c5634f2')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/22863>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588e6e31518ac4c5634f3')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:10 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588e6e31518ac4c5634f4')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/889>, '_id': ObjectId('5db588e6e31518ac4c5634f5')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/889>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/1528668>, '_id': ObjectId('5db588e7e31518ac4c5634f6')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1528668>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/813602>, '_id': ObjectId('5db588e7e31518ac4c5634f7')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/813602>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, '_id': ObjectId('5db588e7e31518ac4c5634f8')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/616>, '_id': ObjectId('5db588e7e31518ac4c5634f9')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/616>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588e7e31518ac4c5634fa')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1245452>, '_id': ObjectId('5db588e7e31518ac4c5634fb')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1245452>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588e7e31518ac4c5634fc')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2416804>, '_id': ObjectId('5db588e7e31518ac4c5634fd')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2416804>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588e7e31518ac4c5634fe')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/63225>, '_id': ObjectId('5db588e7e31518ac4c5634ff')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/63225>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588e7e31518ac4c563500')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3931730>, '_id': ObjectId('5db588e7e31518ac4c563501')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3931730>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/168421>, '_id': ObjectId('5db588e7e31518ac4c563502')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168421>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2625959>, '_id': ObjectId('5db588e7e31518ac4c563503')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2625959>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588e7e31518ac4c563504')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e7e31518ac4c563505')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588e7e31518ac4c563506')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588e7e31518ac4c563507')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e7e31518ac4c563508')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/168307>, '_id': ObjectId('5db588e7e31518ac4c563509')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/168307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588e7e31518ac4c56350a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/834307>, '_id': ObjectId('5db588e7e31518ac4c56350b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/834307>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/314182>, '_id': ObjectId('5db588e7e31518ac4c56350c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/314182>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588e8e31518ac4c56350d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/789662>, '_id': ObjectId('5db588e8e31518ac4c56350e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/789662>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588e8e31518ac4c56350f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588e8e31518ac4c563510')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/681575>, '_id': ObjectId('5db588e8e31518ac4c563511')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/681575>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/41862>, '_id': ObjectId('5db588e8e31518ac4c563512')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/41862>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588e8e31518ac4c563513')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588e8e31518ac4c563514')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588e8e31518ac4c563515')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588e8e31518ac4c563516')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2618793>, '_id': ObjectId('5db588e8e31518ac4c563517')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2618793>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588e8e31518ac4c563518')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588e8e31518ac4c563519')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Analytics Intern /     ', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3659721>, '_id': ObjectId('5db588e8e31518ac4c56351a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3659721>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e8e31518ac4c56351b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': [' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2232>, '_id': ObjectId('5db588e8e31518ac4c56351c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2232>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  Team Lead (     )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588e8e31518ac4c56351d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588e8e31518ac4c56351e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588e8e31518ac4c56351f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', ' ', 'SQL', ' ', ' ', ' '], 'salary': ' 350\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588e8e31518ac4c563520')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' BI (QlikView, Qlik Sense)', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', '', ' ', ' ', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', ' ', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', '   ', 'Cognos BI'], 'salary': ' 30\xa0000  110\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2160858>, '_id': ObjectId('5db588e8e31518ac4c563521')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2160858>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588e8e31518ac4c563522')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:12 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '   ', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588e8e31518ac4c563523')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563524')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563525')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563526')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c563527')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Chief  Officer', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588e9e31518ac4c563528')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', '-', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1040131>, '_id': ObjectId('5db588e9e31518ac4c563529')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1040131>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c56352a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/49556>, '_id': ObjectId('5db588e9e31518ac4c56352b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/49556>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Head Of Growth', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588e9e31518ac4c56352c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior C#. Net Developer', 'company_name': ' HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': ' 3\xa0000 USD  ', 'company_url': <GET https://spb.hh.ru/employer/2661749>, '_id': ObjectId('5db588e9e31518ac4c56352d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2661749>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588e9e31518ac4c56352e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/130986>, '_id': ObjectId('5db588e9e31518ac4c56352f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/130986>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', ' ', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', ' ', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3425814>, '_id': ObjectId('5db588e9e31518ac4c563530')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3425814>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588e9e31518ac4c563531')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3984>, '_id': ObjectId('5db588e9e31518ac4c563532')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3984>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588e9e31518ac4c563533')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/86298>, '_id': ObjectId('5db588e9e31518ac4c563534')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/86298>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588e9e31518ac4c563535')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588e9e31518ac4c563536')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/638130>, '_id': ObjectId('5db588e9e31518ac4c563537')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/638130>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': ' 260\xa0000  370\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588e9e31518ac4c563538')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:13 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588e9e31518ac4c563539')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ( )', 'company_name': '  (   )', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3156389>, '_id': ObjectId('5db588eae31518ac4c56353a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3156389>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588eae31518ac4c56353b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/574211>, '_id': ObjectId('5db588eae31518ac4c56353c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/574211>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '   / Tech Lead', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', ' ', ' ', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4585>, '_id': ObjectId('5db588eae31518ac4c56353d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4585>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  [id20580]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, '_id': ObjectId('5db588eae31518ac4c56353e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  /  Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6769>, '_id': ObjectId('5db588eae31518ac4c56353f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6769>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588eae31518ac4c563540')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 100\xa0000  120\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588eae31518ac4c563541')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6301>, '_id': ObjectId('5db588eae31518ac4c563542')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6301>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [' ', ' ', '  ', 'SQL', 'ORACLE'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, '_id': ObjectId('5db588eae31518ac4c563543')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2873>, '_id': ObjectId('5db588eae31518ac4c563544')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2873>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '-', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, '_id': ObjectId('5db588eae31518ac4c563545')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', ' ', 'MS Visio'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/213004>, '_id': ObjectId('5db588eae31518ac4c563546')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/213004>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', ' ', 'C/C++'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/6500>, '_id': ObjectId('5db588eae31518ac4c563547')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/6500>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (  DWH  BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['-', ' -', '-', ' ', ' ', 'MS Visio', ' ', ' ', 'Big Data', 'Data Science', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/139>, '_id': ObjectId('5db588eae31518ac4c563548')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/139>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588eae31518ac4c563549')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588eae31518ac4c56354a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588eae31518ac4c56354b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588eae31518ac4c56354c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588eae31518ac4c56354d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588eae31518ac4c56354e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:14 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/245657>, '_id': ObjectId('5db588eae31518ac4c56354f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/245657>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product IT Manager/ SCRUM Master   Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1186>, '_id': ObjectId('5db588ebe31518ac4c563550')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1186>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588ebe31518ac4c563551')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588ebe31518ac4c563552')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET developer /  ', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', 'company_url': <GET https://spb.hh.ru/employer/79595>, '_id': ObjectId('5db588ebe31518ac4c563553')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/79595>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ebe31518ac4c563554')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  -', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', ' ', 'Google Tag Manager', ' '], 'salary': ' 60\xa0000  120\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2559110>, '_id': ObjectId('5db588ebe31518ac4c563555')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2559110>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/25>, '_id': ObjectId('5db588ebe31518ac4c563556')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/25>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps engineer', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/44272>, '_id': ObjectId('5db588ebe31518ac4c563557')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/44272>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Finance Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['  ', 'MS Excel', ' ', 'SQL'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588ebe31518ac4c563558')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/914078>, '_id': ObjectId('5db588ebe31518ac4c563559')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/914078>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior SEO-', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', '.', 'Google Analytics', 'SEO ', '', '  ', '  '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/633069>, '_id': ObjectId('5db588ebe31518ac4c56355a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/633069>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3543358>, '_id': ObjectId('5db588ebe31518ac4c56355b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3543358>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', ''], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/654435>, '_id': ObjectId('5db588ebe31518ac4c56355c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/654435>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ebe31518ac4c56355d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Trials Assistant (CTA) /    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588ebe31518ac4c56355e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', ' ', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588ebe31518ac4c56355f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (      )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': ' 100\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588ebe31518ac4c563560')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': [' ', ' ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3455245>, '_id': ObjectId('5db588ebe31518ac4c563561')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3455245>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2617114>, '_id': ObjectId('5db588ebe31518ac4c563562')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2617114>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/162>, '_id': ObjectId('5db588ebe31518ac4c563563')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/162>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/854>, '_id': ObjectId('5db588ebe31518ac4c563564')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/854>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ebe31518ac4c563565')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:15 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', ' ', 'DWH', ' ', 'Business Development', 'ETL', 'AWS', ' ', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1574063>, '_id': ObjectId('5db588ebe31518ac4c563566')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1574063>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR,  ', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', ' ', ' ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/217737>, '_id': ObjectId('5db588ece31518ac4c563567')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/217737>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', ' ', '  ', 'Analytical skills', ' ', 'Time management', ' ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1263188>, '_id': ObjectId('5db588ece31518ac4c563568')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1263188>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '  (         )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': ' 100\xa0000 .   ', 'company_url': <GET https://spb.hh.ru/employer/1795976>, '_id': ObjectId('5db588ece31518ac4c563569')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1795976>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/802184>, '_id': ObjectId('5db588ece31518ac4c56356a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/802184>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['', ' ', ' ', ' ', '1C: ', ' ', '  ', ' ', '   ', '  ', '  ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588ece31518ac4c56356b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate / (Senior/)    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/11687>, '_id': ObjectId('5db588ece31518ac4c56356c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/11687>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': ' 2\xa0500  3\xa0500 USD  ', 'company_url': <GET https://spb.hh.ru/employer/3289413>, '_id': ObjectId('5db588ece31518ac4c56356d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3289413>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/62126>, '_id': ObjectId('5db588ece31518ac4c56356e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/62126>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' -', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': [' ', '', ' ', '   ', ' '], 'salary': ' 40\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2869446>, '_id': ObjectId('5db588ece31518ac4c56356f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2869446>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Android Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588ece31518ac4c563570')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588ece31518ac4c563571')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' Nissan,  ', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, '_id': ObjectId('5db588ece31518ac4c563572')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Java-', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/80>, '_id': ObjectId('5db588ece31518ac4c563573')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/80>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c563574')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588ece31518ac4c563575')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588ece31518ac4c563576')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/45743>, '_id': ObjectId('5db588ece31518ac4c563577')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/45743>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Python developer /  Python', 'company_name': ' -     ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3358623>, '_id': ObjectId('5db588ece31518ac4c563578')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3358623>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Web ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', '  ', ''], 'salary': ' 90\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/97598>, '_id': ObjectId('5db588ece31518ac4c563579')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/97598>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Scala Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': [' ', 'SCALA', 'Python', 'Spark', '-'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/3491569>, '_id': ObjectId('5db588ece31518ac4c56357a')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/3491569>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c56357b')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c56357c')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': ' 200\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/9281>, '_id': ObjectId('5db588ece31518ac4c56357d')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/9281>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', ' ', 'MS SQL Server', 'Business English'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/14153>, '_id': ObjectId('5db588ece31518ac4c56357e')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/14153>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': '- (  IoT, Ml)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1587>, '_id': ObjectId('5db588ece31518ac4c56357f')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1587>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:16 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/815269>, '_id': ObjectId('5db588ece31518ac4c563580')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/815269>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': ' 1\xa0800 USD  ', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588ede31518ac4c563581')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Market Researcher ( )', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [' ', ' ', ' ', '  ', ' ', ' '], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1876072>, '_id': ObjectId('5db588ede31518ac4c563582')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1876072>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': ' ', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': ' 650 USD  ', 'company_url': <GET https://spb.hh.ru/employer/2558659>, '_id': ObjectId('5db588ede31518ac4c563583')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2558659>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/1304>, '_id': ObjectId('5db588ede31518ac4c563584')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/1304>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior .NET Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/208902>, '_id': ObjectId('5db588ede31518ac4c563585')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/208902>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588ede31518ac4c563586')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Senior iOS Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/2610798>, '_id': ObjectId('5db588ede31518ac4c563587')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2610798>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', 'company_url': <GET https://spb.hh.ru/employer/52729>, '_id': ObjectId('5db588ede31518ac4c563588')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/52729>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.scraper] ERROR: Error processing {'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': ' 100\xa0000 .  ', 'company_url': <GET https://spb.hh.ru/employer/2097195>, '_id': ObjectId('5db588ede31518ac4c563589')}
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\data science\data mininig\lesson3\hh\pipelines.py", line 20, in process_item
    self.hh_collec.insert_one(item)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 698, in insert_one
    session=session),
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 612, in _insert
    bypass_doc_val, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 600, in _insert_one
    acknowledged, _insert_command, session)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1492, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\mongo_client.py", line 1385, in _retry_with_session
    return func(session, sock_info, retryable)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\collection.py", line 595, in _insert_command
    retryable_write=retryable_write)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 618, in command
    self._raise_connection_failure(error)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\pool.py", line 613, in command
    user_fields=user_fields)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\network.py", line 129, in command
    codec_options, ctx=compression_ctx)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\pymongo\message.py", line 704, in _op_msg
    flags, command, identifier, docs, check_keys, opts)
bson.errors.InvalidDocument: cannot encode object: <GET https://spb.hh.ru/employer/2097195>, of type: <class 'scrapy.http.request.Request'>
2019-10-27 15:09:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 15:09:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 81070,
 'downloader/request_count': 176,
 'downloader/request_method_count/GET': 176,
 'downloader/response_bytes': 6407539,
 'downloader/response_count': 176,
 'downloader/response_status_count/200': 176,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 7.966885,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 12, 9, 17, 291520),
 'log_count/DEBUG': 177,
 'log_count/ERROR': 167,
 'log_count/INFO': 10,
 'request_depth_max': 9,
 'response_received_count': 176,
 'scheduler/dequeued': 176,
 'scheduler/dequeued/memory': 176,
 'scheduler/enqueued': 176,
 'scheduler/enqueued/memory': 176,
 'start_time': datetime.datetime(2019, 10, 27, 12, 9, 9, 324635)}
2019-10-27 15:09:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-27 15:10:55 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:10:55 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:10:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:10:55 [scrapy.extensions.telnet] INFO: Telnet Password: 3914ae08bfaeee9d
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:10:55 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:10:55 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:10:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:10:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:10:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c381')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c382')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c383')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c384')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db589507f59ace01f76c385')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c386')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c387')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c388')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c389')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c38a')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c38b')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db589507f59ace01f76c38c')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c38d')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db589507f59ace01f76c38e')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c38f')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c390')}
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db589507f59ace01f76c391')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c392')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c393')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:10:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589507f59ace01f76c394')}
2019-10-27 15:10:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 15:10:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:10:56 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db589507f59ace01f76c395')}
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c396')}
2019-10-27 15:11:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c397')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c398')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c399')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c39a')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db589587f59ace01f76c39b')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c39c')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c39d')}
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c39e')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c39f')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c3a0')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c3a1')}
2019-10-27 15:11:04 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db589587f59ace01f76c3a2')}
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589597f59ace01f76c3a3')}
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db589597f59ace01f76c3a4')}
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db589597f59ace01f76c3a5')}
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db589597f59ace01f76c3a6')}
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 15:11:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:11:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589817f59ace01f76c3a7')}
2019-10-27 15:11:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db589817f59ace01f76c3a8')}
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', '_id': ObjectId('5db589817f59ace01f76c3a9')}
2019-10-27 15:11:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/22863>
{'company_url': 'http://www.protei.ru', '_id': ObjectId('5db589817f59ace01f76c3aa')}
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:11:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:11:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
2019-10-27 15:11:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db589877f59ace01f76c3ab')}
2019-10-27 15:12:15 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/826770>
{'company_url': 'http://www.weareworldquant.com', '_id': ObjectId('5db5899f7f59ace01f76c3ac')}
2019-10-27 15:12:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/813602>
{'company_url': 'http://www.kofax.com', '_id': ObjectId('5db589a07f59ace01f76c3ad')}
2019-10-27 15:12:16 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/889>
{'company_url': 'http://www.kodeks.ru', '_id': ObjectId('5db589a07f59ace01f76c3ae')}
2019-10-27 15:12:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db589a17f59ace01f76c3af')}
2019-10-27 15:12:17 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db589a17f59ace01f76c3b0')}
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.extensions.logstats] INFO: Crawled 74 pages (at 74 pages/min), scraped 48 items (at 48 items/min)
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589a27f59ace01f76c3b1')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', '_id': ObjectId('5db589a27f59ace01f76c3b2')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', '_id': ObjectId('5db589a27f59ace01f76c3b3')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589a27f59ace01f76c3b4')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', '_id': ObjectId('5db589a27f59ace01f76c3b5')}
2019-10-27 15:12:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/33810925?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db589a27f59ace01f76c3b6')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589a27f59ace01f76c3b7')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1040131>
{'company_url': 'http://segmento.ru', '_id': ObjectId('5db589a27f59ace01f76c3b8')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/834307>
{'company_url': 'http://www.dm-matrix.com', '_id': ObjectId('5db589a27f59ace01f76c3b9')}
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/616>
{'company_url': 'http://lancktele.com', '_id': ObjectId('5db589a27f59ace01f76c3ba')}
2019-10-27 15:12:18 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/63225>
{'company_url': 'https://vk.com', '_id': ObjectId('5db589a27f59ace01f76c3bb')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589a37f59ace01f76c3bc')}
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', '_id': ObjectId('5db589a37f59ace01f76c3bd')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db589a37f59ace01f76c3be')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2625959>
{'company_url': 'http://www.idrnd.ai', '_id': ObjectId('5db589a37f59ace01f76c3bf')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1263188>
{'company_url': 'http://www.weigandt-consulting.com', '_id': ObjectId('5db589a37f59ace01f76c3c0')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3931730>
{'company_url': 'http://GamesFactoryTalents.com', '_id': ObjectId('5db589a37f59ace01f76c3c1')}
2019-10-27 15:12:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2416804>
{'company_url': 'https://www.welltory.com', '_id': ObjectId('5db589a37f59ace01f76c3c2')}
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:12:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 15:12:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 46, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:12:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/168421>
{'company_url': 'http://www.wandisco.com', '_id': ObjectId('5db589a47f59ace01f76c3c3')}
2019-10-27 15:58:17 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: hh)
2019-10-27 15:58:17 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.18362-SP0
2019-10-27 15:58:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'hh', 'CONCURRENT_REQUESTS': 32, 'COOKIES_ENABLED': False, 'LOG_FILE': 'geek_log.log', 'NEWSPIDER_MODULE': 'hh.spiders', 'SPIDER_MODULES': ['hh.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'}
2019-10-27 15:58:17 [scrapy.extensions.telnet] INFO: Telnet Password: 4fc9a7d0917cecd9
2019-10-27 15:58:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-27 15:58:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-27 15:58:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-27 15:58:18 [scrapy.middleware] INFO: Enabled item pipelines:
['hh.pipelines.HhPipeline']
2019-10-27 15:58:18 [scrapy.core.engine] INFO: Spider opened
2019-10-27 15:58:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-27 15:58:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-27 15:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0> (referer: None)
2019-10-27 15:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34189146?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33487817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34261089?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33206558?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34193685?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34074931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34190195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34189146?query=data%20science>
{'vacancy_title': 'Principal Researcher /  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/813602', 'skills': ['Machine Learning', 'Natural Language Processing', ' ', 'Images Processing', 'Artificial Intelligence', 'Computer science'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e005')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33487817?query=data%20science>
{'vacancy_title': ' scientist ( )', 'company_name': ' ANCOR FinTech', 'company_url_hh': 'spb.hh.ru/employer/1947330', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e006')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34261089?query=data%20science>
{'vacancy_title': 'Senior  Scientist  NLP   ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Tensorflow', 'PyTorch', 'Pandas', 'NLP'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e007')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33206558?query=data%20science>
{'vacancy_title': 'Senior  Scientist ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e008')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28481921?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34193685?query=data%20science>
{'vacancy_title': ' Manager', 'company_name': ' PSI Co Ltd.', 'company_url_hh': 'spb.hh.ru/employer/83746', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e009')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34074931?query=data%20science>
{'vacancy_title': 'IoT and AI Security Researcher', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'Python', 'Android', 'Linux', 'C/C++', 'Cyber Security', 'OWASP', 'Malware Research'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e00a')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33753181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34249249?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33562986?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33797496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34190195?query=data%20science>
{'vacancy_title': ' scientist [20529]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e00b')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28481921?query=data%20science>
{'vacancy_title': ' Scientist (speech synthesis)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': [' ', 'Deep Learning', 'Data Science', 'Machine Learning', 'Python', 'C++', 'TensorFlow', 'Theano', ' ', 'synthesis', 'speech synthesis'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e00c')}
2019-10-27 15:58:19 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33753181?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/3529?dpt=3529-3529-prof', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e00d')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34249249?query=data%20science>
{'vacancy_title': '(Senior) Clinical Research Associate', 'company_name': ' PRA International', 'company_url_hh': 'spb.hh.ru/employer/44458', 'skills': ['Sub-/Co-Investigator', 'Knowledge of\xa0ICH-GCP', 'Business English', ' ', 'GCP (Good Clinical Practice)', 'Clinical Research'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e00e')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33562986?query=data%20science>
{'vacancy_title': ' Scientist (Amber Team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'Spark', 'hadoop', 'pandas', 'numpy'], 'salary': ' 150\xa0000 .   ', '_id': ObjectId('5db5946b280a695c8af9e00f')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33797496?query=data%20science>
{'vacancy_title': 'Medical  Liaison (Pulmonary hypertension)', 'company_name': ' Johnson & Johnson', 'company_url_hh': 'spb.hh.ru/employer/2768', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e010')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34113382?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32322427?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34028801?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194682?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115107?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810925?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31460069?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34113382?query=data%20science>
{'vacancy_title': ' /  Analyst', 'company_name': ' Wargaming, Saint Petersburg (Lesta Studio)', 'company_url_hh': 'spb.hh.ru/employer/662769?dpt=662769-662769-piter', 'skills': ['Python', 'Oracle Pl/SQL', 'SQL', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e011')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32322427?query=data%20science>
{'vacancy_title': 'Computer Vision Developer', 'company_name': ' , ', 'company_url_hh': 'spb.hh.ru/employer/22863', 'skills': ['Computer Vision', 'C++', 'Python', 'OpenCV', 'TensorFlow', 'Linux'], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db5946b280a695c8af9e012')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1947330> (referer: https://spb.hh.ru/vacancy/33487817?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34028801?query=data%20science>
{'vacancy_title': 'Middle  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e013')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/813602> (referer: https://spb.hh.ru/vacancy/34189146?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31381678?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=0)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194682?query=data%20science>
{'vacancy_title': ' Scientist - NLP Engineer', 'company_name': ' WorldQuant', 'company_url_hh': 'spb.hh.ru/employer/826770', 'skills': ['Python', 'Linux', 'SQL', 'Data Mining', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e014')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115107?query=data%20science>
{'vacancy_title': '  specialist ( )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1528668', 'skills': ['SQL', 'MS Excel', 'HTTP', 'REST', 'CPA', 'Internet Marketing', 'Data Analysis'], 'salary': ' 100\xa0000  150\xa0000 .  ', '_id': ObjectId('5db5946b280a695c8af9e015')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810925?query=data%20science>
{'vacancy_title': 'Senior  scientist', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': ['Big Data', 'Data Analysis', 'Python', 'Mathematical Statistics', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e016')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31460069?query=data%20science>
{'vacancy_title': '  (gamedev)', 'company_name': ' Panoramik', 'company_url_hh': 'spb.hh.ru/employer/2061860', 'skills': ['Python', ' ', 'MySQL', 'MS SQL', ' ', 'Machine Learning', 'Statistica', 'Data Mining: Statistics'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e017')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/83746> (referer: https://spb.hh.ru/vacancy/34193685?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1947330>
{'company_url': 'http://www.ancor.ru', '_id': ObjectId('5db5946b280a695c8af9e018')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/813602>
{'company_url': 'http://www.kofax.com', '_id': ObjectId('5db5946b280a695c8af9e019')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31381678?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/889', 'skills': [], 'salary': ' 130\xa0000 .   ', '_id': ObjectId('5db5946b280a695c8af9e01a')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34046892?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2873> (referer: https://spb.hh.ru/vacancy/34074931?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/41862> (referer: https://spb.hh.ru/vacancy/34261089?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/83746>
{'company_url': 'http://www.psi-cro.com', '_id': ObjectId('5db5946b280a695c8af9e01b')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148922?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3529?dpt=3529-3529-prof> (referer: https://spb.hh.ru/vacancy/33206558?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33035065?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33622749?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4585> (referer: https://spb.hh.ru/vacancy/28481921?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34046892?query=data%20science>
{'vacancy_title': 'Research engineer/ Scientist', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLP', 'ML', 'DL', 'CNN', 'RNN', 'NLU', 'C++', 'Java', 'Python', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e01c')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194943?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34061762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148922?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e01d')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34102073?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33035065?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Lanck Telecom', 'company_url_hh': 'spb.hh.ru/employer/616', 'skills': ['Python', ' ', ' ', ' ', ' '], 'salary': ' 150\xa0000  200\xa0000 .   ', '_id': ObjectId('5db5946b280a695c8af9e01e')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33622749?query=data%20science>
{'vacancy_title': 'Senior  Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Python', 'SQL', 'Data Mining', 'SCALA', 'ML', 'Data Science'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e01f')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD> (referer: https://spb.hh.ru/vacancy/34190195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194943?query=data%20science>
{'vacancy_title': ' Scientist / backend-  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/63225', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e020')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34061762?query=data%20science>
{'vacancy_title': 'Clinical  Manager', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e021')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34131592?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33913313?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34102073?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e022')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34262064?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34140496?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31701083?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34164081?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34131592?query=data%20science>
{'vacancy_title': '  Engineer', 'company_name': ' Welltory', 'company_url_hh': 'spb.hh.ru/employer/2416804', 'skills': ['Data Analysis', 'Python', ' ', 'PostgreSQL'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e023')}
2019-10-27 15:58:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33913313?query=data%20science>
{'vacancy_title': 'Junior Python Developer', 'company_name': ' Cloud Linux', 'company_url_hh': 'spb.hh.ru/employer/1245452', 'skills': ['Python', 'JavaScript', 'Linux', 'SQL', 'PHP'], 'salary': ' /  ', '_id': ObjectId('5db5946b280a695c8af9e024')}
2019-10-27 15:58:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32586338?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34262064?query=data%20science>
{'vacancy_title': 'Senior Java Developer for Big  product', 'company_name': ' WANdisco', 'company_url_hh': 'spb.hh.ru/employer/168421', 'skills': ['Java'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e025')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34140496?query=data%20science>
{'vacancy_title': ' Analyst to Finland', 'company_name': ' We Got Talent OY', 'company_url_hh': 'spb.hh.ru/employer/3931730', 'skills': ['SQL', 'Data Analysis', 'Python'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e026')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33984432?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31701083?query=data%20science>
{'vacancy_title': 'Middle/ Senior  Scientist (NLP)', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['NLP', 'natural language processing', 'machine learning', 'Python', 'neural networks', 'data science', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e027')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44458> (referer: https://spb.hh.ru/vacancy/34249249?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34164081?query=data%20science>
{'vacancy_title': ' Analyst', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Data Mining', 'SQL', 'Python', ' ', '', '   ', '   ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e028')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34237639?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32550481?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32586338?query=data%20science>
{'vacancy_title': 'Senior Escalation Engineer, ScaleIO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e029')}
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/789662> (referer: https://spb.hh.ru/vacancy/33562986?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282102?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2768> (referer: https://spb.hh.ru/vacancy/33797496?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/22863> (referer: https://spb.hh.ru/vacancy/32322427?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33984432?query=data%20science>
{'vacancy_title': 'ML Lead    (computer vision)', 'company_name': ' DSP Labs', 'company_url_hh': 'spb.hh.ru/employer/2625959', 'skills': ['DNN', 'Linux', 'Machine Learning', 'Computer Science', 'Agile Project Management', 'Computer Vision', 'Technical Lead', ' ', 'Data Analysis', 'Data Mining', 'Pattern Recognition', 'computer vision', 'face recognition'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e02a')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32830582?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/44458>
{'company_url': 'http://www.praintl.com', '_id': ObjectId('5db5946c280a695c8af9e02b')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34237639?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e02c')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/826770> (referer: https://spb.hh.ru/vacancy/34194682?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32550481?query=data%20science>
{'vacancy_title': 'Middle/Senior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'PostgreSQL', 'SQL', 'MongoDB', 'AWS', 'R'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e02d')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2061860> (referer: https://spb.hh.ru/vacancy/31460069?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282102?query=data%20science>
{'vacancy_title': ' engineer (Big  team)', 'company_name': ' DINO Systems', 'company_url_hh': 'spb.hh.ru/employer/168307', 'skills': ['Python', 'Java', 'MongoDB', 'Linux', 'Data Mining'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e02e')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/22863>
{'company_url': 'http://www.protei.ru', '_id': ObjectId('5db5946c280a695c8af9e02f')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32830582?query=data%20science>
{'vacancy_title': 'Team Lead/Senior  Scientist [id9672]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e030')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/889> (referer: https://spb.hh.ru/vacancy/31381678?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/662769?dpt=662769-662769-piter> (referer: https://spb.hh.ru/vacancy/34113382?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/245657> (referer: https://spb.hh.ru/vacancy/34148922?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/63225> (referer: https://spb.hh.ru/vacancy/34194943?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/826770>
{'company_url': 'http://www.weareworldquant.com', '_id': ObjectId('5db5946c280a695c8af9e031')}
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1528668> (referer: https://spb.hh.ru/vacancy/34115107?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/616> (referer: https://spb.hh.ru/vacancy/33035065?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2061860>
{'company_url': 'http://panoramikinc.com/', '_id': ObjectId('5db5946c280a695c8af9e032')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076305?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=1)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/834307> (referer: https://spb.hh.ru/vacancy/34061762?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/889>
{'company_url': 'http://www.kodeks.ru', '_id': ObjectId('5db5946c280a695c8af9e033')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/245657>
{'company_url': 'http://www.gctrials.com/', '_id': ObjectId('5db5946c280a695c8af9e034')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/63225>
{'company_url': 'https://vk.com', '_id': ObjectId('5db5946c280a695c8af9e035')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/616>
{'company_url': 'http://lancktele.com', '_id': ObjectId('5db5946c280a695c8af9e036')}
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1876072> (referer: https://spb.hh.ru/vacancy/33622749?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/34028801?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1040131> (referer: https://spb.hh.ru/vacancy/34102073?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33963085?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33794429?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076305?query=data%20science>
{'vacancy_title': '       ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 70\xa0000  90\xa0000 .   ', '_id': ObjectId('5db5946c280a695c8af9e037')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/834307>
{'company_url': 'http://www.dm-matrix.com', '_id': ObjectId('5db5946c280a695c8af9e038')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962746?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32672003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34127373?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/4934?dpt=bil-4934-beel> (referer: https://spb.hh.ru/vacancy/34028801?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33310304?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1245452> (referer: https://spb.hh.ru/vacancy/33913313?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33537422?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33350219?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1040131>
{'company_url': 'http://segmento.ru', '_id': ObjectId('5db5946c280a695c8af9e039')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33963085?query=data%20science>
{'vacancy_title': '  Team Lead ( )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Python', 'Data Mining', 'Java', 'Olap (online analytical processing)'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e03a')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33794429?query=data%20science>
{'vacancy_title': 'Senior Big  Engineer / ML Engineer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e03b')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2416804> (referer: https://spb.hh.ru/vacancy/34131592?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962746?query=data%20science>
{'vacancy_title': '  Team Lead (     )', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['Data Mining', 'Python', 'SQL', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e03c')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32672003?query=data%20science>
{'vacancy_title': 'Big  Engineer (Backend Developer)', 'company_name': ' Saber Interactive', 'company_url_hh': 'spb.hh.ru/employer/314182', 'skills': ['Python', 'MongoDB', 'Data Mining', 'SCALA', 'ETL', 'Big Data', 'OLTP', 'Java', 'C#', '.NET Framework'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e03d')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3931730> (referer: https://spb.hh.ru/vacancy/34140496?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34127373?query=data%20science>
{'vacancy_title': ' Engineer ( Lab team)', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': ['Python', 'SQL', 'hadoop', 'Spark', 'ansible', 'kafka'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e03e')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168421> (referer: https://spb.hh.ru/vacancy/34262064?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33310304?query=data%20science>
{'vacancy_title': 'Research Team Lead/ Project manager in PR  Scientist Team', 'company_name': ' SEMrush', 'company_url_hh': 'spb.hh.ru/employer/789662', 'skills': [' ', 'Data Analysis', 'Big Data'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e03f')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1245452>
{'company_url': 'http://www.cloudlinux.com', '_id': ObjectId('5db5946c280a695c8af9e040')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33537422?query=data%20science>
{'vacancy_title': 'Big  Engineer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2618793', 'skills': ['Big Data', 'Java', 'PostgreSQL', 'Spring Framework', 'Spark', 'Kafka', 'SCALA', 'Hadoop', 'AWS'], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e041')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33350219?query=data%20science>
{'vacancy_title': 'SAS Programmer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/834307', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e042')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2416804>
{'company_url': 'https://www.welltory.com', '_id': ObjectId('5db5946c280a695c8af9e043')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1263188> (referer: https://spb.hh.ru/vacancy/34164081?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3931730>
{'company_url': 'http://GamesFactoryTalents.com', '_id': ObjectId('5db5946c280a695c8af9e044')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/168421>
{'company_url': 'http://www.wandisco.com', '_id': ObjectId('5db5946c280a695c8af9e045')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2625959> (referer: https://spb.hh.ru/vacancy/33984432?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33583909?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33463435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32256533?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1263188>
{'company_url': 'http://www.weigandt-consulting.com', '_id': ObjectId('5db5946c280a695c8af9e046')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2625959>
{'company_url': 'http://www.idrnd.ai', '_id': ObjectId('5db5946c280a695c8af9e047')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
2019-10-27 15:58:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/49556> (referer: https://spb.hh.ru/vacancy/32586338?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33583909?query=data%20science>
{'vacancy_title': 'Junior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946c280a695c8af9e048')}
2019-10-27 15:58:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33463435?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Kotlin for  )', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db5946c280a695c8af9e049')}
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32113032?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34089554?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/815269> (referer: https://spb.hh.ru/vacancy/34237639?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32256533?query=data%20science>
{'vacancy_title': 'Lead  scientist', 'company_name': ' FBS Inc.', 'company_url_hh': 'spb.hh.ru/employer/681575', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e04a')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32463248?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31895772?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33962599?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3659721> (referer: https://spb.hh.ru/vacancy/32550481?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32113032?query=data%20science>
{'vacancy_title': 'Senior  Scientist  .', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/41862', 'skills': ['Python', 'Data Science', 'Machine Learning', 'Pandas', 'Numpy', 'Sklearn', 'Keras', 'PyTorch'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e04b')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34089554?query=data%20science>
{'vacancy_title': ' Engineer/\xa0BI Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e04c')}
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/168307> (referer: https://spb.hh.ru/vacancy/34282102?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912001?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33773888?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32463248?query=data%20science>
{'vacancy_title': 'Junior  Analyst', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Google Analytics', 'JavaScript', 'SQL', 'Big Query', 'Python', 'Jupyter Notebook', 'Pandas', 'Elasticsearch', 'Clickhouse'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e04d')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/314182> (referer: https://spb.hh.ru/vacancy/32672003?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31895772?query=data%20science>
{'vacancy_title': ' Analytics Intern /     ', 'company_name': ' Adhack.io', 'company_url_hh': 'spb.hh.ru/employer/3659721', 'skills': ['Python', 'R', 'Mathematical Statistics', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e04e')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31036822?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33962599?query=data%20science>
{'vacancy_title': 'Middle/Senior  scientist', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e04f')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=2)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912001?query=data%20science>
{'vacancy_title': ' Scientist', 'company_name': ' Actimind', 'company_url_hh': 'spb.hh.ru/employer/2232', 'skills': [' '], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e050')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33773888?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Data Mining', 'Python', 'Java', 'Presentation skills', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e051')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/314182>
{'company_url': 'http://www.saber3d.com', '_id': ObjectId('5db5946d280a695c8af9e052')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31036822?query=data%20science>
{'vacancy_title': ' Engineer (Business Analysis & Automation)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db5946d280a695c8af9e053')}
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2618793> (referer: https://spb.hh.ru/vacancy/33537422?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/802184> (referer: https://spb.hh.ru/vacancy/33794429?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286344?query=data%20science>
{'vacancy_title': 'IoT Big  Security Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e054')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33795953?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1795976> (referer: https://spb.hh.ru/vacancy/34076305?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34175818?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32036856?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33488319?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33879591?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31532753?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34187250?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33810802?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33795953?query=data%20science>
{'vacancy_title': 'Senior C#. Net Developer', 'company_name': ' HazelTree', 'company_url_hh': 'spb.hh.ru/employer/2661749', 'skills': ['C#', 'SQL', 'JavaScript', 'MVC', 'HTML', 'MS Visual Studio', 'Ajax', '.NET Framework', 'AngularJS', 'Angular4', 'Git', 'REST', 'ASP.NET'], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db5946d280a695c8af9e055')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34175818?query=data%20science>
{'vacancy_title': 'Java Developer, IEO, SSMO', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e056')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32036856?query=data%20science>
{'vacancy_title': 'Senior Python developer (Big )', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e057')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33488319?query=data%20science>
{'vacancy_title': 'Head Of Growth', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e058')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33879591?query=data%20science>
{'vacancy_title': 'Senior Scientist Process Simulation', 'company_name': ' Corning Optical Communications GmbH', 'company_url_hh': 'spb.hh.ru/employer/3425814', 'skills': ['Python', 'SPSS', 'Time management', ' ', 'C++', 'IBM SPSS', 'MATLAB', 'C/C++', ' ', 'Team management', 'Project management', 'SQL', 'Data Analysis'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e059')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31532753?query=data%20science>
{'vacancy_title': 'Lead  Scientist', 'company_name': ' Weatherford', 'company_url_hh': 'spb.hh.ru/employer/130986', 'skills': ['AI', 'Artificial intelligence', 'Machine learning', 'Big Data', 'Predictive analytics', 'Lenear algebra', 'Mathematical Statistics', 'Statistics', 'R', 'Phyton', 'C++', 'Java', 'JavaScript', 'SCALA', 'SCADA', 'Ridge', 'Lasso', 'MapReduce', 'GLM', 'Random Forests', 'Computer Science', 'linear/non-linear regression', 'elastic nets', 'Data Science', 'Data Analytics', 'Data Analysis', 'Data Mining', 'Electrical Engineering'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e05a')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33927421?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286423?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34187250?query=data%20science>
{'vacancy_title': 'Senior Python Automation Engineer, ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Git', 'Linux', 'QA', 'Robot Framework', 'Kubernetes', 'Docker', 'Jenkins', 'Go', 'CI/CD'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e05b')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33810802?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/4934?dpt=bil-4934-beel', 'skills': [' ', ' ', '  ', 'SQL', 'ORACLE'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e05c')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30411796?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34282390?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34100092?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34020096?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34078819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019637?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33927421?query=data%20science>
{'vacancy_title': 'Big  Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e05d')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286423?query=data%20science>
{'vacancy_title': 'IoT Big  Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e05e')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30411796?query=data%20science>
{'vacancy_title': '   ', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e05f')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34282390?query=data%20science>
{'vacancy_title': ' BI (QlikView, Qlik Sense)', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2160858', 'skills': ['SQL', 'HTML', 'JavaScript', '', ' ', ' ', 'C#', 'C++', 'PHP', 'CSS', 'Qlik', 'QlikView', 'jQuery', ' ', 'MS SQL Server', 'Big Data', 'Qlik Sense', 'Python', '   ', 'Cognos BI'], 'salary': ' 30\xa0000  110\xa0000 .  ', '_id': ObjectId('5db5946d280a695c8af9e060')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34100092?query=data%20science>
{'vacancy_title': 'Search Engine Senior Engineer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Python', 'Java', 'Linux', 'SQL', 'C++'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e061')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/45743> (referer: https://spb.hh.ru/vacancy/33583909?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34020096?query=data%20science>
{'vacancy_title': 'Lead QA Automation Engineer (XtremIO)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Python', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e062')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34078819?query=data%20science>
{'vacancy_title': '  ', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['Big Data', 'Data Mining', ' ', 'SQL', ' ', ' ', ' '], 'salary': ' 350\xa0000 .  ', '_id': ObjectId('5db5946d280a695c8af9e063')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019637?query=data%20science>
{'vacancy_title': 'Senior Java Developer (Control Path team), Midrange', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Java 8', 'Java', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e064')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34222931?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/9281> (referer: https://spb.hh.ru/vacancy/33463435?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34019609?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33986966?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31123318?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/45743>
{'company_url': 'http://www.oct-clinicaltrials.com/', '_id': ObjectId('5db5946d280a695c8af9e065')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34222931?query=data%20science>
{'vacancy_title': 'Senior Product Manager', 'company_name': ' Segmento', 'company_url_hh': 'spb.hh.ru/employer/1040131', 'skills': ['Agile Project Management', 'Scrum', 'Product Management', '-', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e066')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=3)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/9281>
{'company_url': 'http://www.jetbrains.com', '_id': ObjectId('5db5946d280a695c8af9e067')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34019609?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (Performance Investigation team), ECS', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': ['Linux', 'Performance analysis tools'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e068')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6769> (referer: https://spb.hh.ru/vacancy/33773888?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2610798> (referer: https://spb.hh.ru/vacancy/33488319?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33986966?query=data%20science>
{'vacancy_title': ' Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ETL', 'MS SQL', 'Machine Learning', 'Data Science', 'CRM'], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e069')}
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/681575> (referer: https://spb.hh.ru/vacancy/32256533?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31123318?query=data%20science>
{'vacancy_title': 'Chief  Officer', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946d280a695c8af9e06a')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3425814> (referer: https://spb.hh.ru/vacancy/33879591?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3543358> (referer: https://spb.hh.ru/vacancy/34089554?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/130986> (referer: https://spb.hh.ru/vacancy/31532753?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2160858> (referer: https://spb.hh.ru/vacancy/34282390?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/6769>
{'company_url': 'http://www.epam-group.ru', '_id': ObjectId('5db5946d280a695c8af9e06b')}
2019-10-27 15:58:21 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2610798>
{'company_url': 'http://www.melscience.com', '_id': ObjectId('5db5946d280a695c8af9e06c')}
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/208902> (referer: https://spb.hh.ru/vacancy/33986966?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2232> (referer: https://spb.hh.ru/vacancy/33912001?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/25> (referer: https://spb.hh.ru/vacancy/33286344?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/2661749> (referer: https://spb.hh.ru/vacancy/33795953?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3425814>
{'company_url': 'http://www.corning.com', '_id': ObjectId('5db5946e280a695c8af9e06d')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/130986>
{'company_url': 'http://www.weatherford.ru', '_id': ObjectId('5db5946e280a695c8af9e06e')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2160858>
{'company_url': 'http://qleversolutions.ru/', '_id': ObjectId('5db5946e280a695c8af9e06f')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/208902>
{'company_url': 'https://softwarecountry.ru', '_id': ObjectId('5db5946e280a695c8af9e070')}
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/15478?dpt=mailru-15478-od> (referer: https://spb.hh.ru/vacancy/30411796?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263207?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32213205?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33508195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34265514?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34047143?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33024347?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263207?query=data%20science>
{'vacancy_title': 'Technical Product Manager', 'company_name': ' Akvelon', 'company_url_hh': 'spb.hh.ru/employer/213004', 'skills': ['Product Management', 'MS PowerPoint', 'Project management', ' ', 'MS Visio'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e071')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32213205?query=data%20science>
{'vacancy_title': '-', 'company_name': ' Mail.Ru Group,  , ', 'company_url_hh': 'spb.hh.ru/employer/15478?dpt=mailru-15478-od', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e072')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33508195?query=data%20science>
{'vacancy_title': 'Software Engineer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3984', 'skills': ['Erlang', 'C/C++', 'Python', 'Java', 'Unix', 'Linux', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e073')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34076290?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34265514?query=data%20science>
{'vacancy_title': 'Middle/Senior Front-end Developer', 'company_name': ' Murano Software', 'company_url_hh': 'spb.hh.ru/employer/6301', 'skills': ['JavaScript', 'TypeScript', 'React', 'Webpack', 'Cypress', 'Jest', 'CSS', 'HTML'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e074')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33635762?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34047143?query=data%20science>
{'vacancy_title': 'ML Engineer (NLU)', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['NLU', 'ML', 'DL', 'CNN', 'RNN', 'NLP', 'speech processing'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e075')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34217579?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33024347?query=data%20science>
{'vacancy_title': 'Senior Java Developer (IEO, DRI)', 'company_name': ' Dell EMC', 'company_url_hh': 'spb.hh.ru/employer/49556', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e076')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32247492?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32603556?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34076290?query=data%20science>
{'vacancy_title': '  ', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': [], 'salary': ' 100\xa0000  120\xa0000 .   ', '_id': ObjectId('5db5946e280a695c8af9e077')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33635762?query=data%20science>
{'vacancy_title': 'Senior Manual QA Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e078')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33698754?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34217579?query=data%20science>
{'vacancy_title': 'QA Engineer (Back Office Voice team)', 'company_name': ' Intermedia', 'company_url_hh': 'spb.hh.ru/employer/86298', 'skills': ['Java', 'Atlassian Jira', 'SQL', 'Linux', 'Unix'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e079')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32247492?query=data%20science>
{'vacancy_title': 'GCP Lead Engineer', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Google Cloud Platform'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e07a')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32603556?query=data%20science>
{'vacancy_title': 'Senior JavaScript Engineer', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['TypeScript', 'ES6', 'Node.js', 'JavaScript', 'ReactJS'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e07b')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698477?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32602243?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/28698410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34080695?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34225410?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33698754?query=data%20science>
{'vacancy_title': 'ETL Developer', 'company_name': ' First Line Software', 'company_url_hh': 'spb.hh.ru/employer/638130', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e07c')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32908577?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225819?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala, AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['PostgreSQL', 'MongoDB', 'Java', 'Leadership Skills', 'SCALA', 'ActiveMQ', 'RabbitMQ', 'AWS\xa0SQS', 'Kinesis', 'Spark', 'Cassandra', 'Couchbase', 'Redis', 'Elasticsearch', 'Big Data'], 'salary': ' 260\xa0000  370\xa0000 .   ', '_id': ObjectId('5db5946e280a695c8af9e07d')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/213004> (referer: https://spb.hh.ru/vacancy/34263207?query=data%20science)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34263551?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=4)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698477?query=data%20science>
{'vacancy_title': 'Senior Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e07e')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32602243?query=data%20science>
{'vacancy_title': '  ( )', 'company_name': '  (   )', 'company_url_hh': 'spb.hh.ru/employer/3156389', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e07f')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/28698410?query=data%20science>
{'vacancy_title': 'Software Engineer (C#/.NET)', 'company_name': ' Netwrix Corporation', 'company_url_hh': 'spb.hh.ru/employer/574211', 'skills': ['C#', 'Visual Studio C#', '.NET Framework', 'MS Visual Studio', 'Software Development'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e080')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34080695?query=data%20science>
{'vacancy_title': '   / Tech Lead', 'company_name': '  |  ', 'company_url_hh': 'spb.hh.ru/employer/4585', 'skills': ['Linux', ' ', ' ', 'continuous integration', 'VMware', 'Windows Server', 'CentOS', 'CLion', 'TeamCity', 'upsource'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e081')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34225410?query=data%20science>
{'vacancy_title': '  [id20580]', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/39305?dpt=gpn-39305-HOLD', 'skills': ['SCALA', 'PostgreSQL', 'Java', 'SQL', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e082')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32908577?query=data%20science>
{'vacancy_title': '  /  Big ', 'company_name': ' EPAM Systems, Inc.', 'company_url_hh': 'spb.hh.ru/employer/6769', 'skills': ['Linux', 'Ansible', 'Hadoop', 'Kafka', 'ElasticSearch', 'Jenkins'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e083')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34194228?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34201196?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/213004>
{'company_url': 'http://akvelon-ivanovo.ru/', '_id': ObjectId('5db5946e280a695c8af9e084')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34082344?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126486?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34263551?query=data%20science>
{'vacancy_title': 'Compiler Back-end Developer', 'company_name': ' HUAWEI', 'company_url_hh': 'spb.hh.ru/employer/2873', 'skills': ['Java', 'LLVM', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e085')}
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/6301> (referer: https://spb.hh.ru/vacancy/34265514?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34194228?query=data%20science>
{'vacancy_title': '  -', 'company_name': ' InsightWhale', 'company_url_hh': 'spb.hh.ru/employer/2559110', 'skills': ['Google Analytics', ' ', 'Google Tag Manager', ' '], 'salary': ' 60\xa0000  120\xa0000 .  ', '_id': ObjectId('5db5946e280a695c8af9e086')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34201196?query=data%20science>
{'vacancy_title': 'Middle Java Developer', 'company_name': ' Spice Agency', 'company_url_hh': 'spb.hh.ru/employer/654435', 'skills': ['Spring Framework', 'Java', 'Unix', 'spring boot', 'docker', ''], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e087')}
2019-10-27 15:58:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3984> (referer: https://spb.hh.ru/vacancy/33508195?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34115215?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34082344?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e088')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126486?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e089')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33286484?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34148951?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33644435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34115215?query=data%20science>
{'vacancy_title': 'Senior Backend Developer (Golang/Python)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3543358', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e08a')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33790547?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32526166?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32439299?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33930847?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33286484?query=data%20science>
{'vacancy_title': 'IoT Cloud Solution Engineer', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e08b')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34148951?query=data%20science>
{'vacancy_title': 'Clinical Research Associate (CRA, Saint-Petersburg, Russia)', 'company_name': ' Global Clinical Trials', 'company_url_hh': 'spb.hh.ru/employer/245657', 'skills': ['  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e08c')}
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33644435?query=data%20science>
{'vacancy_title': '  (  DWH  BI)', 'company_name': ' IBS', 'company_url_hh': 'spb.hh.ru/employer/139', 'skills': ['-', ' -', '-', ' ', ' ', 'MS Visio', ' ', ' ', 'Big Data', 'Data Science', '  '], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e08d')}
2019-10-27 15:58:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33472664?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33790547?query=data%20science>
{'vacancy_title': 'Senior R&D Engineer', 'company_name': ' Synopsys', 'company_url_hh': 'spb.hh.ru/employer/6500', 'skills': ['Qt', 'C++', 'Linux', ' ', 'C/C++'], 'salary': ' /  ', '_id': ObjectId('5db5946e280a695c8af9e08e')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30270817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32526166?query=data%20science>
{'vacancy_title': 'DevOps engineer', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/44272', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e08f')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/30653181?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33199359?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32439299?query=data%20science>
{'vacancy_title': 'Product IT Manager/ SCRUM Master   Driven Marketing', 'company_name': ' HEINEKEN Russia', 'company_url_hh': 'spb.hh.ru/employer/1186', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e090')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33930847?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (GSC Finance area)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': ['SAP BW', 'VBA', 'Project management', 'Leadership Skills'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e091')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33147339?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/914078> (referer: https://spb.hh.ru/vacancy/33635762?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34191817?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33472664?query=data%20science>
{'vacancy_title': 'BI Analyst', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Business Intelligence Systems', 'SQL', 'BI tools', 'Data Analysis', 'Business intelligence', 'BI analyst', 'Reporting Analyst'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e092')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30270817?query=data%20science>
{'vacancy_title': 'Full-stack Senior Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db5946f280a695c8af9e093')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/26983093?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/30653181?query=data%20science>
{'vacancy_title': 'Frontend Developer', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e094')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33199359?query=data%20science>
{'vacancy_title': 'AWS Cloud Engineer (Java/Scala)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['aws', 'SQL', 'RDS', 'PostgreSQL', 'ORACLE', 'DB2', 'AWS Kinesis', 'Java', 'Spark', 'Kafka', 'Cassandra', 'MongoDB', 'Elasticsearch', 'Big Data', 'Hadoop', 'Hive', 'ActiveMQ', 'RabbitMQ', 'AWS SQS', 'AWS Glue', 'Airflow', 'NiFi', 'Lambda', 'Redshift', 'Tableau', 'Snowflake', 'Pentaho', 'DWH', 'data pipelines', 'SCALA'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e095')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32699819?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=5)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33147339?query=data%20science>
{'vacancy_title': 'Team lead', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e096')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/914078>
{'company_url': 'https://ix.co/', '_id': ObjectId('5db5946f280a695c8af9e097')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34191817?query=data%20science>
{'vacancy_title': 'System Architect', 'company_name': ' iX.co', 'company_url_hh': 'spb.hh.ru/employer/914078', 'skills': ['Node.js', 'JavaScript', 'AngularJS', 'Google Analytics', 'UML'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e098')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3156389> (referer: https://spb.hh.ru/vacancy/32602243?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/638130> (referer: https://spb.hh.ru/vacancy/33698754?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/574211> (referer: https://spb.hh.ru/vacancy/28698477?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1574063> (referer: https://spb.hh.ru/vacancy/34225819?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2559110> (referer: https://spb.hh.ru/vacancy/34194228?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/26983093?query=data%20science>
{'vacancy_title': 'Senior .NET developer /  ', 'company_name': ' Broadridge', 'company_url_hh': 'spb.hh.ru/employer/79595', 'skills': [], 'salary': ' 3\xa0000 USD  ', '_id': ObjectId('5db5946f280a695c8af9e099')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/654435> (referer: https://spb.hh.ru/vacancy/34201196?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32699819?query=data%20science>
{'vacancy_title': 'Senior SEO-', 'company_name': ' Selectel', 'company_url_hh': 'spb.hh.ru/employer/633069', 'skills': ['SEO', '.', 'Google Analytics', 'SEO ', '', '  ', '  '], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e09a')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/139> (referer: https://spb.hh.ru/vacancy/33644435?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/86298> (referer: https://spb.hh.ru/vacancy/34217579?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3156389>
{'company_url': 'http://www.crpt.ru', '_id': ObjectId('5db5946f280a695c8af9e09b')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/638130>
{'company_url': 'http://www.firstlinesoftware.ru', '_id': ObjectId('5db5946f280a695c8af9e09c')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34075003?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/574211>
{'company_url': 'http://www.netwrix.com', '_id': ObjectId('5db5946f280a695c8af9e09d')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1574063>
{'company_url': 'http://www.clearscale.com', '_id': ObjectId('5db5946f280a695c8af9e09e')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2559110>
{'company_url': 'https://insightwhale.com', '_id': ObjectId('5db5946f280a695c8af9e09f')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/654435>
{'company_url': 'http://spiceit.ru', '_id': ObjectId('5db5946f280a695c8af9e0a0')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/139>
{'company_url': 'http://www.ibs.ru', '_id': ObjectId('5db5946f280a695c8af9e0a1')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34128256?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34124580?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33409224?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33659438?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33627189?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34032012?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34075003?query=data%20science>
{'vacancy_title': ' -', 'company_name': '   ', 'company_url_hh': 'spb.hh.ru/employer/2869446', 'skills': [' ', '', ' ', '   ', ' '], 'salary': ' 40\xa0000 .  ', '_id': ObjectId('5db5946f280a695c8af9e0a2')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34128256?query=data%20science>
{'vacancy_title': 'DevOps Engineer for an AI Swiss Startup', 'company_name': ' Assaia International AG', 'company_url_hh': 'spb.hh.ru/employer/3289413', 'skills': ['DevOps', 'Linux', 'Docker', 'CI/CD', 'Ansible', 'Kubernetes', 'Git', 'SRE'], 'salary': ' 2\xa0500  3\xa0500 USD  ', '_id': ObjectId('5db5946f280a695c8af9e0a3')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34124580?query=data%20science>
{'vacancy_title': 'Quantitative trader', 'company_name': ' Alber Blanc', 'company_url_hh': 'spb.hh.ru/employer/3455245', 'skills': [' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0a4')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33409224?query=data%20science>
{'vacancy_title': 'Delivery Manager (DIH/IoT)', 'company_name': ' T-Systems RUS', 'company_url_hh': 'spb.hh.ru/employer/25', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0a5')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33659438?query=data%20science>
{'vacancy_title': 'System Developer (C/C++)', 'company_name': ' Sperasoft', 'company_url_hh': 'spb.hh.ru/employer/62126', 'skills': ['C/C++', 'Linux'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0a6')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33627189?query=data%20science>
{'vacancy_title': 'Finance Analyst', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': ['  ', 'MS Excel', ' ', 'SQL'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0a7')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34032012?query=data%20science>
{'vacancy_title': 'BI Developer / Analyst', 'company_name': ' IT Retail', 'company_url_hh': 'spb.hh.ru/employer/2617114', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0a8')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34111148?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/29024867?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33609223?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34103998?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33630076?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34064602?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34111148?query=data%20science>
{'vacancy_title': 'Senior PHP Developer (Drupal)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Symfony', 'Design Patterns', 'PHP', 'Yii', 'Web Application Development', 'Drupal API', ' ', 'Organic Groups', 'Panelizer', 'Features', 'Views API', 'Form API', 'Entity API'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0a9')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/6500> (referer: https://spb.hh.ru/vacancy/33790547?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34088195?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/29024867?query=data%20science>
{'vacancy_title': 'Information Security Specialist', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0aa')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34166457?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33609223?query=data%20science>
{'vacancy_title': '  (      )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Finance', 'Simulation', 'Forecasting', 'Algorithms Design', 'Data Mining'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db5946f280a695c8af9e0ab')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34103998?query=data%20science>
{'vacancy_title': 'Consultant SAP Solution Manager 7.2 (Premium Engagement Customers)', 'company_name': ' SAP', 'company_url_hh': 'spb.hh.ru/employer/854', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0ac')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33630076?query=data%20science>
{'vacancy_title': 'Applications Solution Architect (AWS)', 'company_name': ' ClearScale', 'company_url_hh': 'spb.hh.ru/employer/1574063', 'skills': ['Analytical skills', 'data warehouse', 'Product Management', 'Architecture', 'Business English', 'architecture design', 'Leadership Skills', 'IoT', 'Business Management', 'Technical documentation', ' ', 'DWH', ' ', 'Business Development', 'ETL', 'AWS', ' ', 'Presales', 'PreSales', 'Post-Sales', 'Development'], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0ad')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34064602?query=data%20science>
{'vacancy_title': 'Clinical Trials Assistant (CTA) /    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0ae')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32987063?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/6500>
{'company_url': 'http://www.synopsys.com', '_id': ObjectId('5db5946f280a695c8af9e0af')}
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34088195?query=data%20science>
{'vacancy_title': ' / Accountant', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': ['', ' ', ' ', ' ', '1C: ', ' ', '  ', ' ', '   ', '  ', '  ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0b0')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/79595> (referer: https://spb.hh.ru/vacancy/30270817?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34166457?query=data%20science>
{'vacancy_title': 'IT Process Analyst Manager (Track & Trace)', 'company_name': ' JTI', 'company_url_hh': 'spb.hh.ru/employer/162', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0b1')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33404502?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/162> (referer: https://spb.hh.ru/vacancy/33930847?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32987063?query=data%20science>
{'vacancy_title': 'Product Manager (relocation to Prague)', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0b2')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1186> (referer: https://spb.hh.ru/vacancy/32439299?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34126519?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/79595>
{'company_url': 'http://www.broadridge.com', '_id': ObjectId('5db5946f280a695c8af9e0b3')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33608720?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:23 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33404502?query=data%20science>
{'vacancy_title': 'Senior CV Engineer', 'company_name': ' DSR,  ', 'company_url_hh': 'spb.hh.ru/employer/217737', 'skills': ['machine learning', 'computer vision', 'data science', 'Artificial intelligence', ' ', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db5946f280a695c8af9e0b4')}
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2869446> (referer: https://spb.hh.ru/vacancy/34075003?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3455245> (referer: https://spb.hh.ru/vacancy/34124580?query=data%20science)
2019-10-27 15:58:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32725935?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=6)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/633069> (referer: https://spb.hh.ru/vacancy/32699819?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34126519?query=data%20science>
{'vacancy_title': 'Talent Sourcing Manager', 'company_name': ' Weigandt-consulting', 'company_url_hh': 'spb.hh.ru/employer/1263188', 'skills': ['Recruitment', ' ', '  ', 'Analytical skills', ' ', 'Time management', ' ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0b5')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2617114> (referer: https://spb.hh.ru/vacancy/34032012?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33608720?query=data%20science>
{'vacancy_title': '  (         )', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/1795976', 'skills': ['Python', 'C++', 'Machine Learning', 'Data Analysis', 'Data Mining', 'Hadoop'], 'salary': ' 100\xa0000 .   ', '_id': ObjectId('5db59470280a695c8af9e0b6')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/44272> (referer: https://spb.hh.ru/vacancy/32526166?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2869446>
{'company_url': 'http://skillfactory.ru', '_id': ObjectId('5db59470280a695c8af9e0b7')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3455245>
{'company_url': 'https://www.alberblanc.com/', '_id': ObjectId('5db59470280a695c8af9e0b8')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32725935?query=data%20science>
{'vacancy_title': 'Java Engineer (Poland)', 'company_name': ' Grid Dynamics', 'company_url_hh': 'spb.hh.ru/employer/802184', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0b9')}
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3289413> (referer: https://spb.hh.ru/vacancy/34128256?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31189716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2617114>
{'company_url': 'https://itretail.com/', '_id': ObjectId('5db59470280a695c8af9e0ba')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/21963813?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33497437?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34031778?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/62126> (referer: https://spb.hh.ru/vacancy/33659438?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33964564?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33998747?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/34024188?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31189716?query=data%20science>
{'vacancy_title': 'Senior Java/Kotlin Developer (Cloud integration)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': ['Apache Spark', 'Apache Hadoop', 'Amazon', 'Kotlin'], 'salary': ' 200\xa0000 .  ', '_id': ObjectId('5db59470280a695c8af9e0bb')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/21963813?query=data%20science>
{'vacancy_title': 'Product Manager', 'company_name': ' Wrike', 'company_url_hh': 'spb.hh.ru/employer/815269', 'skills': ['Product Management', 'Product Strategy', 'Agile Project Management', 'Scrum', 'Launching new products', ' '], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0bc')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33497437?query=data%20science>
{'vacancy_title': 'Senior System Architect', 'company_name': ' Emergn (ex. Return on Intelligence)', 'company_url_hh': 'spb.hh.ru/employer/14153', 'skills': ['Architecture', '.NET Framework', 'ASP.NET', '.NET Core', 'ORACLE', 'MS SQL', ' ', 'MS SQL Server', 'Business English'], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0bd')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34031778?query=data%20science>
{'vacancy_title': '- (  IoT, Ml)', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/1587', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0be')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33850385?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33964564?query=data%20science>
{'vacancy_title': 'Market Researcher ( )', 'company_name': ' IQ tin Sftwre', 'company_url_hh': 'spb.hh.ru/employer/1876072', 'skills': [' ', ' ', ' ', '  ', ' ', ' '], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0bf')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33998747?query=data%20science>
{'vacancy_title': 'Senior ML developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': ['Python'], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c0')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/34024188?query=data%20science>
{'vacancy_title': 'Clinical Research Associate / (Senior/)    ', 'company_name': ' IQVIA', 'company_url_hh': 'spb.hh.ru/employer/11687', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c1')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135827?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33465835?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33369948?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33847716?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33354237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33850385?query=data%20science>
{'vacancy_title': 'Design Engineer (R&D)', 'company_name': ' Nissan,  ', 'company_url_hh': 'spb.hh.ru/employer/3832?dpt=3832-3832-const', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c2')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33864444?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135827?query=data%20science>
{'vacancy_title': 'Senior Android Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': ['Android', 'iOS', 'Kotlin', 'Java'], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c3')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33465835?query=data%20science>
{'vacancy_title': 'Marketing research manager (Kotlin)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c4')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33613135?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33369948?query=data%20science>
{'vacancy_title': 'Java-', 'company_name': ' -', 'company_url_hh': 'spb.hh.ru/employer/80', 'skills': ['Java', 'Spring Framework'], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c5')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33847716?query=data%20science>
{'vacancy_title': 'Senior Java Developer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c6')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31015468?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33354237?query=data%20science>
{'vacancy_title': 'Python Engineer', 'company_name': ' Luxoft', 'company_url_hh': 'spb.hh.ru/employer/1304', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c7')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/11687> (referer: https://spb.hh.ru/vacancy/34064602?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33912707?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33864444?query=data%20science>
{'vacancy_title': 'Medical Writer', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c8')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33921789?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31680010?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31716323?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=7)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33613135?query=data%20science>
{'vacancy_title': 'Clinical Research Associate/Senior Clinical Research Associate', 'company_name': ' OCT, Russia', 'company_url_hh': 'spb.hh.ru/employer/45743', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0c9')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31015468?query=data%20science>
{'vacancy_title': 'Python developer /  Python', 'company_name': ' -     ', 'company_url_hh': 'spb.hh.ru/employer/3358623', 'skills': ['Python', 'SQL', 'Data visualisation', 'Cython'], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0ca')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/217737> (referer: https://spb.hh.ru/vacancy/33404502?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/854> (referer: https://spb.hh.ru/vacancy/34103998?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/11687>
{'company_url': 'http://www.jobs.iqvia.com', '_id': ObjectId('5db59470280a695c8af9e0cb')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33912707?query=data%20science>
{'vacancy_title': 'Senior Scala Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/3491569', 'skills': [' ', 'SCALA', 'Python', 'Spark', '-'], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0cc')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1587> (referer: https://spb.hh.ru/vacancy/34031778?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33921789?query=data%20science>
{'vacancy_title': 'Web ', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/97598', 'skills': ['Google Analytics', 'SQL', '  ', ''], 'salary': ' 90\xa0000 .  ', '_id': ObjectId('5db59470280a695c8af9e0cd')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31680010?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (PyCharm)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0ce')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31716323?query=data%20science>
{'vacancy_title': 'Product Marketing Manager (RubyMine)', 'company_name': ' JetBrains', 'company_url_hh': 'spb.hh.ru/employer/9281', 'skills': [], 'salary': ' 150\xa0000 .  ', '_id': ObjectId('5db59470280a695c8af9e0cf')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813435?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/33813402?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33998747?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/14153> (referer: https://spb.hh.ru/vacancy/33497437?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32135809?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933692?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/217737>
{'company_url': 'http://ru.dsr-corporation.com', '_id': ObjectId('5db59470280a695c8af9e0d0')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/1587>
{'company_url': 'http://www.complete.ru', '_id': ObjectId('5db59470280a695c8af9e0d1')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32422629?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813435?query=data%20science>
{'vacancy_title': ' ', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['TFS', 'Scrum', 'Agile Project Management'], 'salary': ' 650 USD  ', '_id': ObjectId('5db59470280a695c8af9e0d2')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/32933697?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/33813402?query=data%20science>
{'vacancy_title': ' .NET', 'company_name': ' UnivIntel', 'company_url_hh': 'spb.hh.ru/employer/2558659', 'skills': ['C#', 'MS Visual Studio', 'ASP.NET', 'JavaScript', '.NET Framework', 'HTML', 'Entity Framework', 'jQuery', 'Git', 'SQL', 'AngularJS', 'PostgreSQL', 'VueJS', 'HTML5', 'CSS'], 'salary': ' 1\xa0800 USD  ', '_id': ObjectId('5db59470280a695c8af9e0d3')}
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/1304> (referer: https://spb.hh.ru/vacancy/33998747?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/14153>
{'company_url': 'https://www.emergn.com/', '_id': ObjectId('5db59470280a695c8af9e0d4')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32135809?query=data%20science>
{'vacancy_title': 'Senior iOS Developer', 'company_name': '  ', 'company_url_hh': 'spb.hh.ru/employer/2610798', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0d5')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933692?query=data%20science>
{'vacancy_title': 'IOS Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0d6')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/97598> (referer: https://spb.hh.ru/vacancy/33921789?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/vacancy/31252237?query=data%20science> (referer: https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=data+science&page=8)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3358623> (referer: https://spb.hh.ru/vacancy/31015468?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32422629?query=data%20science>
{'vacancy_title': 'Senior .NET Developer', 'company_name': ' ', 'company_url_hh': 'spb.hh.ru/employer/208902', 'skills': ['ASP.NET MVC', 'C#', 'ASP.NET', '.NET Framework', 'HTML', 'JavaScript', 'CSS'], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0d7')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/3491569> (referer: https://spb.hh.ru/vacancy/33912707?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/32933697?query=data%20science>
{'vacancy_title': 'Android Developer', 'company_name': ' Devexperts (  -)', 'company_url_hh': 'spb.hh.ru/employer/52729', 'skills': [], 'salary': ' /  ', '_id': ObjectId('5db59470280a695c8af9e0d8')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/97598>
{'company_url': 'https://www.reveltime.ru', '_id': ObjectId('5db59470280a695c8af9e0d9')}
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2558659> (referer: https://spb.hh.ru/vacancy/33813435?query=data%20science)
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/vacancy/31252237?query=data%20science>
{'vacancy_title': 'MS SQL Developer', 'company_name': ' Devim', 'company_url_hh': 'spb.hh.ru/employer/2097195', 'skills': [], 'salary': ' 100\xa0000 .  ', '_id': ObjectId('5db59470280a695c8af9e0da')}
2019-10-27 15:58:24 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3358623>
{'company_url': 'https://www.gazprom-germania.de/home.html', '_id': ObjectId('5db59470280a695c8af9e0db')}
2019-10-27 15:58:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/3832?dpt=3832-3832-const> (referer: https://spb.hh.ru/vacancy/33850385?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/3491569>
{'company_url': 'http://kryptonite.ru', '_id': ObjectId('5db59471280a695c8af9e0dc')}
2019-10-27 15:58:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/80> (referer: https://spb.hh.ru/vacancy/33369948?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://spb.hh.ru/employer/52729> (referer: https://spb.hh.ru/vacancy/32933692?query=data%20science)
Traceback (most recent call last):
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "C:\Users\Sladkish\Anaconda3\envs\data mininig\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\data science\data mininig\lesson3\hh\spiders\hh_vacancy.py", line 40, in parse_company_page
    company_url=response.css('div.HH-SidebarView-UrlContainer a::attr(href)').extract()[0]
IndexError: list index out of range
2019-10-27 15:58:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2558659>
{'company_url': 'https://www.univintel.com', '_id': ObjectId('5db59471280a695c8af9e0dd')}
2019-10-27 15:58:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://spb.hh.ru/employer/2097195> (referer: https://spb.hh.ru/vacancy/31252237?query=data%20science)
2019-10-27 15:58:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://spb.hh.ru/employer/2097195>
{'company_url': 'http://devim.team', '_id': ObjectId('5db59471280a695c8af9e0de')}
2019-10-27 15:58:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-27 15:58:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 114079,
 'downloader/request_count': 265,
 'downloader/request_method_count/GET': 265,
 'downloader/response_bytes': 10887812,
 'downloader/response_count': 265,
 'downloader/response_status_count/200': 265,
 'dupefilter/filtered': 79,
 'elapsed_time_seconds': 7.082154,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 27, 12, 58, 25, 235847),
 'item_scraped_count': 218,
 'log_count/DEBUG': 484,
 'log_count/ERROR': 38,
 'log_count/INFO': 10,
 'request_depth_max': 10,
 'response_received_count': 265,
 'scheduler/dequeued': 265,
 'scheduler/dequeued/memory': 265,
 'scheduler/enqueued': 265,
 'scheduler/enqueued/memory': 265,
 'spider_exceptions/IndexError': 38,
 'start_time': datetime.datetime(2019, 10, 27, 12, 58, 18, 153693)}
2019-10-27 15:58:25 [scrapy.core.engine] INFO: Spider closed (finished)
